"""
ç°¡åŒ–ç‰ˆäº¤é€šæµé‡åˆ†æå™¨ - æ ¸å¿ƒåŠŸèƒ½
=====================================

å°ˆæ³¨æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ğŸ¯ æ•¸æ“šè¼‰å…¥å’ŒåŸºæœ¬åˆ†æ
2. ğŸ¤– AIæ¨¡å‹é©ç”¨æ€§è©•ä¼° 
3. ğŸ“Š é æ¸¬å°±ç·’åº¦æª¢æŸ¥
4. ğŸ“‹ åˆ†æå ±å‘Šç”Ÿæˆ

ä½œè€…: äº¤é€šé æ¸¬å°ˆæ¡ˆåœ˜éšŠ
æ—¥æœŸ: 2025-07-21 (ç°¡åŒ–æ ¸å¿ƒç‰ˆ)
"""

import pandas as pd
import numpy as np
import json
import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import warnings
warnings.filterwarnings('ignore')

class SimplifiedTrafficAnalyzer:
    """ç°¡åŒ–ç‰ˆäº¤é€šæµé‡åˆ†æå™¨ - å°ˆæ³¨æ ¸å¿ƒåŠŸèƒ½"""
    
    def __init__(self, base_folder: str = "data"):
        self.base_folder = Path(base_folder)
        self.cleaned_folder = self.base_folder / "cleaned"
        self.datasets = {}
        self.analysis_results = {}
        self.insights = []
        
        # æ ¸å¿ƒé æ¸¬é…ç½®
        self.prediction_config = {
            'target_columns': ['speed', 'volume_total', 'occupancy'],
            'min_records_for_lstm': 50000,
            'min_time_span_days': 5
        }
        
        print("ğŸ”¬ ç°¡åŒ–ç‰ˆäº¤é€šåˆ†æå™¨åˆå§‹åŒ–")
        print(f"   ğŸ“ æ•¸æ“šç›®éŒ„: {self.cleaned_folder}")
        print(f"   ğŸ¯ é æ¸¬ç›®æ¨™: {', '.join(self.prediction_config['target_columns'])}")
        
        self._detect_data_files()
    
    def _detect_data_files(self):
        """æª¢æ¸¬å¯ç”¨çš„æ•¸æ“šæª”æ¡ˆ"""
        if not self.cleaned_folder.exists():
            print(f"âŒ æ¸…ç†æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {self.cleaned_folder}")
            self.available_dates = []
            return
        
        # æª¢æ¸¬æ—¥æœŸè³‡æ–™å¤¾
        date_folders = [d for d in self.cleaned_folder.iterdir() 
                       if d.is_dir() and d.name.count('-') == 2]
        
        if date_folders:
            self.available_dates = sorted([d.name for d in date_folders])
            print(f"âœ… ç™¼ç¾ {len(self.available_dates)} å€‹å·²æ¸…ç†æ—¥æœŸ")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°å·²æ¸…ç†çš„æ—¥æœŸè³‡æ–™å¤¾")
            self.available_dates = []
    
    def load_data(self, merge_dates: bool = True, sample_rate: float = 1.0) -> bool:
        """è¼‰å…¥æ•¸æ“š"""
        print("ğŸ“Š è¼‰å…¥æ•¸æ“š...")
        
        try:
            if not self.available_dates:
                print("âŒ ç„¡å¯ç”¨æ•¸æ“š")
                return False
            
            all_data = {}
            
            for date_str in self.available_dates:
                date_folder = self.cleaned_folder / date_str
                
                # è¼‰å…¥ç›®æ¨™è·¯æ®µæ•¸æ“š
                data_file = date_folder / "target_route_data_cleaned.csv"
                peak_file = date_folder / "target_route_peak_cleaned.csv"
                offpeak_file = date_folder / "target_route_offpeak_cleaned.csv"
                
                for file_path, key in [
                    (data_file, 'target_data'),
                    (peak_file, 'target_peak'),
                    (offpeak_file, 'target_offpeak')
                ]:
                    if file_path.exists():
                        if key not in all_data:
                            all_data[key] = []
                        
                        # è®€å–æ•¸æ“š
                        df = pd.read_csv(file_path, low_memory=True)
                        
                        # æ¡æ¨£ï¼ˆå¦‚æœéœ€è¦ï¼‰
                        if sample_rate < 1.0:
                            df = df.sample(frac=sample_rate, random_state=42)
                        
                        df['source_date'] = date_str
                        all_data[key].append(df)
            
            # åˆä½µæ•¸æ“š
            for key, df_list in all_data.items():
                if df_list:
                    self.datasets[key] = pd.concat(df_list, ignore_index=True)
                    print(f"   âœ… {key}: {len(self.datasets[key]):,} ç­†è¨˜éŒ„")
            
            return len(self.datasets) > 0
                
        except Exception as e:
            print(f"âŒ æ•¸æ“šè¼‰å…¥å¤±æ•—: {e}")
            return False
    
    def analyze_data_characteristics(self) -> Dict[str, Any]:
        """åˆ†ææ•¸æ“šç‰¹æ€§"""
        print("ğŸ” åˆ†ææ•¸æ“šç‰¹æ€§...")
        
        characteristics = {
            'data_summary': {},
            'quality_metrics': {},
            'time_coverage': {},
            'prediction_readiness': {}
        }
        
        total_records = 0
        all_dates = set()
        
        for name, df in self.datasets.items():
            if not df.empty:
                # åŸºæœ¬çµ±è¨ˆ
                characteristics['data_summary'][name] = {
                    'records': len(df),
                    'avg_speed': round(df['speed'].mean(), 2) if 'speed' in df.columns else 0,
                    'avg_volume': round(df['volume_total'].mean(), 2) if 'volume_total' in df.columns else 0,
                    'unique_vd_stations': df['vd_id'].nunique() if 'vd_id' in df.columns else 0,
                    'completeness': round(df.notna().sum().sum() / (len(df) * len(df.columns)) * 100, 1)
                }
                
                total_records += len(df)
                
                # æ”¶é›†æ—¥æœŸ
                if 'source_date' in df.columns:
                    all_dates.update(df['source_date'].unique())
        
        # æ™‚é–“è¦†è“‹åˆ†æ
        date_span_days = len(all_dates)
        characteristics['time_coverage'] = {
            'total_records': total_records,
            'unique_dates': len(all_dates),
            'date_span_days': date_span_days,
            'records_per_day': round(total_records / max(len(all_dates), 1), 0),
            'estimated_ai_training_hours': round(total_records / 720, 1)
        }
        
        # é æ¸¬å°±ç·’åº¦è©•ä¼°
        main_df = self.datasets.get('target_data')
        if main_df is not None and not main_df.empty:
            prediction_readiness = self._assess_prediction_readiness(main_df, date_span_days)
            characteristics['prediction_readiness'] = prediction_readiness
        
        # å“è³ªè©•ä¼°
        if total_records > 0:
            volume_score = min(100, total_records / 1000)
            time_score = min(100, date_span_days * 20)
            balance_score = self._calculate_balance_score()
            prediction_score = self._calculate_prediction_readiness_score(characteristics)
            
            overall_quality = (volume_score * 0.3 + time_score * 0.3 + 
                             balance_score * 0.2 + prediction_score * 0.2)
            
            characteristics['quality_metrics'] = {
                'volume_score': round(volume_score, 1),
                'time_score': round(time_score, 1),
                'balance_score': round(balance_score, 1),
                'prediction_score': round(prediction_score, 1),
                'overall_quality': round(overall_quality, 1)
            }
        
        self.analysis_results['data_characteristics'] = characteristics
        
        # ç”Ÿæˆæ´å¯Ÿ
        self._generate_insights(characteristics)
        
        return characteristics
    
    def _assess_prediction_readiness(self, df: pd.DataFrame, date_span_days: int) -> Dict[str, Any]:
        """è©•ä¼°é æ¸¬å°±ç·’åº¦"""
        config = self.prediction_config
        
        total_records = len(df)
        unique_vds = df['vd_id'].nunique()
        
        # è¨ˆç®—æ™‚é–“è·¨åº¦ï¼ˆå®‰å…¨æ–¹å¼ï¼‰
        try:
            if 'update_time' in df.columns:
                df['update_time'] = pd.to_datetime(df['update_time'], errors='coerce')
                valid_times = df['update_time'].dropna()
                if len(valid_times) > 0:
                    time_span_hours = (valid_times.max() - valid_times.min()).total_seconds() / 3600
                else:
                    time_span_hours = date_span_days * 24
            else:
                time_span_hours = date_span_days * 24
        except:
            time_span_hours = date_span_days * 24
        
        # ç¼ºå¤±å€¼è©•ä¼°
        target_completeness = {}
        for col in config['target_columns']:
            if col in df.columns:
                completeness = (1 - df[col].isna().sum() / len(df)) * 100
                target_completeness[col] = completeness
        
        avg_completeness = np.mean(list(target_completeness.values())) if target_completeness else 0
        
        # æ¨¡å‹å°±ç·’åº¦è©•ä¼°
        lstm_ready = (
            total_records >= config['min_records_for_lstm'] and
            date_span_days >= config['min_time_span_days'] and
            avg_completeness >= 80 and
            unique_vds >= 3
        )
        
        xgboost_ready = (
            total_records >= 10000 and
            avg_completeness >= 70 and
            unique_vds >= 2
        )
        
        rf_ready = (
            total_records >= 5000 and
            avg_completeness >= 60
        )
        
        return {
            'total_records': total_records,
            'unique_vd_stations': unique_vds,
            'time_span_days': date_span_days,
            'time_span_hours': time_span_hours,
            'target_completeness': target_completeness,
            'avg_completeness': avg_completeness,
            'lstm_ready': lstm_ready,
            'xgboost_ready': xgboost_ready,
            'rf_ready': rf_ready,
            'prediction_ready': lstm_ready or xgboost_ready
        }
    
    def _calculate_prediction_readiness_score(self, characteristics: Dict[str, Any]) -> float:
        """è¨ˆç®—é æ¸¬æº–å‚™åº¦è©•åˆ†"""
        readiness = characteristics.get('prediction_readiness', {})
        
        if not readiness:
            return 50
        
        score = 0
        
        # æ¨¡å‹æº–å‚™åº¦è©•åˆ†
        if readiness.get('lstm_ready', False):
            score += 40
        elif readiness.get('xgboost_ready', False):
            score += 25
        elif readiness.get('rf_ready', False):
            score += 15
        
        # æ•¸æ“šå®Œæ•´æ€§è©•åˆ†
        completeness = readiness.get('avg_completeness', 0)
        score += min(30, completeness * 0.3)
        
        # æ™‚é–“è·¨åº¦è©•åˆ†
        time_span = readiness.get('time_span_days', 0)
        score += min(20, time_span * 2)
        
        # VDç«™é»æ•¸é‡è©•åˆ†
        vd_count = readiness.get('unique_vd_stations', 0)
        score += min(10, vd_count * 2)
        
        return score
    
    def _generate_insights(self, characteristics: Dict[str, Any]):
        """ç”Ÿæˆåˆ†ææ´å¯Ÿ"""
        total_records = characteristics['time_coverage']['total_records']
        prediction_readiness = characteristics.get('prediction_readiness', {})
        
        # æ•¸æ“šè¦æ¨¡æ´å¯Ÿ
        if total_records > 50000:
            self.insights.append(f"ğŸš€ å„ªç§€æ•¸æ“šè¦æ¨¡ï¼š{total_records:,}ç­†è¨˜éŒ„æ”¯æ´æ·±åº¦å­¸ç¿’æ¨¡å‹")
        
        # é æ¸¬æ¨¡å‹æ´å¯Ÿ
        if prediction_readiness.get('lstm_ready', False):
            self.insights.append("ğŸ¯ LSTMæ¨¡å‹å°±ç·’ï¼šæ•¸æ“šç¬¦åˆæ™‚é–“åºåˆ—æ·±åº¦å­¸ç¿’è¦æ±‚")
        elif prediction_readiness.get('xgboost_ready', False):
            self.insights.append("ğŸ“Š XGBoostæ¨¡å‹å°±ç·’ï¼šé©åˆé«˜ç²¾åº¦æ¢¯åº¦æå‡é æ¸¬")
        elif prediction_readiness.get('rf_ready', False):
            self.insights.append("ğŸŒ² éš¨æ©Ÿæ£®æ—æ¨¡å‹å°±ç·’ï¼šé©åˆä½œç‚ºé æ¸¬åŸºç·š")
        
        # æ™‚é–“ç‰¹æ€§æ´å¯Ÿ
        time_span = prediction_readiness.get('time_span_days', 0)
        if time_span >= 7:
            self.insights.append(f"ğŸ“… é€±æœŸæ€§åˆ†æå°±ç·’ï¼š{time_span}å¤©æ•¸æ“šæ”¯æ´é€±æœŸæ¨¡å¼å­¸ç¿’")
        
        # VDç«™é»æ´å¯Ÿ
        vd_count = prediction_readiness.get('unique_vd_stations', 0)
        if vd_count >= 5:
            self.insights.append(f"ğŸ›£ï¸ å¤šç«™é»é æ¸¬ï¼š{vd_count}å€‹VDç«™é»æ”¯æ´è·¯æ®µç´šé æ¸¬")
    
    def evaluate_ai_model_suitability(self) -> Dict[str, Any]:
        """è©•ä¼°AIæ¨¡å‹é©ç”¨æ€§"""
        print("ğŸ¤– è©•ä¼°AIæ¨¡å‹é©ç”¨æ€§...")
        
        characteristics = self.analysis_results.get('data_characteristics', {})
        prediction_readiness = characteristics.get('prediction_readiness', {})
        
        # æ¨¡å‹è©•ä¼°
        model_suitability = {
            'lstm_time_series': self._evaluate_lstm(prediction_readiness),
            'xgboost_ensemble': self._evaluate_xgboost(prediction_readiness),
            'random_forest_baseline': self._evaluate_rf(prediction_readiness)
        }
        
        # ç”Ÿæˆæ¨è–¦
        recommendations = self._generate_recommendations(model_suitability)
        
        ai_evaluation = {
            'model_suitability': model_suitability,
            'recommendations': recommendations,
            'data_readiness': prediction_readiness
        }
        
        self.analysis_results['ai_evaluation'] = ai_evaluation
        return ai_evaluation
    
    def _evaluate_lstm(self, readiness: Dict[str, Any]) -> Dict[str, Any]:
        """è©•ä¼°LSTMæ¨¡å‹"""
        records = readiness.get('total_records', 0)
        time_span = readiness.get('time_span_days', 0)
        completeness = readiness.get('avg_completeness', 0)
        
        # è©•åˆ†è¨ˆç®—
        data_score = min(50, records / 1000)
        time_score = min(30, time_span * 4)
        quality_score = min(20, completeness * 0.2)
        
        total_score = data_score + time_score + quality_score
        
        return {
            'score': round(total_score, 1),
            'suitable': readiness.get('lstm_ready', False),
            'pros': ['æ™‚é–“åºåˆ—å­¸ç¿’', 'é•·æœŸä¾è³´æ•æ‰', '15åˆ†é˜é æ¸¬'],
            'cons': ['éœ€è¦å¤§é‡æ•¸æ“š', 'è¨“ç·´æ™‚é–“é•·', 'éœ€è¦GPU'],
            'expected_accuracy': '85-92%'
        }
    
    def _evaluate_xgboost(self, readiness: Dict[str, Any]) -> Dict[str, Any]:
        """è©•ä¼°XGBoostæ¨¡å‹"""
        records = readiness.get('total_records', 0)
        completeness = readiness.get('avg_completeness', 0)
        
        score = min(100, (records / 10000) * 40 + completeness * 0.6)
        
        return {
            'score': round(score, 1),
            'suitable': readiness.get('xgboost_ready', False),
            'pros': ['é«˜é æ¸¬ç²¾åº¦', 'ç‰¹å¾µé‡è¦æ€§', 'å¿«é€Ÿè¨“ç·´'],
            'cons': ['éœ€è¦èª¿åƒ', 'è¨˜æ†¶é«”éœ€æ±‚é«˜'],
            'expected_accuracy': '80-88%'
        }
    
    def _evaluate_rf(self, readiness: Dict[str, Any]) -> Dict[str, Any]:
        """è©•ä¼°éš¨æ©Ÿæ£®æ—æ¨¡å‹"""
        records = readiness.get('total_records', 0)
        completeness = readiness.get('avg_completeness', 0)
        
        score = min(100, (records / 5000) * 35 + completeness * 0.65)
        
        return {
            'score': round(score, 1),
            'suitable': readiness.get('rf_ready', False),
            'pros': ['ç©©å®šåŸºç·š', 'æ˜“æ–¼ç†è§£', 'æŠ—éæ“¬åˆ'],
            'cons': ['ç²¾åº¦è¼ƒä½', 'é æ¸¬é€Ÿåº¦æ…¢'],
            'expected_accuracy': '75-82%'
        }
    
    def _generate_recommendations(self, suitability: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¨¡å‹æ¨è–¦"""
        scored_models = []
        
        for model_name, info in suitability.items():
            if info['suitable']:
                scored_models.append((model_name, info['score'], info))
        
        # æŒ‰è©•åˆ†æ’åº
        scored_models.sort(key=lambda x: x[1], reverse=True)
        
        recommendations = []
        priorities = ['ğŸ¥‡ é¦–é¸', 'ğŸ¥ˆ æ¬¡é¸', 'ğŸ¥‰ å‚™é¸']
        
        for i, (model_name, score, info) in enumerate(scored_models[:3]):
            priority = priorities[i] if i < len(priorities) else f"#{i+1}"
            
            recommendations.append({
                'rank': i + 1,
                'model': model_name,
                'priority': priority,
                'score': score,
                'reason': f"é©åˆäº¤é€šé æ¸¬çš„{model_name}æ¨¡å‹",
                'pros': info['pros'][:2],
                'expected_accuracy': info['expected_accuracy']
            })
        
        return recommendations
    
    def _calculate_balance_score(self) -> float:
        """è¨ˆç®—æ•¸æ“šå¹³è¡¡æ€§è©•åˆ†"""
        if 'target_peak' in self.datasets and 'target_offpeak' in self.datasets:
            peak_count = len(self.datasets['target_peak'])
            offpeak_count = len(self.datasets['target_offpeak'])
            
            if peak_count > 0 and offpeak_count > 0:
                balance_ratio = min(peak_count, offpeak_count) / max(peak_count, offpeak_count)
                return balance_ratio * 100
        
        return 50
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç¶œåˆåˆ†æå ±å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆç¶œåˆåˆ†æå ±å‘Š...")
        
        # ç¢ºä¿å·²å®Œæˆå¿…è¦åˆ†æ
        if not self.analysis_results.get('data_characteristics'):
            self.analyze_data_characteristics()
        
        if not self.analysis_results.get('ai_evaluation'):
            self.evaluate_ai_model_suitability()
        
        # å»ºæ§‹å ±å‘Š
        report = {
            'metadata': {
                'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'analyzer_version': 'simplified_core_v1.0',
                'total_insights': len(self.insights)
            },
            'data_summary': self.analysis_results['data_characteristics']['data_summary'],
            'quality_assessment': self.analysis_results['data_characteristics']['quality_metrics'],
            'time_coverage': self.analysis_results['data_characteristics']['time_coverage'],
            'prediction_readiness': self.analysis_results['data_characteristics']['prediction_readiness'],
            'ai_model_evaluation': self.analysis_results['ai_evaluation'],
            'key_insights': self.insights,
            'actionable_recommendations': self._generate_actionable_recommendations()
        }
        
        return report
    
    def _generate_actionable_recommendations(self) -> List[str]:
        """ç”Ÿæˆå¯è¡Œå‹•å»ºè­°"""
        recommendations = []
        
        ai_eval = self.analysis_results.get('ai_evaluation', {})
        recommendations_list = ai_eval.get('recommendations', [])
        
        # åŸºæ–¼é ‚ç´šæ¨è–¦çš„å»ºè­°
        if recommendations_list:
            top_model = recommendations_list[0]
            recommendations.append(f"ğŸ¯ ç«‹å³é–‹ç™¼ {top_model['model']} æ¨¡å‹")
            recommendations.append(f"ğŸ“ˆ é æœŸæº–ç¢ºç‡: {top_model['expected_accuracy']}")
        
        # åŸºæ–¼æ•¸æ“šç‹€æ³çš„å»ºè­°
        prediction_readiness = self.analysis_results.get('data_characteristics', {}).get('prediction_readiness', {})
        
        if prediction_readiness.get('lstm_ready', False):
            recommendations.append("ğŸš€ æ•¸æ“šç¬¦åˆLSTMè¦æ±‚ - å¯é–‹å§‹æ·±åº¦å­¸ç¿’é–‹ç™¼")
        
        if prediction_readiness.get('unique_vd_stations', 0) >= 5:
            recommendations.append("ğŸ›£ï¸ å¤šç«™é»é æ¸¬èƒ½åŠ› - å¯å¯¦ç¾è·¯æ®µç´šäº¤é€šé æ¸¬")
        
        return recommendations
    
    def save_report(self, report: Dict[str, Any], filename: str = None):
        """ä¿å­˜åˆ†æå ±å‘Š"""
        output_dir = Path("outputs/reports")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        if not filename:
            filename = f"simplified_traffic_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        output_path = output_dir / filename
        
        # ç¢ºä¿æ‰€æœ‰æ•¸æ“šéƒ½å¯ä»¥JSONåºåˆ—åŒ–
        safe_report = self._safe_json_convert(report)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(safe_report, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å ±å‘Šå·²ä¿å­˜: {output_path}")
    
    def _safe_json_convert(self, obj):
        """å®‰å…¨çš„JSONè½‰æ›"""
        if isinstance(obj, dict):
            return {str(k): self._safe_json_convert(v) for k, v in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [self._safe_json_convert(item) for item in obj]
        elif isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif pd.isna(obj):
            return None
        elif hasattr(obj, 'isoformat'):  # datetime objects
            return obj.isoformat()
        else:
            try:
                return str(obj)
            except:
                return None
    
    def print_summary(self):
        """åˆ—å°åˆ†ææ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š ç°¡åŒ–ç‰ˆäº¤é€šæµé‡åˆ†ææ‘˜è¦")
        print("="*60)
        
        # æ•¸æ“šæ¦‚è¦½
        if 'data_characteristics' in self.analysis_results:
            char = self.analysis_results['data_characteristics']
            time_coverage = char['time_coverage']
            quality_metrics = char['quality_metrics']
            prediction_readiness = char['prediction_readiness']
            
            print(f"ğŸ“ˆ æ•¸æ“šè¦æ¨¡:")
            print(f"   ç¸½è¨˜éŒ„æ•¸: {time_coverage.get('total_records', 0):,}")
            print(f"   æ™‚é–“è·¨åº¦: {time_coverage.get('date_span_days', 0)} å¤©")
            print(f"   æ•´é«”å“è³ª: {quality_metrics.get('overall_quality', 0):.1f}/100")
            
            print(f"\nğŸ¯ é æ¸¬å°±ç·’åº¦:")
            print(f"   LSTMå°±ç·’: {'âœ… æ˜¯' if prediction_readiness.get('lstm_ready') else 'âŒ å¦'}")
            print(f"   XGBoostå°±ç·’: {'âœ… æ˜¯' if prediction_readiness.get('xgboost_ready') else 'âŒ å¦'}")
            print(f"   éš¨æ©Ÿæ£®æ—å°±ç·’: {'âœ… æ˜¯' if prediction_readiness.get('rf_ready') else 'âŒ å¦'}")
            print(f"   VDç«™é»æ•¸: {prediction_readiness.get('unique_vd_stations', 0)} å€‹")
        
        # AIæ¨¡å‹æ¨è–¦
        if 'ai_evaluation' in self.analysis_results:
            recommendations = self.analysis_results['ai_evaluation']['recommendations']
            print(f"\nğŸ¤– AIæ¨¡å‹æ¨è–¦:")
            
            for rec in recommendations:
                print(f"   {rec['priority']} {rec['model']}: {rec['score']:.1f}åˆ†")
                print(f"      é æœŸæº–ç¢ºç‡: {rec['expected_accuracy']}")
        
        # é—œéµæ´å¯Ÿ
        if self.insights:
            print(f"\nğŸ’¡ é—œéµç™¼ç¾:")
            for i, insight in enumerate(self.insights, 1):
                print(f"   {i}. {insight}")
        
        print("\nâœ… ç°¡åŒ–ç‰ˆåˆ†æå®Œæˆï¼")


def quick_analyze(base_folder: str = "data", sample_rate: float = 1.0) -> Dict[str, Any]:
    """å¿«é€Ÿåˆ†æå‡½æ•¸"""
    print("ğŸš€ å•Ÿå‹•ç°¡åŒ–ç‰ˆäº¤é€šæµé‡åˆ†æ...")
    
    analyzer = SimplifiedTrafficAnalyzer(base_folder)
    
    if analyzer.load_data(sample_rate=sample_rate):
        analyzer.analyze_data_characteristics()
        analyzer.evaluate_ai_model_suitability()
        report = analyzer.generate_comprehensive_report()
        analyzer.print_summary()
        analyzer.save_report(report)
        return report
    else:
        print("âŒ æ•¸æ“šè¼‰å…¥å¤±æ•—")
        return {}


if __name__ == "__main__":
    print("ğŸš€ å•Ÿå‹•ç°¡åŒ–ç‰ˆäº¤é€šæµé‡åˆ†æå™¨")
    print("=" * 60)
    print("ğŸ¯ æ ¸å¿ƒåŠŸèƒ½:")
    print("   ğŸ“Š æ•¸æ“šè¼‰å…¥å’Œç‰¹æ€§åˆ†æ")
    print("   ğŸ¤– AIæ¨¡å‹é©ç”¨æ€§è©•ä¼°")
    print("   ğŸ“‹ åˆ†æå ±å‘Šç”Ÿæˆ")
    print("=" * 60)
    
    quick_analyze()