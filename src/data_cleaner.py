# src/data_cleaner.py - é©é…ç‰ˆ

"""
VDæ•¸æ“šæ¸…ç†å™¨ - é©é…å¼·åŒ–ç‰ˆè¼‰å…¥å™¨
===============================

å°ˆé–€é©é…å¼·åŒ–ç‰ˆdata_loader.pyçš„è¼¸å‡ºæ ¼å¼ï¼š
1. ğŸ¯ å°ˆæ³¨ç›®æ¨™è·¯æ®µæª”æ¡ˆæ¸…ç†
2. ğŸ“ é©é…æ–°çš„æª”æ¡ˆçµæ§‹ (target_route_*.csv)
3. ğŸ’¾ ä¿æŒè¨˜æ†¶é«”å„ªåŒ–ç‰¹æ€§
4. âš¡ ç°¡åŒ–æ¸…ç†æµç¨‹
5. ğŸ”„ å®Œç¾é…åˆå½ˆæ€§è™•ç†
"""

import pandas as pd
import numpy as np
import os
import json
import gc
import psutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
from contextlib import contextmanager
import warnings
warnings.filterwarnings('ignore')


class VDTargetRouteCleaner:
    """VDç›®æ¨™è·¯æ®µæ¸…ç†å™¨ - é©é…å¼·åŒ–ç‰ˆè¼‰å…¥å™¨"""
    
    def __init__(self, base_folder: str = "data", target_memory_percent: float = 70.0):
        """
        åˆå§‹åŒ–ç›®æ¨™è·¯æ®µæ¸…ç†å™¨
        
        Args:
            base_folder: åŸºç¤è³‡æ–™å¤¾è·¯å¾‘
            target_memory_percent: ç›®æ¨™è¨˜æ†¶é«”ä½¿ç”¨ç‡
        """
        self.base_folder = Path(base_folder)
        self.processed_base_folder = self.base_folder / "processed"
        self.cleaned_base_folder = self.base_folder / "cleaned"
        self.target_memory_percent = target_memory_percent
        
        # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
        self.cleaned_base_folder.mkdir(parents=True, exist_ok=True)
        
        # ç•°å¸¸å€¼å®šç¾©
        self.invalid_markers = [-99, -1, 999, 9999, float('inf'), -float('inf')]
        
        # åˆç†æ•¸å€¼ç¯„åœ
        self.valid_ranges = {
            'speed': (0, 150),
            'occupancy': (0, 100),
            'volume_total': (0, 100),
            'volume_small': (0, 100),
            'volume_large': (0, 50),
            'volume_truck': (0, 50),
            'speed_small': (0, 150),
            'speed_large': (0, 150),
            'speed_truck': (0, 150)
        }
        
        # ğŸ†• é©é…å¼·åŒ–ç‰ˆè¼‰å…¥å™¨çš„æª”æ¡ˆçµæ§‹
        self.target_file_mappings = {
            'target_route_data': {
                'pattern': 'target_route_data.csv',
                'output': 'target_route_data_cleaned.csv',
                'description': "ç›®æ¨™è·¯æ®µæ‰€æœ‰æ•¸æ“š"
            },
            'target_route_peak': {
                'pattern': 'target_route_peak.csv',
                'output': 'target_route_peak_cleaned.csv',
                'description': "ç›®æ¨™è·¯æ®µå°–å³°æ•¸æ“š"
            },
            'target_route_offpeak': {
                'pattern': 'target_route_offpeak.csv',
                'output': 'target_route_offpeak_cleaned.csv',
                'description': "ç›®æ¨™è·¯æ®µé›¢å³°æ•¸æ“š"
            }
        }
        
        print("ğŸ§¹ VDç›®æ¨™è·¯æ®µæ¸…ç†å™¨é©é…ç‰ˆåˆå§‹åŒ–")
        print(f"   ğŸ“ è¼¸å…¥ç›®éŒ„: {self.processed_base_folder}")
        print(f"   ğŸ“ è¼¸å‡ºç›®éŒ„: {self.cleaned_base_folder}")
        print(f"   ğŸ’¾ ç›®æ¨™è¨˜æ†¶é«”: {target_memory_percent}%")
        print(f"   ğŸ¯ ç›®æ¨™æª”æ¡ˆ: {len(self.target_file_mappings)} ç¨®")
    
    @contextmanager
    def memory_monitor(self, operation_name: str = "æ¸…ç†æ“ä½œ"):
        """è¨˜æ†¶é«”ç›£æ§ä¸Šä¸‹æ–‡"""
        start_memory = psutil.virtual_memory().percent
        
        try:
            yield
        finally:
            end_memory = psutil.virtual_memory().percent
            
            # æ™ºèƒ½åƒåœ¾å›æ”¶
            if end_memory > self.target_memory_percent:
                gc.collect()
                final_memory = psutil.virtual_memory().percent
                if abs(final_memory - start_memory) > 5:  # è¨˜æ†¶é«”è®ŠåŒ–è¶…é5%æ‰é¡¯ç¤º
                    print(f"   ğŸ§¹ {operation_name}: {start_memory:.1f}% â†’ {final_memory:.1f}%")
    
    def detect_available_dates(self) -> List[str]:
        """æª¢æ¸¬å¯ç”¨çš„æ—¥æœŸè³‡æ–™å¤¾ï¼ˆé©é…å¼·åŒ–ç‰ˆï¼‰"""
        print("ğŸ” æª¢æ¸¬å¯ç”¨æ—¥æœŸè³‡æ–™å¤¾...")
        
        available_dates = []
        
        if not self.processed_base_folder.exists():
            print(f"   âŒ è™•ç†è³‡æ–™å¤¾ä¸å­˜åœ¨: {self.processed_base_folder}")
            return available_dates
        
        # æƒææ—¥æœŸè³‡æ–™å¤¾
        for item in self.processed_base_folder.iterdir():
            if item.is_dir() and item.name.count('-') == 2:  # YYYY-MM-DD æ ¼å¼
                date_str = item.name
                
                # æª¢æŸ¥æ˜¯å¦æœ‰ç›®æ¨™è·¯æ®µæª”æ¡ˆï¼ˆé©é…å¼·åŒ–ç‰ˆè¼¸å‡ºï¼‰
                has_target_files = False
                target_files_found = []
                
                for name, file_info in self.target_file_mappings.items():
                    target_file = item / file_info['pattern']
                    if target_file.exists():
                        has_target_files = True
                        target_files_found.append(file_info['pattern'])
                
                if has_target_files:
                    available_dates.append(date_str)
                    print(f"   âœ… {date_str}: {len(target_files_found)} å€‹ç›®æ¨™æª”æ¡ˆ")
                else:
                    print(f"   âš ï¸ {date_str}: ç„¡ç›®æ¨™è·¯æ®µæª”æ¡ˆ")
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(available_dates)} å€‹å¯æ¸…ç†æ—¥æœŸ")
        return sorted(available_dates)
    
    def clean_date_folder(self, date_str: str, method: str = 'mark_nan') -> Dict[str, Any]:
        """æ¸…ç†å–®ä¸€æ—¥æœŸè³‡æ–™å¤¾ï¼ˆé©é…å¼·åŒ–ç‰ˆï¼‰"""
        print(f"ğŸ“… æ¸…ç† {date_str}...")
        
        with self.memory_monitor(f"æ—¥æœŸ {date_str} æ¸…ç†"):
            date_input_folder = self.processed_base_folder / date_str
            date_output_folder = self.cleaned_base_folder / date_str
            date_output_folder.mkdir(parents=True, exist_ok=True)
            
            cleaning_results = []
            total_files = 0
            successful_files = 0
            
            # è™•ç†ç›®æ¨™è·¯æ®µæª”æ¡ˆ
            for name, file_info in self.target_file_mappings.items():
                input_file = date_input_folder / file_info['pattern']
                
                if input_file.exists():
                    total_files += 1
                    output_file = date_output_folder / file_info['output']
                    
                    result = self._clean_single_file(
                        input_file, output_file, 
                        file_info['description'], method
                    )
                    
                    cleaning_results.append(result)
                    if result['success']:
                        successful_files += 1
                        print(f"      âœ… {file_info['description']}")
                    else:
                        print(f"      âŒ {file_info['description']}: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
            
            # è¤‡è£½æ‘˜è¦æª”æ¡ˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            summary_source = date_input_folder / "target_route_summary.json"
            if summary_source.exists():
                summary_dest = date_output_folder / "target_route_summary.json"
                import shutil
                shutil.copy2(summary_source, summary_dest)
                print(f"      ğŸ“‹ è¤‡è£½æ‘˜è¦æª”æ¡ˆ")
            
            # ç”Ÿæˆæ—¥æœŸæ¸…ç†å ±å‘Š
            date_report = {
                'date': date_str,
                'total_files': total_files,
                'successful_files': successful_files,
                'success_rate': f"{(successful_files/total_files*100):.1f}%" if total_files > 0 else "0%",
                'cleaning_results': cleaning_results,
                'output_folder': str(date_output_folder)
            }
            
            # ä¿å­˜å ±å‘Š
            report_path = date_output_folder / "cleaning_report.json"
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(date_report, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"   âœ… {date_str}: {successful_files}/{total_files} æª”æ¡ˆæˆåŠŸ")
            return date_report
    
    def _clean_single_file(self, input_path: Path, output_path: Path, 
                          description: str, method: str) -> Dict[str, Any]:
        """æ¸…ç†å–®ä¸€æª”æ¡ˆï¼ˆè¨˜æ†¶é«”å„ªåŒ–ç‰ˆï¼‰"""
        try:
            with self.memory_monitor(f"æ¸…ç† {description}"):
                # æª¢æŸ¥æª”æ¡ˆå¤§å°ï¼Œæ±ºå®šè™•ç†ç­–ç•¥
                file_size_mb = input_path.stat().st_size / (1024 * 1024)
                
                if file_size_mb > 50:  # å¤§æª”æ¡ˆåˆ†æ‰¹è™•ç†ï¼ˆèª¿ä½é–¾å€¼ï¼‰
                    return self._clean_large_file(input_path, output_path, description, method)
                else:
                    return self._clean_small_file(input_path, output_path, description, method)
                    
        except Exception as e:
            return {
                'file': description,
                'success': False,
                'error': str(e),
                'input_path': str(input_path),
                'output_path': str(output_path)
            }
    
    def _clean_small_file(self, input_path: Path, output_path: Path, 
                         description: str, method: str) -> Dict[str, Any]:
        """æ¸…ç†å°æª”æ¡ˆ"""
        # è¼‰å…¥æ•¸æ“š
        df = pd.read_csv(input_path, low_memory=True)
        original_count = len(df)
        
        # å„ªåŒ–æ•¸æ“šé¡å‹
        df = self._optimize_dtypes(df)
        
        # è­˜åˆ¥ä¸¦æ¸…ç†ç•°å¸¸å€¼
        invalid_count = self._count_invalid_values(df)
        df_cleaned = self._apply_cleaning_method(df, method)
        cleaned_count = len(df_cleaned)
        
        # ä¿å­˜æ¸…ç†å¾Œæ•¸æ“š
        df_cleaned.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        return {
            'file': description,
            'success': True,
            'original_records': original_count,
            'cleaned_records': cleaned_count,
            'removed_records': original_count - cleaned_count,
            'invalid_values': invalid_count,
            'input_path': str(input_path),
            'output_path': str(output_path),
            'file_size_mb': round(output_path.stat().st_size / (1024 * 1024), 1)
        }
    
    def _clean_large_file(self, input_path: Path, output_path: Path, 
                         description: str, method: str) -> Dict[str, Any]:
        """æ¸…ç†å¤§æª”æ¡ˆï¼ˆåˆ†æ‰¹è™•ç†ï¼‰"""
        chunk_size = 30000  # æ¯æ‰¹è™•ç†3è¬è¨˜éŒ„ï¼ˆé™ä½æ‰¹æ¬¡å¤§å°ï¼‰
        total_original = 0
        total_cleaned = 0
        total_invalid = 0
        
        # åˆ†æ‰¹è®€å–å’Œè™•ç†
        first_chunk = True
        
        for chunk in pd.read_csv(input_path, chunksize=chunk_size, low_memory=True):
            total_original += len(chunk)
            
            # å„ªåŒ–æ•¸æ“šé¡å‹
            chunk = self._optimize_dtypes(chunk)
            
            # æ¸…ç†ç•°å¸¸å€¼
            total_invalid += self._count_invalid_values(chunk)
            chunk_cleaned = self._apply_cleaning_method(chunk, method)
            total_cleaned += len(chunk_cleaned)
            
            # ä¿å­˜ï¼ˆç¬¬ä¸€æ‰¹åŒ…å«æ¨™é ­ï¼‰
            mode = 'w' if first_chunk else 'a'
            header = first_chunk
            
            chunk_cleaned.to_csv(output_path, mode=mode, header=header, 
                               index=False, encoding='utf-8-sig')
            
            first_chunk = False
            
            # æ¸…ç†è¨˜æ†¶é«”
            del chunk, chunk_cleaned
            gc.collect()
        
        return {
            'file': description,
            'success': True,
            'original_records': total_original,
            'cleaned_records': total_cleaned,
            'removed_records': total_original - total_cleaned,
            'invalid_values': total_invalid,
            'input_path': str(input_path),
            'output_path': str(output_path),
            'file_size_mb': round(output_path.stat().st_size / (1024 * 1024), 1),
            'processing_method': 'chunked'
        }
    
    def _optimize_dtypes(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¿«é€Ÿå„ªåŒ–æ•¸æ“šé¡å‹"""
        if df.empty:
            return df
        
        # æ™‚é–“é¡å‹
        if 'update_time' in df.columns:
            df['update_time'] = pd.to_datetime(df['update_time'], errors='coerce')
        
        # æ•¸å€¼é¡å‹
        numeric_cols = [col for col in self.valid_ranges.keys() if col in df.columns]
        for col in numeric_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce', downcast='integer')
        
        # é¡åˆ¥é¡å‹
        if 'vd_id' in df.columns:
            df['vd_id'] = df['vd_id'].astype('category')
        
        if 'time_category' in df.columns:
            df['time_category'] = df['time_category'].astype('category')
        
        return df
    
    def _count_invalid_values(self, df: pd.DataFrame) -> int:
        """è¨ˆç®—ç•°å¸¸å€¼æ•¸é‡"""
        invalid_count = 0
        
        numeric_cols = [col for col in self.valid_ranges.keys() if col in df.columns]
        
        for col in numeric_cols:
            # ç•°å¸¸æ¨™è¨˜
            invalid_count += df[col].isin(self.invalid_markers).sum()
            
            # è¶…å‡ºç¯„åœ
            min_val, max_val = self.valid_ranges[col]
            invalid_count += ((df[col] < min_val) | (df[col] > max_val)).sum()
            
            # NaNå€¼
            invalid_count += df[col].isna().sum()
        
        return int(invalid_count)
    
    def _apply_cleaning_method(self, df: pd.DataFrame, method: str) -> pd.DataFrame:
        """æ‡‰ç”¨æ¸…ç†æ–¹æ³•"""
        df_cleaned = df.copy()
        numeric_cols = [col for col in self.valid_ranges.keys() if col in df_cleaned.columns]
        
        if method == 'mark_nan':
            for col in numeric_cols:
                # æ›¿æ›ç•°å¸¸æ¨™è¨˜
                df_cleaned.loc[df_cleaned[col].isin(self.invalid_markers), col] = np.nan
                
                # æ›¿æ›è¶…å‡ºç¯„åœçš„å€¼
                min_val, max_val = self.valid_ranges[col]
                out_of_range = (df_cleaned[col] < min_val) | (df_cleaned[col] > max_val)
                df_cleaned.loc[out_of_range, col] = np.nan
                
        elif method == 'remove_rows':
            for col in numeric_cols:
                # ç§»é™¤ç•°å¸¸è¡Œ
                df_cleaned = df_cleaned[~df_cleaned[col].isin(self.invalid_markers)]
                
                # ç§»é™¤è¶…å‡ºç¯„åœçš„è¡Œ
                min_val, max_val = self.valid_ranges[col]
                df_cleaned = df_cleaned[
                    (df_cleaned[col] >= min_val) & (df_cleaned[col] <= max_val)
                ]
                
                # ç§»é™¤NaNè¡Œ
                df_cleaned = df_cleaned.dropna(subset=[col])
        
        return df_cleaned
    
    def clean_all_dates(self, method: str = 'mark_nan') -> Dict[str, Any]:
        """æ‰¹æ¬¡æ¸…ç†æ‰€æœ‰æ—¥æœŸï¼ˆè¨˜æ†¶é«”å„ªåŒ–ç‰ˆï¼‰"""
        print("ğŸš€ é–‹å§‹æ‰¹æ¬¡æ¸…ç†æ‰€æœ‰ç›®æ¨™è·¯æ®µæ•¸æ“š")
        print("=" * 50)
        
        start_time = datetime.now()
        
        with self.memory_monitor("æ‰¹æ¬¡æ¸…ç†"):
            available_dates = self.detect_available_dates()
            
            if not available_dates:
                return {"success": False, "error": "ç„¡å¯æ¸…ç†æ—¥æœŸ"}
            
            print(f"ğŸ§¹ æ¸…ç† {len(available_dates)} å€‹æ—¥æœŸï¼Œæ–¹æ³•: {method}")
            
            # æ‰¹æ¬¡æ¸…ç†
            date_results = []
            successful_dates = 0
            total_files = 0
            successful_files = 0
            
            for i, date_str in enumerate(available_dates, 1):
                try:
                    print(f"   ğŸ“… [{i}/{len(available_dates)}] {date_str}")
                    date_result = self.clean_date_folder(date_str, method)
                    date_results.append(date_result)
                    
                    if date_result['successful_files'] > 0:
                        successful_dates += 1
                    
                    total_files += date_result['total_files']
                    successful_files += date_result['successful_files']
                    
                    # å®šæœŸæ¸…ç†è¨˜æ†¶é«”
                    if i % 3 == 0:
                        gc.collect()
                        
                except Exception as e:
                    print(f"   âŒ {date_str}: æ¸…ç†å¤±æ•— - {e}")
                    date_results.append({
                        'date': date_str,
                        'success': False,
                        'error': str(e)
                    })
            
            # ç”Ÿæˆç¸½å ±å‘Š
            duration = (datetime.now() - start_time).total_seconds()
            
            batch_report = {
                'metadata': {
                    'start_time': start_time.isoformat(),
                    'duration_seconds': round(duration, 2),
                    'method': method,
                    'target_files': list(self.target_file_mappings.keys())
                },
                'summary': {
                    'total_dates': len(available_dates),
                    'successful_dates': successful_dates,
                    'total_files': total_files,
                    'successful_files': successful_files,
                    'success_rate': f"{(successful_dates/len(available_dates)*100):.1f}%"
                },
                'date_results': date_results,
                'output_folder': str(self.cleaned_base_folder)
            }
            
            # ä¿å­˜æ‰¹æ¬¡å ±å‘Š
            report_path = self.cleaned_base_folder / "batch_cleaning_report.json"
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(batch_report, f, ensure_ascii=False, indent=2, default=str)
            
            print(f"\nğŸ æ‰¹æ¬¡æ¸…ç†å®Œæˆ")
            print(f"   â±ï¸ è€—æ™‚: {duration:.1f} ç§’")
            print(f"   ğŸ“… æˆåŠŸæ—¥æœŸ: {successful_dates}/{len(available_dates)}")
            print(f"   ğŸ“„ æˆåŠŸæª”æ¡ˆ: {successful_files}/{total_files}")
            print(f"   ğŸ“ å ±å‘Š: {report_path}")
            
            return batch_report
    
    def get_cleaned_summary(self) -> Dict[str, Any]:
        """ç²å–æ¸…ç†æ‘˜è¦"""
        print("ğŸ“Š æª¢æŸ¥æ¸…ç†çµæœ...")
        
        summary = {
            'cleaned_dates': 0,
            'total_files': 0,
            'total_records': 0,
            'total_size_mb': 0,
            'date_details': {}
        }
        
        if not self.cleaned_base_folder.exists():
            return summary
        
        # æƒææ¸…ç†å¾Œæ—¥æœŸ
        for date_folder in self.cleaned_base_folder.iterdir():
            if date_folder.is_dir() and date_folder.name.count('-') == 2:
                date_str = date_folder.name
                
                # çµ±è¨ˆè©²æ—¥æœŸçš„æª”æ¡ˆ
                csv_files = list(date_folder.glob("*_cleaned.csv"))
                if csv_files:
                    summary['cleaned_dates'] += 1
                    summary['total_files'] += len(csv_files)
                    
                    date_size = 0
                    date_records = 0
                    
                    for csv_file in csv_files:
                        try:
                            file_size = csv_file.stat().st_size / (1024 * 1024)
                            date_size += file_size
                            
                            # ä¼°ç®—è¨˜éŒ„æ•¸
                            estimated_records = int(file_size * 1000)
                            date_records += estimated_records
                            
                        except:
                            continue
                    
                    summary['total_size_mb'] += date_size
                    summary['total_records'] += date_records
                    
                    summary['date_details'][date_str] = {
                        'files': len(csv_files),
                        'size_mb': round(date_size, 1),
                        'estimated_records': date_records
                    }
                    
                    print(f"   âœ… {date_str}: {len(csv_files)} æª”æ¡ˆ ({date_size:.1f}MB)")
        
        summary['total_size_mb'] = round(summary['total_size_mb'], 1)
        
        print(f"\nğŸ“ˆ æ¸…ç†æ‘˜è¦:")
        print(f"   ğŸ“… å·²æ¸…ç†æ—¥æœŸ: {summary['cleaned_dates']}")
        print(f"   ğŸ“„ ç¸½æª”æ¡ˆæ•¸: {summary['total_files']}")
        print(f"   ğŸ“Š ç¸½è¨˜éŒ„æ•¸: ~{summary['total_records']:,}")
        print(f"   ğŸ’¾ ç¸½å¤§å°: {summary['total_size_mb']:.1f}MB")
        
        return summary
    
    def load_cleaned_date(self, date_str: str) -> Dict[str, pd.DataFrame]:
        """è¼‰å…¥ç‰¹å®šæ—¥æœŸçš„æ¸…ç†æ•¸æ“š"""
        print(f"ğŸ“… è¼‰å…¥ {date_str} æ¸…ç†æ•¸æ“š...")
        
        date_folder = self.cleaned_base_folder / date_str
        if not date_folder.exists():
            print(f"   âŒ æ—¥æœŸè³‡æ–™å¤¾ä¸å­˜åœ¨: {date_str}")
            return {}
        
        cleaned_data = {}
        
        # è¼‰å…¥ç›®æ¨™æª”æ¡ˆ
        for name, file_info in self.target_file_mappings.items():
            csv_file = date_folder / file_info['output']
            
            if csv_file.exists():
                try:
                    with self.memory_monitor(f"è¼‰å…¥ {file_info['description']}"):
                        df = pd.read_csv(csv_file, low_memory=True)
                        df = self._optimize_dtypes(df)
                        cleaned_data[name] = df
                        print(f"   âœ… {file_info['description']}: {len(df):,} ç­†")
                        
                except Exception as e:
                    print(f"   âŒ {file_info['description']}: è¼‰å…¥å¤±æ•— - {e}")
                    cleaned_data[name] = pd.DataFrame()
            else:
                print(f"   âš ï¸ {file_info['description']}: æª”æ¡ˆä¸å­˜åœ¨")
                cleaned_data[name] = pd.DataFrame()
        
        return cleaned_data


# ============================================================
# ä¾¿åˆ©å‡½æ•¸
# ============================================================

def clean_all_target_data(base_folder: str = "data", method: str = 'mark_nan') -> Dict[str, Any]:
    """ä¸€éµæ¸…ç†æ‰€æœ‰ç›®æ¨™è·¯æ®µæ•¸æ“š"""
    cleaner = VDTargetRouteCleaner(base_folder)
    return cleaner.clean_all_dates(method)


def get_cleaning_summary(base_folder: str = "data") -> Dict[str, Any]:
    """ç²å–æ¸…ç†æ‘˜è¦"""
    cleaner = VDTargetRouteCleaner(base_folder)
    return cleaner.get_cleaned_summary()


def load_cleaned_data(base_folder: str = "data", date_str: str = None) -> Dict[str, pd.DataFrame]:
    """è¼‰å…¥æ¸…ç†æ•¸æ“š"""
    cleaner = VDTargetRouteCleaner(base_folder)
    
    if date_str:
        return cleaner.load_cleaned_date(date_str)
    else:
        # è¼‰å…¥æœ€æ–°æ—¥æœŸ
        available_dates = cleaner.detect_available_dates()
        if available_dates:
            return cleaner.load_cleaned_date(available_dates[-1])
        return {}


# ä¿æŒå‘å¾Œç›¸å®¹æ€§
clean_all_data = clean_all_target_data


if __name__ == "__main__":
    print("ğŸ§¹ VDç›®æ¨™è·¯æ®µæ¸…ç†å™¨ - é©é…å¼·åŒ–ç‰ˆè¼‰å…¥å™¨")
    print("=" * 60)
    print("ğŸ¯ å°ˆé–€è™•ç†ç›®æ¨™è·¯æ®µæª”æ¡ˆ:")
    print("   â€¢ target_route_data.csv â†’ target_route_data_cleaned.csv")
    print("   â€¢ target_route_peak.csv â†’ target_route_peak_cleaned.csv")
    print("   â€¢ target_route_offpeak.csv â†’ target_route_offpeak_cleaned.csv")
    print("=" * 60)
    
    cleaner = VDTargetRouteCleaner()
    
    # æª¢æ¸¬å¯ç”¨æ—¥æœŸ
    available_dates = cleaner.detect_available_dates()
    
    if available_dates:
        print(f"\nğŸ“… ç™¼ç¾ {len(available_dates)} å€‹å¯æ¸…ç†æ—¥æœŸ")
        for date_str in available_dates:
            print(f"   â€¢ {date_str}")
        
        response = input(f"\né–‹å§‹æ‰¹æ¬¡æ¸…ç†ç›®æ¨™è·¯æ®µæ•¸æ“šï¼Ÿ(y/N): ")
        
        if response.lower() in ['y', 'yes']:
            # åŸ·è¡Œæ¸…ç†
            report = cleaner.clean_all_dates()
            
            if report['summary']['successful_dates'] > 0:
                print(f"\nğŸ‰ æ¸…ç†å®Œæˆï¼")
                
                # é¡¯ç¤ºæ‘˜è¦
                summary = cleaner.get_cleaned_summary()
                print(f"\nğŸ“Š æ¸…ç†çµæœ:")
                print(f"   ğŸ“… å·²æ¸…ç†æ—¥æœŸ: {summary['cleaned_dates']}")
                print(f"   ğŸ“„ ç¸½æª”æ¡ˆæ•¸: {summary['total_files']}")
                print(f"   ğŸ“Š ç¸½è¨˜éŒ„æ•¸: ~{summary['total_records']:,}")
                print(f"   ğŸ’¾ ç¸½å¤§å°: {summary['total_size_mb']:.1f}MB")
                
                print(f"\nğŸ“ æ¸…ç†å¾Œçµæ§‹:")
                print(f"   ğŸ“‚ data/cleaned/")
                for date_str in summary['date_details']:
                    details = summary['date_details'][date_str]
                    print(f"      â”œâ”€â”€ {date_str}/ ({details['files']} æª”æ¡ˆ)")
                    print(f"      â”‚   â”œâ”€â”€ target_route_data_cleaned.csv")
                    print(f"      â”‚   â”œâ”€â”€ target_route_peak_cleaned.csv")
                    print(f"      â”‚   â””â”€â”€ target_route_offpeak_cleaned.csv")
                
                print(f"\nğŸ¯ å»ºè­°ä¸‹ä¸€æ­¥:")
                print("   1. æª¢æŸ¥æ¸…ç†å¾Œæ•¸æ“š: python test_cleaner.py")
                print("   2. é–‹å§‹AIæ¨¡å‹é–‹ç™¼: python src/predictor.py")
                print("   3. ä½¿ç”¨æ¸…ç†æ•¸æ“šé€²è¡Œäº¤é€šé æ¸¬åˆ†æ")
            else:
                print("âŒ æ¸…ç†å¤±æ•—")
    else:
        print("\nğŸ’¡ è«‹å…ˆåŸ·è¡Œæ•¸æ“šè¼‰å…¥:")
        print("   python -c \"from src.data_loader import auto_process_data; auto_process_data()\"")
        print("   æˆ–")
        print("   python test_loader.py")