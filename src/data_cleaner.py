# src/data_cleaner.py - æ—¥æœŸè³‡æ–™å¤¾çµ„ç¹”ç‰ˆ

"""
VDæ•¸æ“šæ¸…ç†å™¨ - æ—¥æœŸè³‡æ–™å¤¾çµ„ç¹”ç‰ˆ
==============================

æ–°å¢åŠŸèƒ½ï¼š
1. æ”¯æ´æŒ‰æ—¥æœŸçµ„ç¹”çš„è™•ç†è³‡æ–™å¤¾ï¼šdata/processed/2025-06-27/
2. è¼¸å‡ºåˆ°æŒ‰æ—¥æœŸçµ„ç¹”çš„æ¸…ç†è³‡æ–™å¤¾ï¼šdata/cleaned/2025-06-27/
3. è‡ªå‹•åµæ¸¬å¤šæ—¥æœŸè³‡æ–™å¤¾ä¸¦æ‰¹æ¬¡æ¸…ç†
4. ä¿æŒåŸæœ‰çš„æ‰¹æ¬¡æ¸…ç†æ‰€æœ‰åˆ†é¡æª”æ¡ˆåŠŸèƒ½

é…åˆæ–°ç‰ˆ data_loader.pyï¼Œæ”¯æ´æ‰¹æ¬¡æ¸…ç†æ‰€æœ‰åˆ†é¡æª”æ¡ˆï¼š
- è‡ªå‹•åµæ¸¬ data/processed/YYYY-MM-DD/ ä¸­çš„æª”æ¡ˆ
- æ‰¹æ¬¡æ¸…ç†ä¸¦ä¿å­˜åˆ° data/cleaned/YYYY-MM-DD/
- ä¿æŒåŸæœ‰æª”æ¡ˆçµæ§‹å’Œå‘½å
- ç”Ÿæˆå®Œæ•´çš„æ¸…ç†å ±å‘Š
"""

import pandas as pd
import numpy as np
import os
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional

class VDBatchDataCleaner:
    """
    VDæ‰¹æ¬¡æ•¸æ“šæ¸…ç†å™¨ - æ—¥æœŸè³‡æ–™å¤¾çµ„ç¹”ç‰ˆ
    
    æ”¯æ´æŒ‰æ—¥æœŸçµ„ç¹”çš„æ‰¹æ¬¡æ¸…ç†æ‰€æœ‰åˆ†é¡æª”æ¡ˆ
    """
    
    def __init__(self, base_folder: str = "data"):
        """
        åˆå§‹åŒ–æ‰¹æ¬¡æ¸…ç†å™¨
        
        Args:
            base_folder: åŸºç¤è³‡æ–™å¤¾è·¯å¾‘
        """
        self.base_folder = Path(base_folder)
        self.processed_base_folder = self.base_folder / "processed"
        self.cleaned_base_folder = self.base_folder / "cleaned"
        
        # ç¢ºä¿ cleaned åŸºç¤è³‡æ–™å¤¾å­˜åœ¨
        self.cleaned_base_folder.mkdir(parents=True, exist_ok=True)
        
        # å®šç¾©ç•°å¸¸å€¼æ¨™è¨˜
        self.invalid_markers = [-99, -1, 999, 9999]
        
        # å®šç¾©åˆç†çš„æ•¸å€¼ç¯„åœ
        self.valid_ranges = {
            'speed': (0, 150),        # é€Ÿåº¦ï¼š0-150 km/h
            'occupancy': (0, 100),    # ä½”æœ‰ç‡ï¼š0-100%
            'volume_total': (0, 100), # æµé‡ï¼š0-100è¼›
            'volume_small': (0, 100),
            'volume_large': (0, 50),
            'volume_truck': (0, 50),
            'speed_small': (0, 150),
            'speed_large': (0, 150),
            'speed_truck': (0, 150)
        }
        
        # å®šç¾©è¦æ¸…ç†çš„æª”æ¡ˆæ˜ å°„ï¼ˆç›¸å°æ–¼æ—¥æœŸè³‡æ–™å¤¾ï¼‰
        self.file_mappings = {
            'all': {
                'input_csv': "vd_data_all.csv",
                'input_json': "vd_data_all_summary.json",
                'output_csv': "vd_data_all_cleaned.csv",
                'output_json': "vd_data_all_cleaned_summary.json",
                'description': "å…¨éƒ¨VDè³‡æ–™"
            },
            'peak': {
                'input_csv': "vd_data_peak.csv",
                'input_json': "vd_data_peak_summary.json",
                'output_csv': "vd_data_peak_cleaned.csv",
                'output_json': "vd_data_peak_cleaned_summary.json",
                'description': "æ‰€æœ‰å°–å³°æ™‚æ®µæ•¸æ“š"
            },
            'offpeak': {
                'input_csv': "vd_data_offpeak.csv",
                'input_json': "vd_data_offpeak_summary.json",
                'output_csv': "vd_data_offpeak_cleaned.csv",
                'output_json': "vd_data_offpeak_cleaned_summary.json",
                'description': "æ‰€æœ‰é›¢å³°æ™‚æ®µæ•¸æ“š"
            },
            'target_route': {
                'input_csv': "target_route_data.csv",
                'input_json': "target_route_data_summary.json",
                'output_csv': "target_route_data_cleaned.csv",
                'output_json': "target_route_data_cleaned_summary.json",
                'description': "ç›®æ¨™è·¯æ®µæ•¸æ“š"
            },
            'target_route_peak': {
                'input_csv': "target_route_peak.csv",
                'input_json': "target_route_peak_summary.json",
                'output_csv': "target_route_peak_cleaned.csv",
                'output_json': "target_route_peak_cleaned_summary.json",
                'description': "ç›®æ¨™è·¯æ®µå°–å³°"
            },
            'target_route_offpeak': {
                'input_csv': "target_route_offpeak.csv",
                'input_json': "target_route_offpeak_summary.json",
                'output_csv': "target_route_offpeak_cleaned.csv",
                'output_json': "target_route_offpeak_cleaned_summary.json",
                'description': "ç›®æ¨™è·¯æ®µé›¢å³°"
            }
        }
        
        print("ğŸ§¹ VDæ‰¹æ¬¡æ•¸æ“šæ¸…ç†å™¨æ—¥æœŸçµ„ç¹”ç‰ˆåˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“ è¼¸å…¥åŸºç¤è³‡æ–™å¤¾: {self.processed_base_folder}")
        print(f"   ğŸ“ è¼¸å‡ºåŸºç¤è³‡æ–™å¤¾: {self.cleaned_base_folder}")
        print(f"   ğŸ—‚ï¸ æ—¥æœŸçµ„ç¹”: YYYY-MM-DD/")
        print(f"   ğŸ“Š é è¨ˆæ¸…ç†æª”æ¡ˆ: {len(self.file_mappings)} ç¨®")
    
    def detect_available_date_folders(self) -> Dict[str, Dict[str, Any]]:
        """æª¢æ¸¬å¯ç”¨çš„æ—¥æœŸè³‡æ–™å¤¾"""
        print("ğŸ” æª¢æ¸¬å¯ç”¨çš„æ—¥æœŸè³‡æ–™å¤¾...")
        
        available_date_folders = {}
        
        if not self.processed_base_folder.exists():
            print(f"   âŒ è™•ç†è³‡æ–™å¤¾ä¸å­˜åœ¨: {self.processed_base_folder}")
            return available_date_folders
        
        # æƒææ—¥æœŸè³‡æ–™å¤¾
        for date_folder in self.processed_base_folder.iterdir():
            if date_folder.is_dir() and date_folder.name.count('-') == 2:  # YYYY-MM-DD æ ¼å¼
                date_str = date_folder.name
                
                # æª¢æŸ¥è©²æ—¥æœŸè³‡æ–™å¤¾ä¸­çš„æª”æ¡ˆ
                available_files = self._detect_files_in_date_folder(date_folder, date_str)
                
                if available_files:
                    available_date_folders[date_str] = {
                        'folder_path': date_folder,
                        'available_files': available_files,
                        'file_count': len(available_files)
                    }
                    
                    print(f"   âœ… {date_str}: æ‰¾åˆ° {len(available_files)} å€‹å¯æ¸…ç†æª”æ¡ˆ")
                else:
                    print(f"   âš ï¸ {date_str}: æ²’æœ‰å¯æ¸…ç†æª”æ¡ˆ")
        
        print(f"\nğŸ“Š æª¢æ¸¬çµæœ: æ‰¾åˆ° {len(available_date_folders)} å€‹æ—¥æœŸè³‡æ–™å¤¾")
        return available_date_folders
    
    def _detect_files_in_date_folder(self, date_folder: Path, date_str: str) -> Dict[str, Dict[str, Any]]:
        """æª¢æ¸¬ç‰¹å®šæ—¥æœŸè³‡æ–™å¤¾ä¸­çš„æª”æ¡ˆ"""
        available_files = {}
        
        for name, mapping in self.file_mappings.items():
            input_csv = date_folder / mapping['input_csv']
            description = mapping['description']
            
            if input_csv.exists():
                try:
                    # å¿«é€Ÿæª¢æŸ¥æª”æ¡ˆ
                    df = pd.read_csv(input_csv, nrows=5)
                    file_size = input_csv.stat().st_size / 1024 / 1024  # MB
                    
                    # ä¼°ç®—ç¸½è¨˜éŒ„æ•¸
                    estimated_records = int(file_size * 1000)
                    
                    available_files[name] = {
                        'input_path': input_csv,
                        'description': description,
                        'file_size_mb': round(file_size, 1),
                        'estimated_records': estimated_records,
                        'columns': len(df.columns),
                        'status': 'ready'
                    }
                    
                except Exception as e:
                    available_files[name] = {
                        'input_path': input_csv,
                        'description': description,
                        'status': 'error',
                        'error': str(e)
                    }
        
        return available_files
    
    def clean_single_date_folder(self, date_str: str, date_info: Dict[str, Any], method: str = 'mark_nan') -> Dict[str, Any]:
        """æ¸…ç†å–®ä¸€æ—¥æœŸè³‡æ–™å¤¾"""
        
        print(f"ğŸ“… æ¸…ç† {date_str} è³‡æ–™å¤¾...")
        
        date_folder = date_info['folder_path']
        available_files = date_info['available_files']
        
        # å»ºç«‹å°æ‡‰çš„æ¸…ç†æ—¥æœŸè³‡æ–™å¤¾
        cleaned_date_folder = self.cleaned_base_folder / date_str
        cleaned_date_folder.mkdir(parents=True, exist_ok=True)
        
        cleaning_results = []
        successful_cleanings = 0
        failed_cleanings = 0
        
        # æ¸…ç†è©²æ—¥æœŸè³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰æª”æ¡ˆ
        for name, file_info in available_files.items():
            if file_info.get('status') == 'ready':
                mapping = self.file_mappings[name]
                
                # å»ºç«‹å®Œæ•´çš„æª”æ¡ˆè·¯å¾‘
                full_mapping = {
                    'input_csv': date_folder / mapping['input_csv'],
                    'input_json': date_folder / mapping['input_json'],
                    'output_csv': cleaned_date_folder / mapping['output_csv'],
                    'output_json': cleaned_date_folder / mapping['output_json'],
                    'description': f"{date_str} {mapping['description']}"
                }
                
                result = self.clean_single_file(name, full_mapping, method)
                cleaning_results.append(result)
                
                if result['success']:
                    successful_cleanings += 1
                else:
                    failed_cleanings += 1
            else:
                failed_cleanings += 1
                cleaning_results.append({
                    'name': name,
                    'description': file_info['description'],
                    'success': False,
                    'error': file_info.get('error', 'æª”æ¡ˆä¸å¯ç”¨')
                })
        
        # ç”Ÿæˆè©²æ—¥æœŸçš„æ¸…ç†å ±å‘Š
        date_report = {
            'date': date_str,
            'cleaned_folder': str(cleaned_date_folder),
            'total_files': len(available_files),
            'successful_cleanings': successful_cleanings,
            'failed_cleanings': failed_cleanings,
            'success_rate': f"{(successful_cleanings / len(available_files) * 100):.1f}%",
            'cleaning_results': cleaning_results
        }
        
        # ä¿å­˜è©²æ—¥æœŸçš„æ¸…ç†å ±å‘Š
        date_report_path = cleaned_date_folder / "date_cleaning_report.json"
        with open(date_report_path, 'w', encoding='utf-8') as f:
            json.dump(date_report, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"   âœ… {date_str}: æˆåŠŸæ¸…ç† {successful_cleanings}/{len(available_files)} å€‹æª”æ¡ˆ")
        
        return date_report
    
    def clean_single_file(self, name: str, mapping: Dict[str, Any], method: str = 'mark_nan') -> Dict[str, Any]:
        """æ¸…ç†å–®ä¸€æª”æ¡ˆ"""
        
        input_csv = mapping['input_csv']
        output_csv = mapping['output_csv']
        output_json = mapping['output_json']
        description = mapping['description']
        
        try:
            # è¼‰å…¥æ•¸æ“š
            df_original = pd.read_csv(input_csv)
            
            # æ•¸æ“šé¡å‹æœ€ä½³åŒ–
            df_original = self._optimize_data_types(df_original)
            
            # è­˜åˆ¥ç„¡æ•ˆæ•¸æ“š
            invalid_stats = self._identify_invalid_data_quick(df_original)
            
            # æ¸…ç†æ•¸æ“š
            df_cleaned = self._clean_invalid_values(df_original, method)
            
            # ä¿å­˜æ¸…ç†å¾Œæ•¸æ“š
            df_cleaned.to_csv(output_csv, index=False, encoding='utf-8-sig')
            
            # ç”Ÿæˆæ‘˜è¦
            summary = self._generate_cleaned_summary(df_cleaned, name, description, invalid_stats)
            with open(output_json, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2, default=str)
            
            # è¨ˆç®—æ¸…ç†æ•ˆæœ
            cleaning_result = {
                'name': name,
                'description': description,
                'success': True,
                'original_records': len(df_original),
                'cleaned_records': len(df_cleaned),
                'records_removed': len(df_original) - len(df_cleaned),
                'invalid_values_found': invalid_stats['total_invalid'],
                'invalid_percentage': invalid_stats['invalid_percentage'],
                'output_files': {
                    'csv': str(output_csv),
                    'json': str(output_json)
                }
            }
            
            return cleaning_result
            
        except Exception as e:
            return {
                'name': name,
                'description': description,
                'success': False,
                'error': str(e)
            }
    
    def _optimize_data_types(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¿«é€Ÿæœ€ä½³åŒ–æ•¸æ“šé¡å‹"""
        if df.empty:
            return df
        
        # æ™‚é–“é¡å‹
        time_columns = ['date', 'update_time']
        for col in time_columns:
            if col in df.columns and not pd.api.types.is_datetime64_any_dtype(df[col]):
                df[col] = pd.to_datetime(df[col], errors='coerce')
        
        # æ•¸å€¼é¡å‹
        numeric_columns = [col for col in self.valid_ranges.keys() if col in df.columns]
        for col in numeric_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        return df
    
    def _identify_invalid_data_quick(self, df: pd.DataFrame) -> Dict[str, Any]:
        """å¿«é€Ÿè­˜åˆ¥ç„¡æ•ˆæ•¸æ“š"""
        
        numeric_columns = [col for col in self.valid_ranges.keys() if col in df.columns]
        
        total_invalid = 0
        invalid_by_column = {}
        
        for col in numeric_columns:
            # ç•°å¸¸æ¨™è¨˜
            invalid_markers_count = df[col].isin(self.invalid_markers).sum()
            
            # è¶…å‡ºç¯„åœ
            min_val, max_val = self.valid_ranges[col]
            out_of_range_count = ((df[col] < min_val) | (df[col] > max_val)).sum()
            
            col_invalid = invalid_markers_count + out_of_range_count
            total_invalid += col_invalid
            
            if col_invalid > 0:
                invalid_by_column[col] = {
                    'invalid_markers': int(invalid_markers_count),
                    'out_of_range': int(out_of_range_count),
                    'total': int(col_invalid)
                }
        
        return {
            'total_invalid': int(total_invalid),
            'invalid_percentage': round(total_invalid / (len(df) * len(numeric_columns)) * 100, 2),
            'columns_with_issues': len(invalid_by_column),
            'by_column': invalid_by_column
        }
    
    def _clean_invalid_values(self, df: pd.DataFrame, method: str = 'mark_nan') -> pd.DataFrame:
        """å¿«é€Ÿæ¸…ç†ç„¡æ•ˆå€¼"""
        
        df_cleaned = df.copy()
        numeric_columns = [col for col in self.valid_ranges.keys() if col in df_cleaned.columns]
        
        if method == 'mark_nan':
            for col in numeric_columns:
                # æ›¿æ›ç•°å¸¸æ¨™è¨˜
                mask = df_cleaned[col].isin(self.invalid_markers)
                df_cleaned.loc[mask, col] = np.nan
                
                # æ›¿æ›è¶…å‡ºç¯„åœçš„å€¼
                if col in self.valid_ranges:
                    min_val, max_val = self.valid_ranges[col]
                    out_of_range_mask = (df_cleaned[col] < min_val) | (df_cleaned[col] > max_val)
                    df_cleaned.loc[out_of_range_mask, col] = np.nan
        
        elif method == 'remove_rows':
            for col in numeric_columns:
                # åˆªé™¤ç•°å¸¸æ¨™è¨˜çš„è¡Œ
                df_cleaned = df_cleaned[~df_cleaned[col].isin(self.invalid_markers)]
                
                # åˆªé™¤è¶…å‡ºç¯„åœçš„è¡Œ
                if col in self.valid_ranges:
                    min_val, max_val = self.valid_ranges[col]
                    df_cleaned = df_cleaned[
                        (df_cleaned[col] >= min_val) & (df_cleaned[col] <= max_val)
                    ]
        
        return df_cleaned
    
    def _generate_cleaned_summary(self, df: pd.DataFrame, category: str, description: str, 
                                 invalid_stats: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ¸…ç†å¾Œæ‘˜è¦"""
        
        if df.empty:
            return {"category": category, "description": description, "error": "æ¸…ç†å¾Œç„¡æ•¸æ“š"}
        
        numeric_columns = [col for col in self.valid_ranges.keys() if col in df.columns]
        
        # è¨ˆç®—æ•¸æ“šå®Œæ•´åº¦
        completeness = {}
        for col in numeric_columns:
            if col in df.columns:
                total_values = len(df)
                missing_values = df[col].isnull().sum()
                completeness[col] = {
                    "å®Œæ•´åº¦": f"{((total_values - missing_values) / total_values * 100):.1f}%",
                    "ç¼ºå¤±å€¼": int(missing_values),
                    "æœ‰æ•ˆå€¼": int(total_values - missing_values)
                }
        
        summary = {
            "category": category,
            "description": description,
            "æ¸…ç†æ™‚é–“": datetime.now().isoformat(),
            "åŸºæœ¬è³‡è¨Š": {
                "è¨˜éŒ„æ•¸": len(df),
                "æ¬„ä½æ•¸": len(df.columns),
                "VDè¨­å‚™æ•¸": int(df['vd_id'].nunique()) if 'vd_id' in df.columns else 0
            },
            "æ¸…ç†æ•ˆæœ": {
                "åŸå§‹ç„¡æ•ˆå€¼": invalid_stats['total_invalid'],
                "ç„¡æ•ˆå€¼æ¯”ä¾‹": f"{invalid_stats['invalid_percentage']:.2f}%",
                "å•é¡Œæ¬„ä½æ•¸": invalid_stats['columns_with_issues']
            },
            "æ•¸æ“šå®Œæ•´åº¦": completeness,
            "äº¤é€šçµ±è¨ˆ": {}
        }
        
        # äº¤é€šçµ±è¨ˆ
        if 'speed' in df.columns:
            summary["äº¤é€šçµ±è¨ˆ"]["å¹³å‡é€Ÿåº¦"] = round(df['speed'].mean(), 1)
            summary["äº¤é€šçµ±è¨ˆ"]["æœ€é«˜é€Ÿåº¦"] = int(df['speed'].max())
        
        if 'occupancy' in df.columns:
            summary["äº¤é€šçµ±è¨ˆ"]["å¹³å‡ä½”æœ‰ç‡"] = round(df['occupancy'].mean(), 1)
            summary["äº¤é€šçµ±è¨ˆ"]["æœ€é«˜ä½”æœ‰ç‡"] = int(df['occupancy'].max())
        
        if 'volume_total' in df.columns:
            summary["äº¤é€šçµ±è¨ˆ"]["å¹³å‡æµé‡"] = round(df['volume_total'].mean(), 1)
            summary["äº¤é€šçµ±è¨ˆ"]["æœ€é«˜æµé‡"] = int(df['volume_total'].max())
        
        # æ™‚é–“ç¯„åœ
        if 'date' in df.columns:
            summary["æ™‚é–“ç¯„åœ"] = {
                "é–‹å§‹": str(df['date'].min())[:10],
                "çµæŸ": str(df['date'].max())[:10],
                "å¤©æ•¸": df['date'].nunique()
            }
        
        return summary
    
    def clean_all_date_folders(self, method: str = 'mark_nan') -> Dict[str, Any]:
        """æ‰¹æ¬¡æ¸…ç†æ‰€æœ‰æ—¥æœŸè³‡æ–™å¤¾"""
        
        print("ğŸš€ é–‹å§‹æ‰¹æ¬¡æ¸…ç†æ‰€æœ‰æ—¥æœŸè³‡æ–™å¤¾")
        print("=" * 60)
        
        start_time = datetime.now()
        
        # æª¢æ¸¬å¯ç”¨æ—¥æœŸè³‡æ–™å¤¾
        available_date_folders = self.detect_available_date_folders()
        
        if not available_date_folders:
            print("âŒ æ²’æœ‰æ‰¾åˆ°å¯æ¸…ç†çš„æ—¥æœŸè³‡æ–™å¤¾")
            return {"success": False, "error": "ç„¡å¯æ¸…ç†æ—¥æœŸè³‡æ–™å¤¾"}
        
        print(f"\nğŸ§¹ é–‹å§‹æ¸…ç† {len(available_date_folders)} å€‹æ—¥æœŸè³‡æ–™å¤¾...")
        print(f"   æ¸…ç†æ–¹æ³•: {method}")
        
        # æ‰¹æ¬¡æ¸…ç†å„æ—¥æœŸè³‡æ–™å¤¾
        date_cleaning_results = []
        successful_dates = 0
        failed_dates = 0
        total_successful_files = 0
        total_failed_files = 0
        
        for date_str, date_info in sorted(available_date_folders.items()):
            try:
                date_result = self.clean_single_date_folder(date_str, date_info, method)
                date_cleaning_results.append(date_result)
                
                if date_result['successful_cleanings'] > 0:
                    successful_dates += 1
                    total_successful_files += date_result['successful_cleanings']
                else:
                    failed_dates += 1
                
                total_failed_files += date_result['failed_cleanings']
                
            except Exception as e:
                print(f"   âŒ {date_str}: æ¸…ç†å¤±æ•— - {e}")
                failed_dates += 1
                date_cleaning_results.append({
                    'date': date_str,
                    'success': False,
                    'error': str(e)
                })
        
        # ç”Ÿæˆæ‰¹æ¬¡æ¸…ç†å ±å‘Š
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        batch_report = {
            'æ‰¹æ¬¡æ¸…ç†å…ƒæ•¸æ“š': {
                'æ¸…ç†æ™‚é–“': start_time.isoformat(),
                'å®Œæˆæ™‚é–“': end_time.isoformat(),
                'è€—æ™‚ç§’æ•¸': round(duration, 2),
                'æ¸…ç†æ–¹æ³•': method
            },
            'æ¸…ç†çµ±è¨ˆ': {
                'ç¸½æ—¥æœŸè³‡æ–™å¤¾æ•¸': len(available_date_folders),
                'æˆåŠŸæ¸…ç†æ—¥æœŸæ•¸': successful_dates,
                'å¤±æ•—æ¸…ç†æ—¥æœŸæ•¸': failed_dates,
                'ç¸½æˆåŠŸæª”æ¡ˆæ•¸': total_successful_files,
                'ç¸½å¤±æ•—æª”æ¡ˆæ•¸': total_failed_files,
                'æ—¥æœŸæˆåŠŸç‡': f"{(successful_dates / len(available_date_folders) * 100):.1f}%"
            },
            'å„æ—¥æœŸæ¸…ç†çµæœ': date_cleaning_results,
            'è¼¸å‡ºåŸºç¤ç›®éŒ„': str(self.cleaned_base_folder)
        }
        
        # ä¿å­˜æ‰¹æ¬¡å ±å‘Š
        report_path = self.cleaned_base_folder / "batch_date_cleaning_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(batch_report, f, ensure_ascii=False, indent=2, default=str)
        
        # é¡¯ç¤ºçµæœ
        print(f"\nğŸ æ‰¹æ¬¡æ¸…ç†å®Œæˆ")
        print("=" * 60)
        print(f"ğŸ“Š æ¸…ç†çµ±è¨ˆ:")
        print(f"   â±ï¸ è€—æ™‚: {duration:.1f} ç§’")
        print(f"   ğŸ“… æˆåŠŸæ—¥æœŸ: {successful_dates}/{len(available_date_folders)}")
        print(f"   ğŸ“„ æˆåŠŸæª”æ¡ˆ: {total_successful_files}")
        print(f"   âŒ å¤±æ•—æª”æ¡ˆ: {total_failed_files}")
        print(f"   ğŸ“ˆ æ—¥æœŸæˆåŠŸç‡: {(successful_dates / len(available_date_folders) * 100):.1f}%")
        
        print(f"\nğŸ“ æ¸…ç†å¾Œçµæ§‹:")
        for date_str in sorted(available_date_folders.keys()):
            cleaned_date_folder = self.cleaned_base_folder / date_str
            if cleaned_date_folder.exists():
                cleaned_files = list(cleaned_date_folder.glob("*.csv"))
                print(f"   ğŸ“‚ data/cleaned/{date_str}/ ({len(cleaned_files)} å€‹CSVæª”æ¡ˆ)")
        
        print(f"\nğŸ“„ å ±å‘Šæª”æ¡ˆ: {report_path}")
        
        return batch_report
    
    def get_cleaned_files_summary(self) -> Dict[str, Any]:
        """ç²å–æ¸…ç†å¾Œæª”æ¡ˆæ‘˜è¦"""
        
        print("ğŸ“Š æª¢æŸ¥æ¸…ç†å¾Œæª”æ¡ˆç‹€æ³...")
        
        summary = {
            'æ¸…ç†æª”æ¡ˆçµ±è¨ˆ': {
                'æ¸…ç†æ—¥æœŸæ•¸': 0,
                'å­˜åœ¨æª”æ¡ˆæ•¸': 0,
                'ç¸½è¨˜éŒ„æ•¸': 0,
                'æª”æ¡ˆå¤§å°_MB': 0
            },
            'å„æ—¥æœŸè©³æƒ…': {}
        }
        
        if not self.cleaned_base_folder.exists():
            print(f"   âš ï¸ æ¸…ç†è³‡æ–™å¤¾ä¸å­˜åœ¨: {self.cleaned_base_folder}")
            return summary
        
        # æƒææ¸…ç†å¾Œçš„æ—¥æœŸè³‡æ–™å¤¾
        for cleaned_date_folder in self.cleaned_base_folder.iterdir():
            if cleaned_date_folder.is_dir() and cleaned_date_folder.name.count('-') == 2:
                date_str = cleaned_date_folder.name
                
                # çµ±è¨ˆè©²æ—¥æœŸè³‡æ–™å¤¾ä¸­çš„æª”æ¡ˆ
                date_summary = self._get_date_folder_summary(cleaned_date_folder, date_str)
                
                if date_summary['æª”æ¡ˆæ•¸'] > 0:
                    summary['å„æ—¥æœŸè©³æƒ…'][date_str] = date_summary
                    summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['æ¸…ç†æ—¥æœŸæ•¸'] += 1
                    summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['å­˜åœ¨æª”æ¡ˆæ•¸'] += date_summary['æª”æ¡ˆæ•¸']
                    summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['ç¸½è¨˜éŒ„æ•¸'] += date_summary['ç¸½è¨˜éŒ„æ•¸']
                    summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['æª”æ¡ˆå¤§å°_MB'] += date_summary['ç¸½æª”æ¡ˆå¤§å°_MB']
                    
                    print(f"   âœ… {date_str}: {date_summary['æª”æ¡ˆæ•¸']} æª”æ¡ˆ ({date_summary['ç¸½æª”æ¡ˆå¤§å°_MB']:.1f}MB)")
        
        # å››æ¨äº”å…¥çµ±è¨ˆ
        summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['æª”æ¡ˆå¤§å°_MB'] = round(summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['æª”æ¡ˆå¤§å°_MB'], 1)
        
        print(f"\nğŸ“ˆ ç¸½è¨ˆ:")
        print(f"   ğŸ“… æ¸…ç†æ—¥æœŸæ•¸: {summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['æ¸…ç†æ—¥æœŸæ•¸']}")
        print(f"   ğŸ“„ ç¸½æª”æ¡ˆæ•¸: {summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['å­˜åœ¨æª”æ¡ˆæ•¸']}")
        print(f"   ğŸ“Š ç¸½è¨˜éŒ„æ•¸: {summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['ç¸½è¨˜éŒ„æ•¸']:,}")
        print(f"   ğŸ’¾ ç¸½æª”æ¡ˆå¤§å°: {summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['æª”æ¡ˆå¤§å°_MB']:.1f} MB")
        
        return summary
    
    def _get_date_folder_summary(self, cleaned_date_folder: Path, date_str: str) -> Dict[str, Any]:
        """ç²å–ç‰¹å®šæ—¥æœŸè³‡æ–™å¤¾çš„æ‘˜è¦"""
        
        date_summary = {
            'æ—¥æœŸ': date_str,
            'æª”æ¡ˆæ•¸': 0,
            'ç¸½è¨˜éŒ„æ•¸': 0,
            'ç¸½æª”æ¡ˆå¤§å°_MB': 0,
            'æª”æ¡ˆè©³æƒ…': {}
        }
        
        # æª¢æŸ¥è©²æ—¥æœŸè³‡æ–™å¤¾ä¸­çš„æ¸…ç†æª”æ¡ˆ
        for name, mapping in self.file_mappings.items():
            output_csv = cleaned_date_folder / mapping['output_csv']
            output_json = cleaned_date_folder / mapping['output_json']
            description = mapping['description']
            
            if output_csv.exists():
                try:
                    df = pd.read_csv(output_csv, low_memory=False)
                    file_size_mb = output_csv.stat().st_size / 1024 / 1024
                    
                    date_summary['æª”æ¡ˆæ•¸'] += 1
                    date_summary['ç¸½è¨˜éŒ„æ•¸'] += len(df)
                    date_summary['ç¸½æª”æ¡ˆå¤§å°_MB'] += file_size_mb
                    
                    # è®€å–JSONæ‘˜è¦
                    json_summary = {}
                    if output_json.exists():
                        with open(output_json, 'r', encoding='utf-8') as f:
                            json_summary = json.load(f)
                    
                    date_summary['æª”æ¡ˆè©³æƒ…'][name] = {
                        'æè¿°': description,
                        'è¨˜éŒ„æ•¸': len(df),
                        'æª”æ¡ˆå¤§å°_MB': round(file_size_mb, 1),
                        'CSVè·¯å¾‘': str(output_csv),
                        'JSONè·¯å¾‘': str(output_json),
                        'æ‘˜è¦': json_summary.get('äº¤é€šçµ±è¨ˆ', {})
                    }
                    
                except Exception as e:
                    print(f"      âŒ {date_str} {description}: è®€å–å¤±æ•— - {e}")
        
        # å››æ¨äº”å…¥
        date_summary['ç¸½æª”æ¡ˆå¤§å°_MB'] = round(date_summary['ç¸½æª”æ¡ˆå¤§å°_MB'], 1)
        
        return date_summary
    
    def list_available_cleaned_dates(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨çš„æ¸…ç†æ—¥æœŸè³‡æ–™å¤¾"""
        available_dates = []
        
        if self.cleaned_base_folder.exists():
            for cleaned_date_folder in self.cleaned_base_folder.iterdir():
                if cleaned_date_folder.is_dir() and cleaned_date_folder.name.count('-') == 2:
                    available_dates.append(cleaned_date_folder.name)
        
        return sorted(available_dates)
    
    def load_cleaned_data_by_date(self, target_date: str) -> Dict[str, pd.DataFrame]:
        """è¼‰å…¥ç‰¹å®šæ—¥æœŸçš„æ¸…ç†æ•¸æ“š"""
        print(f"ğŸ“… è¼‰å…¥ {target_date} æ¸…ç†æ•¸æ“š...")
        
        cleaned_date_folder = self.cleaned_base_folder / target_date
        
        if not cleaned_date_folder.exists():
            print(f"   âŒ æ¸…ç†æ—¥æœŸè³‡æ–™å¤¾ä¸å­˜åœ¨: {target_date}")
            return {}
        
        cleaned_data = {}
        
        for name, mapping in self.file_mappings.items():
            output_csv = cleaned_date_folder / mapping['output_csv']
            description = mapping['description']
            
            if output_csv.exists():
                try:
                    df = pd.read_csv(output_csv)
                    df = self._optimize_data_types(df)
                    cleaned_data[name] = df
                    print(f"   âœ… {description}: {len(df):,} ç­†è¨˜éŒ„")
                except Exception as e:
                    print(f"   âŒ {description}: è¼‰å…¥å¤±æ•— - {e}")
                    cleaned_data[name] = pd.DataFrame()
            else:
                cleaned_data[name] = pd.DataFrame()
        
        return cleaned_data


# ä¾¿åˆ©å‡½æ•¸
def clean_all_vd_data_by_date(base_folder: str = "data", method: str = 'mark_nan') -> Dict[str, Any]:
    """
    ä¸€éµæ¸…ç†æ‰€æœ‰VDæ•¸æ“šåˆ†é¡æª”æ¡ˆï¼ˆæŒ‰æ—¥æœŸçµ„ç¹”ï¼‰
    
    Args:
        base_folder: åŸºç¤è³‡æ–™å¤¾è·¯å¾‘
        method: æ¸…ç†æ–¹æ³•
    
    Returns:
        æ‰¹æ¬¡æ¸…ç†å ±å‘Š
    """
    cleaner = VDBatchDataCleaner(base_folder)
    return cleaner.clean_all_date_folders(method)


def get_cleaned_data_summary_by_date(base_folder: str = "data") -> Dict[str, Any]:
    """
    ç²å–æ¸…ç†å¾Œæ•¸æ“šæ‘˜è¦ï¼ˆæŒ‰æ—¥æœŸçµ„ç¹”ï¼‰
    
    Args:
        base_folder: åŸºç¤è³‡æ–™å¤¾è·¯å¾‘
    
    Returns:
        æ¸…ç†æª”æ¡ˆæ‘˜è¦
    """
    cleaner = VDBatchDataCleaner(base_folder)
    return cleaner.get_cleaned_files_summary()


def load_cleaned_data_by_date(base_folder: str = "data", target_date: str = None) -> Dict[str, pd.DataFrame]:
    """
    è¼‰å…¥ç‰¹å®šæ—¥æœŸçš„æ¸…ç†æ•¸æ“š
    
    Args:
        base_folder: åŸºç¤è³‡æ–™å¤¾è·¯å¾‘
        target_date: ç›®æ¨™æ—¥æœŸ (YYYY-MM-DD)
    
    Returns:
        æ¸…ç†æ•¸æ“šå­—å…¸
    """
    cleaner = VDBatchDataCleaner(base_folder)
    return cleaner.load_cleaned_data_by_date(target_date)


if __name__ == "__main__":
    print("ğŸ§¹ VDæ‰¹æ¬¡æ•¸æ“šæ¸…ç†å™¨ - æ—¥æœŸçµ„ç¹”ç‰ˆ")
    print("=" * 50)
    
    cleaner = VDBatchDataCleaner()
    
    # æª¢æ¸¬å¯ç”¨æ—¥æœŸè³‡æ–™å¤¾
    available_date_folders = cleaner.detect_available_date_folders()
    
    if available_date_folders:
        print(f"\nç™¼ç¾ {len(available_date_folders)} å€‹æ—¥æœŸè³‡æ–™å¤¾å¯æ¸…ç†")
        
        # é¡¯ç¤ºå„æ—¥æœŸè©³æƒ…
        for date_str, date_info in sorted(available_date_folders.items()):
            print(f"   ğŸ“… {date_str}: {date_info['file_count']} å€‹æª”æ¡ˆ")
        
        response = input(f"\né–‹å§‹æ‰¹æ¬¡æ¸…ç†æ‰€æœ‰æ—¥æœŸè³‡æ–™å¤¾ï¼Ÿ(y/N): ")
        
        if response.lower() in ['y', 'yes']:
            # åŸ·è¡Œæ‰¹æ¬¡æ¸…ç†
            report = cleaner.clean_all_date_folders()
            
            if report['æ¸…ç†çµ±è¨ˆ']['æˆåŠŸæ¸…ç†æ—¥æœŸæ•¸'] > 0:
                print(f"\nğŸ‰ æ‰¹æ¬¡æ¸…ç†å®Œæˆï¼")
                print(f"âœ… æˆåŠŸæ¸…ç† {report['æ¸…ç†çµ±è¨ˆ']['æˆåŠŸæ¸…ç†æ—¥æœŸæ•¸']} å€‹æ—¥æœŸè³‡æ–™å¤¾")
                print(f"ğŸ“ æ¸…ç†å¾Œæª”æ¡ˆä¿å­˜åœ¨: {cleaner.cleaned_base_folder}")
                
                # é¡¯ç¤ºæ¸…ç†å¾Œæ‘˜è¦
                summary = cleaner.get_cleaned_files_summary()
                print(f"\nğŸ“Š æ¸…ç†å¾Œç¸½è¨ˆ:")
                print(f"   ğŸ“… æ¸…ç†æ—¥æœŸæ•¸: {summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['æ¸…ç†æ—¥æœŸæ•¸']}")
                print(f"   ğŸ“„ ç¸½æª”æ¡ˆæ•¸: {summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['å­˜åœ¨æª”æ¡ˆæ•¸']}")
                print(f"   ğŸ“Š ç¸½è¨˜éŒ„æ•¸: {summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['ç¸½è¨˜éŒ„æ•¸']:,}")
                print(f"   ğŸ’¾ ç¸½æª”æ¡ˆå¤§å°: {summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['æª”æ¡ˆå¤§å°_MB']:.1f} MB")
                
                print(f"\nğŸ“ è¼¸å‡ºçµæ§‹:")
                for date_str in cleaner.list_available_cleaned_dates():
                    print(f"   ğŸ“‚ data/cleaned/{date_str}/")
                    print(f"      â”œâ”€â”€ vd_data_all_cleaned.csv + .json")
                    print(f"      â”œâ”€â”€ vd_data_peak_cleaned.csv + .json")
                    print(f"      â”œâ”€â”€ vd_data_offpeak_cleaned.csv + .json")
                    print(f"      â”œâ”€â”€ target_route_*_cleaned.csv + .json")
                    print(f"      â””â”€â”€ date_cleaning_report.json")
            else:
                print("âŒ æ‰¹æ¬¡æ¸…ç†å¤±æ•—")
    else:
        print("\nğŸ’¡ è«‹å…ˆåŸ·è¡Œ data_loader.py ç”ŸæˆæŒ‰æ—¥æœŸçµ„ç¹”çš„åˆ†é¡æª”æ¡ˆ")
        print("é æœŸçµæ§‹:")
        print("   ğŸ“‚ data/processed/")
        print("      â”œâ”€â”€ 2025-06-27/")
        print("      â”‚   â”œâ”€â”€ vd_data_all.csv")
        print("      â”‚   â”œâ”€â”€ vd_data_peak.csv")
        print("      â”‚   â””â”€â”€ ...")
        print("      â””â”€â”€ 2025-06-26/")
        print("          â”œâ”€â”€ vd_data_all.csv")
        print("          â””â”€â”€ ...")