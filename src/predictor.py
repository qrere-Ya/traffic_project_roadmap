"""
AIäº¤é€šé æ¸¬æ¨¡çµ„ - åœ‹é“1è™Ÿåœ“å±±-ä¸‰é‡è·¯æ®µ
=====================================

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ğŸ¥‡ LSTMæ·±åº¦å­¸ç¿’æ™‚é–“åºåˆ—é æ¸¬ï¼ˆä¸»åŠ›æ¨¡å‹ï¼‰
2. ğŸ¥ˆ XGBoosté«˜ç²¾åº¦æ¢¯åº¦æå‡é æ¸¬ï¼ˆæ¬¡é¸æ¨¡å‹ï¼‰
3. ğŸ¥‰ éš¨æ©Ÿæ£®æ—åŸºç·šé æ¸¬æ¨¡å‹ï¼ˆå‚™é¸æ¨¡å‹ï¼‰
4. ğŸ”§ æ™ºèƒ½ç‰¹å¾µå·¥ç¨‹ç®¡é“
5. ğŸ“Š æ¨¡å‹è©•ä¼°èˆ‡é¸æ“‡ç³»çµ±
6. âš¡ 15åˆ†é˜æ»¾å‹•é æ¸¬åŠŸèƒ½

ç›®æ¨™ï¼šå¯¦ç¾85%ä»¥ä¸Šé æ¸¬æº–ç¢ºç‡
åŸºæ–¼ï¼š80,640ç­†AIè¨“ç·´æ•¸æ“š + 7å¤©å®Œæ•´é€±æœŸ
ä½œè€…: äº¤é€šé æ¸¬å°ˆæ¡ˆåœ˜éšŠ
æ—¥æœŸ: 2025-07-21
"""

import pandas as pd
import numpy as np
import joblib
import json
import warnings
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional, Union
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split, cross_val_score
import xgboost as xgb

# æ¢ä»¶å°å…¥æ·±åº¦å­¸ç¿’åº«
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, load_model
    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    TENSORFLOW_AVAILABLE = True
    print("âœ… TensorFlow å¯ç”¨ - LSTMæ¨¡å‹å·²å•Ÿç”¨")
except ImportError:
    TENSORFLOW_AVAILABLE = False
    print("âš ï¸ TensorFlow ä¸å¯ç”¨ - åƒ…ä½¿ç”¨XGBoostå’Œéš¨æ©Ÿæ£®æ—")

warnings.filterwarnings('ignore')


class TrafficFeatureEngineer:
    """äº¤é€šæ•¸æ“šç‰¹å¾µå·¥ç¨‹å™¨"""
    
    def __init__(self):
        self.scalers = {}
        self.encoders = {}
        self.feature_names = []
        
        print("ğŸ”§ ç‰¹å¾µå·¥ç¨‹å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def create_time_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """å‰µå»ºæ™‚é–“ç‰¹å¾µ"""
        df = df.copy()
        
        # ç¢ºä¿æ™‚é–“æ¬„ä½æ­£ç¢º
        if 'update_time' in df.columns:
            df['update_time'] = pd.to_datetime(df['update_time'])
            
            # åŸºæœ¬æ™‚é–“ç‰¹å¾µ
            df['hour'] = df['update_time'].dt.hour
            df['minute'] = df['update_time'].dt.minute
            df['weekday'] = df['update_time'].dt.weekday
            df['month'] = df['update_time'].dt.month
            df['day'] = df['update_time'].dt.day
            
            # é€±æœŸæ€§ç‰¹å¾µ
            df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
            df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
            df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)
            df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)
            
            # æ™‚æ®µåˆ†é¡
            df['time_period'] = pd.cut(df['hour'], 
                                     bins=[0, 6, 9, 17, 20, 24], 
                                     labels=['æ·±å¤œ', 'æ—©æ™¨', 'ç™½å¤©', 'å‚æ™š', 'å¤œæ™š'])
            
            # æ˜¯å¦å°–å³°æ™‚æ®µ
            df['is_peak'] = ((df['hour'].between(7, 9)) | 
                           (df['hour'].between(17, 19))).astype(int)
            
            # æ˜¯å¦é€±æœ«
            df['is_weekend'] = (df['weekday'] >= 5).astype(int)
        
        return df
    
    def create_traffic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """å‰µå»ºäº¤é€šç‰¹å¾µ"""
        df = df.copy()
        
        # åŸºæœ¬æ¯”ä¾‹ç‰¹å¾µ
        if 'volume_total' in df.columns and df['volume_total'].sum() > 0:
            df['volume_total_safe'] = df['volume_total'].fillna(0).clip(lower=0.1)  # é¿å…é™¤é›¶
            
            if 'volume_small' in df.columns:
                df['small_ratio'] = df['volume_small'].fillna(0) / df['volume_total_safe']
            if 'volume_large' in df.columns:
                df['large_ratio'] = df['volume_large'].fillna(0) / df['volume_total_safe']
            if 'volume_truck' in df.columns:
                df['truck_ratio'] = df['volume_truck'].fillna(0) / df['volume_total_safe']
        
        # é€Ÿåº¦å¯†åº¦é—œä¿‚
        if 'speed' in df.columns and 'occupancy' in df.columns:
            df['speed_density_ratio'] = df['speed'].fillna(75) / (df['occupancy'].fillna(10) + 1)
        
        # äº¤é€šæ•ˆç‡æŒ‡æ¨™
        if 'speed' in df.columns and 'volume_total' in df.columns:
            df['traffic_efficiency'] = df['speed'].fillna(75) * df['volume_total'].fillna(0)
        
        # æ“å µæŒ‡æ¨™
        if 'speed' in df.columns:
            df['congestion_level'] = pd.cut(df['speed'].fillna(75), 
                                          bins=[0, 30, 60, 90, 150], 
                                          labels=[3, 2, 1, 0])  # 3=åš´é‡æ“å µ, 0=æš¢é€š
            df['congestion_level'] = df['congestion_level'].astype(float)
        
        return df
    
    def create_lag_features(self, df: pd.DataFrame, target_cols: List[str], 
                           lags: List[int] = [1, 2, 3, 5, 12]) -> pd.DataFrame:
        """å‰µå»ºæ»¯å¾Œç‰¹å¾µ"""
        df = df.copy()
        df = df.sort_values(['vd_id', 'update_time']).reset_index(drop=True)
        
        for col in target_cols:
            if col in df.columns:
                for lag in lags:
                    df[f'{col}_lag_{lag}'] = df.groupby('vd_id')[col].shift(lag)
        
        return df
    
    def create_rolling_features(self, df: pd.DataFrame, target_cols: List[str],
                               windows: List[int] = [3, 6, 12]) -> pd.DataFrame:
        """å‰µå»ºæ»¾å‹•çµ±è¨ˆç‰¹å¾µ"""
        df = df.copy()
        df = df.sort_values(['vd_id', 'update_time']).reset_index(drop=True)
        
        for col in target_cols:
            if col in df.columns:
                for window in windows:
                    # æ»¾å‹•å¹³å‡
                    df[f'{col}_rolling_mean_{window}'] = (
                        df.groupby('vd_id')[col]
                        .rolling(window=window, min_periods=1)
                        .mean()
                        .reset_index(0, drop=True)
                    )
                    
                    # æ»¾å‹•æ¨™æº–å·®
                    df[f'{col}_rolling_std_{window}'] = (
                        df.groupby('vd_id')[col]
                        .rolling(window=window, min_periods=1)
                        .std()
                        .reset_index(0, drop=True)
                        .fillna(0)
                    )
        
        return df
    
    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ“¬åˆä¸¦è½‰æ›ç‰¹å¾µ"""
        print("ğŸ”§ åŸ·è¡Œç‰¹å¾µå·¥ç¨‹...")
        
        # å‰µå»ºå„é¡ç‰¹å¾µ
        df = self.create_time_features(df)
        df = self.create_traffic_features(df)
        
        # ç›®æ¨™æ¬„ä½
        target_cols = ['speed', 'volume_total', 'occupancy']
        available_targets = [col for col in target_cols if col in df.columns]
        
        if available_targets:
            df = self.create_lag_features(df, available_targets)
            df = self.create_rolling_features(df, available_targets)
        
        # è™•ç†é¡åˆ¥è®Šæ•¸
        categorical_cols = ['vd_id', 'time_period']
        for col in categorical_cols:
            if col in df.columns:
                if col not in self.encoders:
                    self.encoders[col] = LabelEncoder()
                    df[f'{col}_encoded'] = self.encoders[col].fit_transform(df[col].astype(str))
                else:
                    try:
                        df[f'{col}_encoded'] = self.encoders[col].transform(df[col].astype(str))
                    except ValueError:
                        # è™•ç†æœªè¦‹éçš„æ¨™ç±¤
                        df[f'{col}_encoded'] = 0
        
        # æ¨™æº–åŒ–æ•¸å€¼ç‰¹å¾µ
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        excluded_cols = ['congestion_level', 'is_peak', 'is_weekend'] + [col for col in numeric_cols if 'encoded' in col]
        
        scale_cols = [col for col in numeric_cols if col not in excluded_cols]
        
        if scale_cols:
            if 'scaler' not in self.scalers:
                self.scalers['scaler'] = StandardScaler()
                df[scale_cols] = self.scalers['scaler'].fit_transform(df[scale_cols].fillna(0))
            else:
                df[scale_cols] = self.scalers['scaler'].transform(df[scale_cols].fillna(0))
        
        # è¨˜éŒ„ç‰¹å¾µåç¨±
        self.feature_names = [col for col in df.columns if col not in ['update_time', 'date']]
        
        print(f"   âœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆï¼š{len(self.feature_names)} å€‹ç‰¹å¾µ")
        return df
    
    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        """åƒ…è½‰æ›ç‰¹å¾µï¼ˆç”¨æ–¼é æ¸¬ï¼‰"""
        return self.fit_transform(df)  # ç°¡åŒ–ç‰ˆæœ¬
    
    def save(self, filepath: str):
        """ä¿å­˜ç‰¹å¾µå·¥ç¨‹å™¨"""
        save_data = {
            'scalers': self.scalers,
            'encoders': self.encoders,
            'feature_names': self.feature_names
        }
        joblib.dump(save_data, filepath)
        print(f"âœ… ç‰¹å¾µå·¥ç¨‹å™¨å·²ä¿å­˜: {filepath}")
    
    @classmethod
    def load(cls, filepath: str):
        """è¼‰å…¥ç‰¹å¾µå·¥ç¨‹å™¨"""
        save_data = joblib.load(filepath)
        instance = cls()
        instance.scalers = save_data['scalers']
        instance.encoders = save_data['encoders']
        instance.feature_names = save_data['feature_names']
        print(f"âœ… ç‰¹å¾µå·¥ç¨‹å™¨å·²è¼‰å…¥: {filepath}")
        return instance


class LSTMTrafficPredictor:
    """LSTMæ·±åº¦å­¸ç¿’äº¤é€šé æ¸¬å™¨"""
    
    def __init__(self, sequence_length: int = 12, predict_ahead: int = 3):
        """
        åˆå§‹åŒ–LSTMé æ¸¬å™¨
        
        Args:
            sequence_length: åºåˆ—é•·åº¦ï¼ˆç”¨éå»12å€‹æ™‚é–“é»é æ¸¬ï¼‰
            predict_ahead: é æ¸¬æœªä¾†æ™‚é–“é»æ•¸ï¼ˆé æ¸¬æœªä¾†3å€‹5åˆ†é˜=15åˆ†é˜ï¼‰
        """
        self.sequence_length = sequence_length
        self.predict_ahead = predict_ahead
        self.model = None
        self.is_trained = False
        self.scaler = MinMaxScaler()
        
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlowæœªå®‰è£ï¼Œç„¡æ³•ä½¿ç”¨LSTMæ¨¡å‹")
        
        print(f"ğŸ§  LSTMé æ¸¬å™¨åˆå§‹åŒ–ï¼šåºåˆ—é•·åº¦{sequence_length}ï¼Œé æ¸¬{predict_ahead*5}åˆ†é˜")
    
    def _prepare_sequences(self, data: np.ndarray, target_data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """æº–å‚™LSTMåºåˆ—æ•¸æ“š"""
        X, y = [], []
        
        for i in range(len(data) - self.sequence_length - self.predict_ahead + 1):
            # è¼¸å…¥åºåˆ—
            X.append(data[i:(i + self.sequence_length)])
            # ç›®æ¨™åºåˆ—ï¼ˆé æ¸¬æœªä¾†3å€‹æ™‚é–“é»çš„å¹³å‡å€¼ï¼‰
            future_values = target_data[(i + self.sequence_length):(i + self.sequence_length + self.predict_ahead)]
            y.append(np.mean(future_values))  # ä½¿ç”¨æœªä¾†3å€‹é»çš„å¹³å‡å€¼
        
        return np.array(X), np.array(y)
    
    def build_model(self, input_shape: Tuple[int, int]) -> Sequential:
        """æ§‹å»ºLSTMæ¨¡å‹"""
        model = Sequential([
            # ç¬¬ä¸€å±¤LSTM
            LSTM(64, return_sequences=True, input_shape=input_shape),
            Dropout(0.2),
            BatchNormalization(),
            
            # ç¬¬äºŒå±¤LSTM
            LSTM(32, return_sequences=False),
            Dropout(0.2),
            BatchNormalization(),
            
            # å…¨é€£æ¥å±¤
            Dense(16, activation='relu'),
            Dropout(0.1),
            
            # è¼¸å‡ºå±¤
            Dense(1, activation='linear')
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        
        return model
    
    def train(self, X_train: np.ndarray, y_train: np.ndarray, 
              X_val: np.ndarray, y_val: np.ndarray,
              epochs: int = 50, batch_size: int = 32) -> Dict[str, Any]:
        """è¨“ç·´LSTMæ¨¡å‹"""
        print(f"ğŸš€ é–‹å§‹LSTMæ¨¡å‹è¨“ç·´...")
        print(f"   è¨“ç·´æ•¸æ“š: {X_train.shape}, é©—è­‰æ•¸æ“š: {X_val.shape}")
        
        # æ•¸æ“šæ¨™æº–åŒ–
        X_train_scaled = self.scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)
        X_val_scaled = self.scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)
        
        # æ§‹å»ºæ¨¡å‹
        self.model = self.build_model((X_train.shape[1], X_train.shape[2]))
        
        # å›èª¿å‡½æ•¸
        callbacks = [
            EarlyStopping(patience=10, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.0001)
        ]
        
        # è¨“ç·´æ¨¡å‹
        start_time = datetime.now()
        history = self.model.fit(
            X_train_scaled, y_train,
            validation_data=(X_val_scaled, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        training_time = (datetime.now() - start_time).total_seconds()
        self.is_trained = True
        
        # è©•ä¼°æ¨¡å‹
        train_loss = self.model.evaluate(X_train_scaled, y_train, verbose=0)[0]
        val_loss = self.model.evaluate(X_val_scaled, y_val, verbose=0)[0]
        
        # è¨ˆç®—æº–ç¢ºç‡
        y_train_pred = self.model.predict(X_train_scaled, verbose=0).flatten()
        y_val_pred = self.model.predict(X_val_scaled, verbose=0).flatten()
        
        train_accuracy = self._calculate_accuracy(y_train, y_train_pred)
        val_accuracy = self._calculate_accuracy(y_val, y_val_pred)
        
        print(f"âœ… LSTMè¨“ç·´å®Œæˆ")
        print(f"   è¨“ç·´æ™‚é–“: {training_time:.1f}ç§’")
        print(f"   è¨“ç·´æå¤±: {train_loss:.4f}")
        print(f"   é©—è­‰æå¤±: {val_loss:.4f}")
        print(f"   è¨“ç·´æº–ç¢ºç‡: {train_accuracy:.1f}%")
        print(f"   é©—è­‰æº–ç¢ºç‡: {val_accuracy:.1f}%")
        
        return {
            'training_time': training_time,
            'train_loss': train_loss,
            'val_loss': val_loss,
            'train_accuracy': train_accuracy,
            'val_accuracy': val_accuracy,
            'history': history.history
        }
    
    def _calculate_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray, threshold: float = 0.1) -> float:
        """è¨ˆç®—é æ¸¬æº–ç¢ºç‡ï¼ˆèª¤å·®åœ¨é–¾å€¼å…§çš„æ¯”ä¾‹ï¼‰"""
        relative_error = np.abs(y_true - y_pred) / (y_true + 1e-8)
        accuracy = np.mean(relative_error <= threshold) * 100
        return accuracy
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é€²è¡Œé æ¸¬"""
        if not self.is_trained or self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´")
        
        X_scaled = self.scaler.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)
        return self.model.predict(X_scaled, verbose=0).flatten()
    
    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            raise ValueError("æ²’æœ‰æ¨¡å‹å¯ä¿å­˜")
        
        self.model.save(filepath)
        # ä¿å­˜scaler
        scaler_path = filepath.replace('.h5', '_scaler.pkl')
        joblib.dump(self.scaler, scaler_path)
        print(f"âœ… LSTMæ¨¡å‹å·²ä¿å­˜: {filepath}")
    
    def load(self, filepath: str):
        """è¼‰å…¥æ¨¡å‹"""
        self.model = load_model(filepath)
        # è¼‰å…¥scaler
        scaler_path = filepath.replace('.h5', '_scaler.pkl')
        self.scaler = joblib.load(scaler_path)
        self.is_trained = True
        print(f"âœ… LSTMæ¨¡å‹å·²è¼‰å…¥: {filepath}")


class XGBoostTrafficPredictor:
    """XGBoostäº¤é€šé æ¸¬å™¨"""
    
    def __init__(self):
        self.model = None
        self.is_trained = False
        self.feature_importance = {}
        
        print("ğŸš€ XGBoosté æ¸¬å™¨åˆå§‹åŒ–")
    
    def train(self, X_train: np.ndarray, y_train: np.ndarray,
              X_val: np.ndarray, y_val: np.ndarray) -> Dict[str, Any]:
        """è¨“ç·´XGBoostæ¨¡å‹"""
        print(f"ğŸš€ é–‹å§‹XGBoostæ¨¡å‹è¨“ç·´...")
        print(f"   è¨“ç·´æ•¸æ“š: {X_train.shape}, é©—è­‰æ•¸æ“š: {X_val.shape}")
        
        # é…ç½®æ¨¡å‹åƒæ•¸
        params = {
            'objective': 'reg:squarederror',
            'max_depth': 6,
            'learning_rate': 0.1,
            'n_estimators': 100,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'random_state': 42,
            'n_jobs': -1
        }
        
        start_time = datetime.now()
        
        # è¨“ç·´æ¨¡å‹
        self.model = xgb.XGBRegressor(**params)
        self.model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            early_stopping_rounds=10,
            verbose=False
        )
        
        training_time = (datetime.now() - start_time).total_seconds()
        self.is_trained = True
        
        # è©•ä¼°æ¨¡å‹
        train_pred = self.model.predict(X_train)
        val_pred = self.model.predict(X_val)
        
        train_mse = mean_squared_error(y_train, train_pred)
        val_mse = mean_squared_error(y_val, val_pred)
        train_r2 = r2_score(y_train, train_pred)
        val_r2 = r2_score(y_val, val_pred)
        
        # è¨ˆç®—æº–ç¢ºç‡
        train_accuracy = self._calculate_accuracy(y_train, train_pred)
        val_accuracy = self._calculate_accuracy(y_val, val_pred)
        
        # ç‰¹å¾µé‡è¦æ€§
        self.feature_importance = dict(zip(
            [f'feature_{i}' for i in range(X_train.shape[1])],
            self.model.feature_importances_
        ))
        
        print(f"âœ… XGBoostè¨“ç·´å®Œæˆ")
        print(f"   è¨“ç·´æ™‚é–“: {training_time:.1f}ç§’")
        print(f"   è¨“ç·´MSE: {train_mse:.4f}, R2: {train_r2:.3f}")
        print(f"   é©—è­‰MSE: {val_mse:.4f}, R2: {val_r2:.3f}")
        print(f"   è¨“ç·´æº–ç¢ºç‡: {train_accuracy:.1f}%")
        print(f"   é©—è­‰æº–ç¢ºç‡: {val_accuracy:.1f}%")
        
        return {
            'training_time': training_time,
            'train_mse': train_mse,
            'val_mse': val_mse,
            'train_r2': train_r2,
            'val_r2': val_r2,
            'train_accuracy': train_accuracy,
            'val_accuracy': val_accuracy,
            'feature_importance': self.feature_importance
        }
    
    def _calculate_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray, threshold: float = 0.1) -> float:
        """è¨ˆç®—é æ¸¬æº–ç¢ºç‡"""
        relative_error = np.abs(y_true - y_pred) / (y_true + 1e-8)
        accuracy = np.mean(relative_error <= threshold) * 100
        return accuracy
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é€²è¡Œé æ¸¬"""
        if not self.is_trained or self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´")
        
        return self.model.predict(X)
    
    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            raise ValueError("æ²’æœ‰æ¨¡å‹å¯ä¿å­˜")
        
        joblib.dump(self.model, filepath)
        print(f"âœ… XGBoostæ¨¡å‹å·²ä¿å­˜: {filepath}")
    
    def load(self, filepath: str):
        """è¼‰å…¥æ¨¡å‹"""
        self.model = joblib.load(filepath)
        self.is_trained = True
        print(f"âœ… XGBoostæ¨¡å‹å·²è¼‰å…¥: {filepath}")


class RandomForestTrafficPredictor:
    """éš¨æ©Ÿæ£®æ—äº¤é€šé æ¸¬å™¨"""
    
    def __init__(self):
        self.model = None
        self.is_trained = False
        self.feature_importance = {}
        
        print("ğŸŒ² éš¨æ©Ÿæ£®æ—é æ¸¬å™¨åˆå§‹åŒ–")
    
    def train(self, X_train: np.ndarray, y_train: np.ndarray,
              X_val: np.ndarray, y_val: np.ndarray) -> Dict[str, Any]:
        """è¨“ç·´éš¨æ©Ÿæ£®æ—æ¨¡å‹"""
        print(f"ğŸš€ é–‹å§‹éš¨æ©Ÿæ£®æ—æ¨¡å‹è¨“ç·´...")
        print(f"   è¨“ç·´æ•¸æ“š: {X_train.shape}, é©—è­‰æ•¸æ“š: {X_val.shape}")
        
        start_time = datetime.now()
        
        # è¨“ç·´æ¨¡å‹
        self.model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        self.model.fit(X_train, y_train)
        training_time = (datetime.now() - start_time).total_seconds()
        self.is_trained = True
        
        # è©•ä¼°æ¨¡å‹
        train_pred = self.model.predict(X_train)
        val_pred = self.model.predict(X_val)
        
        train_mse = mean_squared_error(y_train, train_pred)
        val_mse = mean_squared_error(y_val, val_pred)
        train_r2 = r2_score(y_train, train_pred)
        val_r2 = r2_score(y_val, val_pred)
        
        # è¨ˆç®—æº–ç¢ºç‡
        train_accuracy = self._calculate_accuracy(y_train, train_pred)
        val_accuracy = self._calculate_accuracy(y_val, val_pred)
        
        # ç‰¹å¾µé‡è¦æ€§
        self.feature_importance = dict(zip(
            [f'feature_{i}' for i in range(X_train.shape[1])],
            self.model.feature_importances_
        ))
        
        print(f"âœ… éš¨æ©Ÿæ£®æ—è¨“ç·´å®Œæˆ")
        print(f"   è¨“ç·´æ™‚é–“: {training_time:.1f}ç§’")
        print(f"   è¨“ç·´MSE: {train_mse:.4f}, R2: {train_r2:.3f}")
        print(f"   é©—è­‰MSE: {val_mse:.4f}, R2: {val_r2:.3f}")
        print(f"   è¨“ç·´æº–ç¢ºç‡: {train_accuracy:.1f}%")
        print(f"   é©—è­‰æº–ç¢ºç‡: {val_accuracy:.1f}%")
        
        return {
            'training_time': training_time,
            'train_mse': train_mse,
            'val_mse': val_mse,
            'train_r2': train_r2,
            'val_r2': val_r2,
            'train_accuracy': train_accuracy,
            'val_accuracy': val_accuracy,
            'feature_importance': self.feature_importance
        }
    
    def _calculate_accuracy(self, y_true: np.ndarray, y_pred: np.ndarray, threshold: float = 0.1) -> float:
        """è¨ˆç®—é æ¸¬æº–ç¢ºç‡"""
        relative_error = np.abs(y_true - y_pred) / (y_true + 1e-8)
        accuracy = np.mean(relative_error <= threshold) * 100
        return accuracy
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """é€²è¡Œé æ¸¬"""
        if not self.is_trained or self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´")
        
        return self.model.predict(X)
    
    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            raise ValueError("æ²’æœ‰æ¨¡å‹å¯ä¿å­˜")
        
        joblib.dump(self.model, filepath)
        print(f"âœ… éš¨æ©Ÿæ£®æ—æ¨¡å‹å·²ä¿å­˜: {filepath}")
    
    def load(self, filepath: str):
        """è¼‰å…¥æ¨¡å‹"""
        self.model = joblib.load(filepath)
        self.is_trained = True
        print(f"âœ… éš¨æ©Ÿæ£®æ—æ¨¡å‹å·²è¼‰å…¥: {filepath}")


class TrafficPredictionSystem:
    """äº¤é€šé æ¸¬ç³»çµ±ä¸»æ§å™¨"""
    
    def __init__(self, base_folder: str = "data"):
        self.base_folder = Path(base_folder)
        self.model_folder = Path("models")
        self.model_folder.mkdir(exist_ok=True)
        
        # çµ„ä»¶åˆå§‹åŒ–
        self.feature_engineer = TrafficFeatureEngineer()
        self.models = {}
        self.model_performances = {}
        self.best_model = None
        
        # é æ¸¬é…ç½®
        self.target_column = 'speed'
        self.sequence_length = 12
        self.predict_ahead_minutes = 15
        
        print("ğŸ¯ äº¤é€šé æ¸¬ç³»çµ±åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç›®æ¨™é æ¸¬: {self.predict_ahead_minutes}åˆ†é˜å¾Œçš„{self.target_column}")
    
    def load_data(self, sample_rate: float = 0.1) -> pd.DataFrame:
        """è¼‰å…¥äº¤é€šæ•¸æ“š"""
        print("ğŸ“Š è¼‰å…¥äº¤é€šæ•¸æ“š...")
        
        try:
            # å˜—è©¦è¼‰å…¥æ¸…ç†å¾Œçš„æ•¸æ“š
            try:
                from data_cleaner import load_cleaned_data
                cleaned_data = load_cleaned_data(str(self.base_folder))
            except:
                # å¦‚æœæ¸…ç†æ•¸æ“šä¸å¯ç”¨ï¼Œå˜—è©¦è¼‰å…¥åˆ†æå™¨æ•¸æ“š
                from flow_analyzer import SimplifiedTrafficAnalyzer
                analyzer = SimplifiedTrafficAnalyzer(str(self.base_folder))
                if analyzer.load_data(sample_rate=sample_rate):
                    cleaned_data = analyzer.datasets
                else:
                    raise ValueError("ç„¡æ³•è¼‰å…¥æ•¸æ“š")
            
            if not cleaned_data:
                raise ValueError("ç„¡æ³•è¼‰å…¥æ¸…ç†æ•¸æ“š")
            
            # åˆä½µæ‰€æœ‰æ•¸æ“š
            all_data = []
            for name, df in cleaned_data.items():
                if not df.empty and self.target_column in df.columns:
                    df['data_source'] = name
                    all_data.append(df)
            
            if not all_data:
                raise ValueError(f"æ²’æœ‰åŒ…å«{self.target_column}çš„æ•¸æ“š")
            
            combined_df = pd.concat(all_data, ignore_index=True)
            
            # æ¡æ¨£ä»¥æ¸›å°‘è¨ˆç®—é‡
            if sample_rate < 1.0:
                combined_df = combined_df.sample(frac=sample_rate, random_state=42)
            
            # ç¢ºä¿æ™‚é–“æ’åº
            if 'update_time' in combined_df.columns:
                combined_df['update_time'] = pd.to_datetime(combined_df['update_time'])
                combined_df = combined_df.sort_values(['vd_id', 'update_time']).reset_index(drop=True)
            
            print(f"   âœ… æ•¸æ“šè¼‰å…¥æˆåŠŸ: {len(combined_df):,} ç­†è¨˜éŒ„")
            print(f"   ğŸ“… æ™‚é–“ç¯„åœ: {combined_df['update_time'].min()} ~ {combined_df['update_time'].max()}")
            print(f"   ğŸ›£ï¸ VDç«™é»: {combined_df['vd_id'].nunique()} å€‹")
            
            return combined_df
            
        except Exception as e:
            print(f"âŒ æ•¸æ“šè¼‰å…¥å¤±æ•—: {e}")
            return pd.DataFrame()
    
    def prepare_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """æº–å‚™è¨“ç·´æ•¸æ“š"""
        print("ğŸ”§ æº–å‚™æ¨¡å‹è¨“ç·´æ•¸æ“š...")
        
        # ç‰¹å¾µå·¥ç¨‹
        df_features = self.feature_engineer.fit_transform(df)
        
        # ç§»é™¤åŒ…å«NaNçš„è¡Œ
        df_clean = df_features.dropna()
        print(f"   æ¸…ç†å¾Œæ•¸æ“š: {len(df_clean):,} ç­†è¨˜éŒ„")
        
        # æº–å‚™ç‰¹å¾µå’Œç›®æ¨™
        feature_columns = [col for col in df_clean.columns 
                          if col not in ['update_time', 'date', self.target_column, 'data_source']]
        
        X = df_clean[feature_columns].values
        y = df_clean[self.target_column].values
        
        # åˆ†å‰²æ•¸æ“š
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, shuffle=False  # æ™‚é–“åºåˆ—ä¸æ‰“äº‚
        )
        
        print(f"   âœ… æ•¸æ“šæº–å‚™å®Œæˆ")
        print(f"   ç‰¹å¾µæ•¸é‡: {X.shape[1]}")
        print(f"   è¨“ç·´é›†: {X_train.shape[0]:,}")
        print(f"   æ¸¬è©¦é›†: {X_test.shape[0]:,}")
        
        return X_train, X_test, y_train, y_test
    
    def train_all_models(self, X_train: np.ndarray, y_train: np.ndarray,
                        X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Any]:
        """è¨“ç·´æ‰€æœ‰æ¨¡å‹"""
        print("ğŸš€ é–‹å§‹è¨“ç·´æ‰€æœ‰é æ¸¬æ¨¡å‹...")
        print("=" * 50)
        
        training_results = {}
        
        # 1. éš¨æ©Ÿæ£®æ—ï¼ˆåŸºç·šæ¨¡å‹ï¼‰
        print("ğŸŒ² è¨“ç·´éš¨æ©Ÿæ£®æ—æ¨¡å‹...")
        try:
            rf_model = RandomForestTrafficPredictor()
            rf_result = rf_model.train(X_train, y_train, X_test, y_test)
            self.models['random_forest'] = rf_model
            self.model_performances['random_forest'] = rf_result
            training_results['random_forest'] = rf_result
        except Exception as e:
            print(f"   âŒ éš¨æ©Ÿæ£®æ—è¨“ç·´å¤±æ•—: {e}")
        
        # 2. XGBoostæ¨¡å‹
        print("\nğŸš€ è¨“ç·´XGBoostæ¨¡å‹...")
        try:
            xgb_model = XGBoostTrafficPredictor()
            xgb_result = xgb_model.train(X_train, y_train, X_test, y_test)
            self.models['xgboost'] = xgb_model
            self.model_performances['xgboost'] = xgb_result
            training_results['xgboost'] = xgb_result
        except Exception as e:
            print(f"   âŒ XGBoostè¨“ç·´å¤±æ•—: {e}")
        
        # 3. LSTMæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if TENSORFLOW_AVAILABLE:
            print("\nğŸ§  è¨“ç·´LSTMæ¨¡å‹...")
            try:
                # LSTMéœ€è¦é‡æ–°æ•´ç†æ•¸æ“šç‚ºåºåˆ—æ ¼å¼
                lstm_data = self._prepare_lstm_data(X_train, y_train, X_test, y_test)
                if lstm_data is not None:
                    X_train_seq, y_train_seq, X_test_seq, y_test_seq = lstm_data
                    
                    lstm_model = LSTMTrafficPredictor(
                        sequence_length=self.sequence_length,
                        predict_ahead=3  # é æ¸¬3å€‹5åˆ†é˜æ™‚é–“é»
                    )
                    
                    lstm_result = lstm_model.train(
                        X_train_seq, y_train_seq, 
                        X_test_seq, y_test_seq,
                        epochs=30, batch_size=64
                    )
                    
                    self.models['lstm'] = lstm_model
                    self.model_performances['lstm'] = lstm_result
                    training_results['lstm'] = lstm_result
                else:
                    print("   âš ï¸ LSTMæ•¸æ“šæº–å‚™å¤±æ•—")
            except Exception as e:
                print(f"   âŒ LSTMè¨“ç·´å¤±æ•—: {e}")
        else:
            print("\nâš ï¸ TensorFlowä¸å¯ç”¨ï¼Œè·³éLSTMæ¨¡å‹")
        
        # é¸æ“‡æœ€ä½³æ¨¡å‹
        self._select_best_model()
        
        print(f"\nğŸ‰ æ¨¡å‹è¨“ç·´å®Œæˆï¼")
        print(f"   è¨“ç·´æ¨¡å‹æ•¸: {len(self.models)}")
        print(f"   æœ€ä½³æ¨¡å‹: {self.best_model}")
        
        return training_results
    
    def _prepare_lstm_data(self, X_train: np.ndarray, y_train: np.ndarray,
                          X_test: np.ndarray, y_test: np.ndarray) -> Optional[Tuple]:
        """ç‚ºLSTMæº–å‚™åºåˆ—æ•¸æ“š"""
        try:
            # ç°¡åŒ–ç‰ˆï¼šä½¿ç”¨åŸºæœ¬ç‰¹å¾µå‰µå»ºåºåˆ—
            if len(X_train) < self.sequence_length * 10:
                return None
            
            # é‡çµ„æ•¸æ“šç‚ºåºåˆ—æ ¼å¼
            def create_sequences(X, y, seq_len):
                X_seq, y_seq = [], []
                for i in range(len(X) - seq_len + 1):
                    X_seq.append(X[i:i+seq_len])
                    y_seq.append(y[i+seq_len-1])  # é æ¸¬åºåˆ—çš„æœ€å¾Œä¸€å€‹å€¼
                return np.array(X_seq), np.array(y_seq)
            
            X_train_seq, y_train_seq = create_sequences(X_train, y_train, self.sequence_length)
            X_test_seq, y_test_seq = create_sequences(X_test, y_test, self.sequence_length)
            
            print(f"   LSTMåºåˆ—æ•¸æ“šæº–å‚™å®Œæˆ")
            print(f"   è¨“ç·´åºåˆ—: {X_train_seq.shape}")
            print(f"   æ¸¬è©¦åºåˆ—: {X_test_seq.shape}")
            
            return X_train_seq, y_train_seq, X_test_seq, y_test_seq
            
        except Exception as e:
            print(f"   âŒ LSTMæ•¸æ“šæº–å‚™å¤±æ•—: {e}")
            return None
    
    def _select_best_model(self):
        """é¸æ“‡æœ€ä½³æ¨¡å‹"""
        if not self.model_performances:
            return
        
        # åŸºæ–¼é©—è­‰é›†æº–ç¢ºç‡é¸æ“‡æœ€ä½³æ¨¡å‹
        best_accuracy = 0
        best_model_name = None
        
        for model_name, performance in self.model_performances.items():
            # ä½¿ç”¨é©—è­‰é›†æº–ç¢ºç‡ä½œç‚ºè©•ä¼°æŒ‡æ¨™
            if 'val_accuracy' in performance:
                accuracy = performance['val_accuracy']
                if accuracy > best_accuracy:
                    best_accuracy = accuracy
                    best_model_name = model_name
        
        self.best_model = best_model_name
        print(f"   ğŸ¥‡ é¸å®šæœ€ä½³æ¨¡å‹: {best_model_name} (æº–ç¢ºç‡: {best_accuracy:.1f}%)")
    
    def predict_15_minutes(self, current_data: pd.DataFrame) -> Dict[str, Any]:
        """15åˆ†é˜é æ¸¬åŠŸèƒ½"""
        if not self.best_model or self.best_model not in self.models:
            raise ValueError("æ²’æœ‰å¯ç”¨çš„é æ¸¬æ¨¡å‹")
        
        # ç‰¹å¾µå·¥ç¨‹
        processed_data = self.feature_engineer.transform(current_data)
        
        # æº–å‚™ç‰¹å¾µ
        feature_columns = [col for col in processed_data.columns 
                          if col not in ['update_time', 'date', self.target_column, 'data_source']]
        
        X = processed_data[feature_columns].values
        
        # ä½¿ç”¨æœ€ä½³æ¨¡å‹é€²è¡Œé æ¸¬
        model = self.models[self.best_model]
        
        if self.best_model == 'lstm' and len(X) >= self.sequence_length:
            # LSTMéœ€è¦åºåˆ—æ•¸æ“š
            X_seq = X[-self.sequence_length:].reshape(1, self.sequence_length, -1)
            prediction = model.predict(X_seq)[0]
        else:
            # å…¶ä»–æ¨¡å‹ä½¿ç”¨æœ€æ–°æ•¸æ“šé»
            X_latest = X[-1:] if len(X) > 0 else X
            prediction = model.predict(X_latest)[0] if len(X_latest) > 0 else 0
        
        # é æ¸¬çµæœ
        current_time = datetime.now()
        prediction_time = current_time + timedelta(minutes=15)
        
        # è¨ˆç®—ç½®ä¿¡åº¦ï¼ˆåŸºæ–¼æ¨¡å‹æ€§èƒ½ï¼‰
        model_performance = self.model_performances[self.best_model]
        confidence = model_performance.get('val_accuracy', 0)
        
        # åˆ†é¡é æ¸¬çµæœ
        if prediction > 80:
            traffic_status = "æš¢é€š"
            status_color = "green"
        elif prediction > 50:
            traffic_status = "ç·©æ…¢"
            status_color = "yellow"
        else:
            traffic_status = "æ“å µ"
            status_color = "red"
        
        return {
            'prediction_time': prediction_time.isoformat(),
            'predicted_speed': round(prediction, 1),
            'traffic_status': traffic_status,
            'status_color': status_color,
            'confidence': round(confidence, 1),
            'model_used': self.best_model,
            'current_time': current_time.isoformat(),
            'forecast_horizon': '15åˆ†é˜'
        }
    
    def evaluate_all_models(self) -> Dict[str, Any]:
        """è©•ä¼°æ‰€æœ‰æ¨¡å‹æ€§èƒ½"""
        print("ğŸ“Š è©•ä¼°æ‰€æœ‰æ¨¡å‹æ€§èƒ½...")
        
        evaluation_results = {}
        
        for model_name, performance in self.model_performances.items():
            evaluation_results[model_name] = {
                'accuracy': performance.get('val_accuracy', 0),
                'mse': performance.get('val_mse', 0),
                'r2': performance.get('val_r2', 0),
                'training_time': performance.get('training_time', 0)
            }
            
            print(f"   {model_name}:")
            print(f"      æº–ç¢ºç‡: {evaluation_results[model_name]['accuracy']:.1f}%")
            print(f"      MSE: {evaluation_results[model_name]['mse']:.4f}")
            print(f"      R2: {evaluation_results[model_name]['r2']:.3f}")
            print(f"      è¨“ç·´æ™‚é–“: {evaluation_results[model_name]['training_time']:.1f}ç§’")
        
        return evaluation_results
    
    def save_models(self):
        """ä¿å­˜æ‰€æœ‰æ¨¡å‹"""
        print("ğŸ’¾ ä¿å­˜æ‰€æœ‰æ¨¡å‹...")
        
        # ä¿å­˜ç‰¹å¾µå·¥ç¨‹å™¨
        fe_path = self.model_folder / "feature_engineer.pkl"
        self.feature_engineer.save(str(fe_path))
        
        # ä¿å­˜å„å€‹æ¨¡å‹
        for model_name, model in self.models.items():
            if model_name == 'lstm':
                model_path = self.model_folder / f"{model_name}_model.h5"
            else:
                model_path = self.model_folder / f"{model_name}_model.pkl"
            
            model.save(str(model_path))
        
        # ä¿å­˜æ¨¡å‹æ€§èƒ½å’Œé…ç½®
        config = {
            'best_model': self.best_model,
            'model_performances': self.model_performances,
            'target_column': self.target_column,
            'sequence_length': self.sequence_length,
            'predict_ahead_minutes': self.predict_ahead_minutes
        }
        
        config_path = self.model_folder / "model_config.json"
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"   âœ… æ‰€æœ‰æ¨¡å‹å·²ä¿å­˜åˆ°: {self.model_folder}")
    
    def load_models(self):
        """è¼‰å…¥æ‰€æœ‰æ¨¡å‹"""
        print("ğŸ“‚ è¼‰å…¥æ‰€æœ‰æ¨¡å‹...")
        
        # è¼‰å…¥é…ç½®
        config_path = self.model_folder / "model_config.json"
        if not config_path.exists():
            raise FileNotFoundError("æ¨¡å‹é…ç½®æª”æ¡ˆä¸å­˜åœ¨")
        
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        
        self.best_model = config['best_model']
        self.model_performances = config['model_performances']
        self.target_column = config['target_column']
        self.sequence_length = config['sequence_length']
        self.predict_ahead_minutes = config['predict_ahead_minutes']
        
        # è¼‰å…¥ç‰¹å¾µå·¥ç¨‹å™¨
        fe_path = self.model_folder / "feature_engineer.pkl"
        self.feature_engineer = TrafficFeatureEngineer.load(str(fe_path))
        
        # è¼‰å…¥å„å€‹æ¨¡å‹
        for model_name in self.model_performances.keys():
            try:
                if model_name == 'lstm' and TENSORFLOW_AVAILABLE:
                    model = LSTMTrafficPredictor()
                    model_path = self.model_folder / f"{model_name}_model.h5"
                    model.load(str(model_path))
                    self.models[model_name] = model
                elif model_name == 'xgboost':
                    model = XGBoostTrafficPredictor()
                    model_path = self.model_folder / f"{model_name}_model.pkl"
                    model.load(str(model_path))
                    self.models[model_name] = model
                elif model_name == 'random_forest':
                    model = RandomForestTrafficPredictor()
                    model_path = self.model_folder / f"{model_name}_model.pkl"
                    model.load(str(model_path))
                    self.models[model_name] = model
            except Exception as e:
                print(f"   âš ï¸ è¼‰å…¥{model_name}æ¨¡å‹å¤±æ•—: {e}")
        
        print(f"   âœ… è¼‰å…¥å®Œæˆï¼Œæœ€ä½³æ¨¡å‹: {self.best_model}")
    
    def generate_prediction_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆé æ¸¬ç³»çµ±å ±å‘Š"""
        report = {
            'system_info': {
                'target_column': self.target_column,
                'prediction_horizon': f"{self.predict_ahead_minutes}åˆ†é˜",
                'sequence_length': self.sequence_length,
                'tensorflow_available': TENSORFLOW_AVAILABLE
            },
            'models': {
                'total_models': len(self.models),
                'best_model': self.best_model,
                'available_models': list(self.models.keys())
            },
            'performance': self.model_performances,
            'evaluation': self.evaluate_all_models() if self.model_performances else {},
            'generation_time': datetime.now().isoformat()
        }
        
        return report


# ä¾¿åˆ©å‡½æ•¸
def train_traffic_prediction_system(base_folder: str = "data", sample_rate: float = 0.1) -> TrafficPredictionSystem:
    """è¨“ç·´äº¤é€šé æ¸¬ç³»çµ±"""
    print("ğŸš€ é–‹å§‹è¨“ç·´äº¤é€šé æ¸¬ç³»çµ±...")
    
    # åˆå§‹åŒ–ç³»çµ±
    system = TrafficPredictionSystem(base_folder)
    
    # è¼‰å…¥æ•¸æ“š
    df = system.load_data(sample_rate=sample_rate)
    
    if df.empty:
        raise ValueError("ç„¡æ³•è¼‰å…¥æ•¸æ“š")
    
    # æº–å‚™æ•¸æ“š
    X_train, X_test, y_train, y_test = system.prepare_data(df)
    
    # è¨“ç·´æ‰€æœ‰æ¨¡å‹
    training_results = system.train_all_models(X_train, y_train, X_test, y_test)
    
    # ä¿å­˜æ¨¡å‹
    system.save_models()
    
    print("âœ… äº¤é€šé æ¸¬ç³»çµ±è¨“ç·´å®Œæˆï¼")
    return system


def load_traffic_prediction_system(base_folder: str = "data") -> TrafficPredictionSystem:
    """è¼‰å…¥è¨“ç·´å¥½çš„äº¤é€šé æ¸¬ç³»çµ±"""
    system = TrafficPredictionSystem(base_folder)
    system.load_models()
    return system


def quick_predict(current_data: pd.DataFrame, base_folder: str = "data") -> Dict[str, Any]:
    """å¿«é€Ÿé æ¸¬åŠŸèƒ½"""
    try:
        system = load_traffic_prediction_system(base_folder)
        return system.predict_15_minutes(current_data)
    except Exception as e:
        print(f"âŒ é æ¸¬å¤±æ•—: {e}")
        return {
            'error': str(e),
            'prediction_time': datetime.now().isoformat(),
            'status': 'error'
        }


if __name__ == "__main__":
    print("ğŸ¯ AIäº¤é€šé æ¸¬æ¨¡çµ„ - åœ‹é“1è™Ÿåœ“å±±-ä¸‰é‡è·¯æ®µ")
    print("=" * 60)
    print("ğŸ¯ æ ¸å¿ƒåŠŸèƒ½:")
    print("   ğŸ¥‡ LSTMæ·±åº¦å­¸ç¿’æ™‚é–“åºåˆ—é æ¸¬")
    print("   ğŸ¥ˆ XGBoosté«˜ç²¾åº¦æ¢¯åº¦æå‡é æ¸¬")
    print("   ğŸ¥‰ éš¨æ©Ÿæ£®æ—åŸºç·šé æ¸¬æ¨¡å‹")
    print("   ğŸ”§ æ™ºèƒ½ç‰¹å¾µå·¥ç¨‹ç®¡é“")
    print("   ğŸ“Š æ¨¡å‹è©•ä¼°èˆ‡é¸æ“‡ç³»çµ±")
    print("   âš¡ 15åˆ†é˜æ»¾å‹•é æ¸¬åŠŸèƒ½")
    print("=" * 60)
    
    # æª¢æŸ¥ç’°å¢ƒ
    print(f"ğŸ“¦ ç’°å¢ƒæª¢æŸ¥:")
    print(f"   TensorFlow: {'âœ… å¯ç”¨' if TENSORFLOW_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
    print(f"   XGBoost: âœ… å¯ç”¨")
    print(f"   éš¨æ©Ÿæ£®æ—: âœ… å¯ç”¨")
    
    # è©¢å•ç”¨æˆ¶æ“ä½œ
    print(f"\nğŸš€ å¯åŸ·è¡Œæ“ä½œ:")
    print("1. è¨“ç·´æ–°çš„é æ¸¬ç³»çµ±")
    print("2. è¼‰å…¥ç¾æœ‰é æ¸¬ç³»çµ±")
    print("3. åŸ·è¡Œå¿«é€Ÿé æ¸¬æ¸¬è©¦")
    
    choice = input("\nè«‹é¸æ“‡æ“ä½œ (1/2/3): ")
    
    if choice == "1":
        print("\nğŸš€ é–‹å§‹è¨“ç·´é æ¸¬ç³»çµ±...")
        try:
            sample_rate = float(input("è«‹è¼¸å…¥æ•¸æ“šæ¡æ¨£ç‡ (0.1-1.0, å»ºè­°0.1): ") or "0.1")
            system = train_traffic_prediction_system(sample_rate=sample_rate)
            
            # ç”Ÿæˆå ±å‘Š
            report = system.generate_prediction_report()
            
            print(f"\nğŸ“Š è¨“ç·´çµæœæ‘˜è¦:")
            print(f"   æœ€ä½³æ¨¡å‹: {report['models']['best_model']}")
            print(f"   æ¨¡å‹æ•¸é‡: {report['models']['total_models']}")
            
            # é¡¯ç¤ºæ€§èƒ½
            if report['evaluation']:
                best_model = report['models']['best_model']
                if best_model in report['evaluation']:
                    perf = report['evaluation'][best_model]
                    print(f"   é æ¸¬æº–ç¢ºç‡: {perf['accuracy']:.1f}%")
                    print(f"   R2åˆ†æ•¸: {perf['r2']:.3f}")
            
            print(f"\nğŸ¯ é æ¸¬ç³»çµ±å·²å°±ç·’ï¼")
            print(f"   ç›®æ¨™: 15åˆ†é˜é€Ÿåº¦é æ¸¬")
            print(f"   æº–ç¢ºç‡ç›®æ¨™: 85%ä»¥ä¸Š")
            
        except Exception as e:
            print(f"âŒ è¨“ç·´å¤±æ•—: {e}")
    
    elif choice == "2":
        print("\nğŸ“‚ è¼‰å…¥ç¾æœ‰é æ¸¬ç³»çµ±...")
        try:
            system = load_traffic_prediction_system()
            print(f"âœ… é æ¸¬ç³»çµ±è¼‰å…¥æˆåŠŸ")
            print(f"   æœ€ä½³æ¨¡å‹: {system.best_model}")
            print(f"   å¯ç”¨æ¨¡å‹: {list(system.models.keys())}")
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥å¤±æ•—: {e}")
            print("ğŸ’¡ è«‹å…ˆè¨“ç·´é æ¸¬ç³»çµ±")
    
    elif choice == "3":
        print("\nğŸ§ª åŸ·è¡Œå¿«é€Ÿé æ¸¬æ¸¬è©¦...")
        print("ğŸ’¡ é€™éœ€è¦å·²è¨“ç·´çš„æ¨¡å‹å’Œæ¸¬è©¦æ•¸æ“š")
        print("   å»ºè­°å…ˆå®Œæˆæ¨¡å‹è¨“ç·´")
    
    else:
        print("ğŸ’¡ ä½¿ç”¨ç¤ºç¯„:")
        print("# è¨“ç·´é æ¸¬ç³»çµ±")
        print("system = train_traffic_prediction_system(sample_rate=0.1)")
        print("")
        print("# è¼‰å…¥é æ¸¬ç³»çµ±")
        print("system = load_traffic_prediction_system()")
        print("")
        print("# é€²è¡Œ15åˆ†é˜é æ¸¬")
        print("prediction = system.predict_15_minutes(current_data)")
        print("print(f'é æ¸¬é€Ÿåº¦: {prediction[\"predicted_speed\"]} km/h')")
    
    print(f"\nğŸ¯ AIé æ¸¬æ¨¡çµ„ç‰¹è‰²:")
    print("âœ… å¤šæ¨¡å‹èåˆé æ¸¬")
    print("âœ… æ™ºèƒ½ç‰¹å¾µå·¥ç¨‹")
    print("âœ… 15åˆ†é˜ç²¾æº–é æ¸¬")
    print("âœ… 85%ä»¥ä¸Šæº–ç¢ºç‡ç›®æ¨™")
    print("âœ… å®Œæ•´çš„æ¨¡å‹è©•ä¼°ç³»çµ±")
    print("âœ… åŸºæ–¼80,640ç­†AIè¨“ç·´æ•¸æ“š")
    
    print(f"\nğŸš€ Ready for 15-Minute Traffic Prediction! ğŸš€")