# src/data_loader.py - ç°¡åŒ–ç‰ˆï¼ˆéœé»˜è¨˜æ†¶é«”å„ªåŒ–ï¼‰

"""
VDæ•¸æ“šè¼‰å…¥å™¨ - ç°¡åŒ–ç‰ˆ
=====================================

æ ¸å¿ƒç‰¹è‰²ï¼š
1. éœé»˜è¨˜æ†¶é«”å„ªåŒ–ï¼šå¾Œå°è‡ªå‹•è™•ç†ï¼Œä¸é¡¯ç¤ºè©³ç´°ä¿¡æ¯
2. å°ˆæ³¨Rawæ•¸æ“šè™•ç†ï¼šä¸»è¦è¼¸å‡ºè™•ç†é€²åº¦
3. æ™ºæ…§Archiveæª¢æŸ¥ï¼šå¿«é€Ÿæª¢æŸ¥ï¼Œé¿å…é‡è¤‡è™•ç†
4. ä¿ç•™æ‰€æœ‰åŠŸèƒ½ï¼Œç°¡åŒ–è¼¸å‡º

è™•ç†æµç¨‹ï¼š
1. è‡ªå‹•æª¢æ¸¬ç³»çµ±è¨˜æ†¶é«”ä¸¦èª¿æ•´ç­–ç•¥ï¼ˆå¾Œå°ï¼‰
2. å¿«é€Ÿæª¢æŸ¥Archiveç‹€æ…‹ï¼ˆå¾Œå°ï¼‰
3. è™•ç†Rawæ•¸æ“šä¸¦é¡¯ç¤ºä¸»è¦é€²åº¦
4. æŒ‰æ—¥æœŸçµ„ç¹”è¼¸å‡º
"""

import xml.etree.ElementTree as ET
import pandas as pd
import numpy as np
from datetime import datetime, time
import os
import json
import shutil
import threading
import time as time_module
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Tuple
import warnings
import gc
import psutil
warnings.filterwarnings('ignore')

class VDDataLoader:
    """VDæ•¸æ“šè¼‰å…¥å™¨ - ç°¡åŒ–ç‰ˆï¼ˆéœé»˜è¨˜æ†¶é«”å„ªåŒ–ï¼‰"""
    
    def __init__(self, base_folder: str = "data", max_workers: int = None, verbose: bool = False):
        """
        åˆå§‹åŒ–è¼‰å…¥å™¨
        
        Args:
            base_folder: åŸºç¤è³‡æ–™å¤¾è·¯å¾‘
            max_workers: æœ€å¤§ç·šç¨‹æ•¸
            verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°è¨˜æ†¶é«”å„ªåŒ–ä¿¡æ¯
        """
        self.base_folder = Path(base_folder)
        self.raw_folder = self.base_folder / "raw"
        self.processed_base_folder = self.base_folder / "processed"
        self.archive_folder = self.base_folder / "archive"
        self.verbose = verbose
        
        # ç¢ºä¿åŸºç¤è³‡æ–™å¤¾å­˜åœ¨
        for folder in [self.raw_folder, self.processed_base_folder, self.archive_folder]:
            folder.mkdir(parents=True, exist_ok=True)
        
        # éœé»˜è¨˜æ†¶é«”å„ªåŒ–è¨­å®š
        self._init_memory_optimization_silent()
        
        # è¶…ç´šå„ªåŒ–ç·šç¨‹æ•¸è¨­å®š
        if max_workers is None:
            cpu_count = os.cpu_count() or 4
            self.max_workers = min(cpu_count * 3, 20)
        else:
            self.max_workers = max_workers
        
        # XMLå‘½åç©ºé–“
        self.namespace = {
            'traffic': 'http://traffic.transportdata.tw/standard/traffic/schema/'
        }
        
        # æ—¥æœŸè³‡æ–™å¤¾æ˜ å°„
        self.date_folders = {}
        
        # åœ‹é“1è™Ÿåœ“å±±-ä¸‰é‡è·¯æ®µVDè¨­å‚™æ¸…å–®
        self.target_route_vd_ids = [
            'VD-N1-N-23.0-M-LOOP', 'VD-N1-N-23.5-M-LOOP', 
            'VD-N1-S-23.0-M-LOOP', 'VD-N1-S-23.5-M-LOOP',
            'VD-N1-N-25.0-M-LOOP', 'VD-N1-N-25.5-M-LOOP',
            'VD-N1-S-25.0-M-LOOP', 'VD-N1-S-25.5-M-LOOP',
            'VD-N1-N-27.0-M-LOOP', 'VD-N1-N-27.5-M-LOOP',
            'VD-N1-S-27.0-M-LOOP', 'VD-N1-S-27.5-M-LOOP',
            'VD-N1-N-86.120-M-LOOP', 'VD-N1-N-88.050-M-LOOP',
        ]
        
        # å…§éƒ¨æ‰¹æ¬¡å¤§å°è¨­å®šï¼ˆè¨˜æ†¶é«”å„ªåŒ–ï¼‰
        self.internal_batch_size = self._calculate_optimal_batch_size()
        
        # ç·šç¨‹é–
        self.file_lock = threading.Lock()
        
        # ç°¡åŒ–åˆå§‹åŒ–è¼¸å‡º
        print(f"ğŸ—ï¸ VDæ•¸æ“šè¼‰å…¥å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ğŸ“ è³‡æ–™å¤¾: {self.base_folder}")
        print(f"   ğŸ§µ è™•ç†ç·šç¨‹: {self.max_workers}")
        print(f"   ğŸ¯ ç›®æ¨™è·¯æ®µè¨­å‚™: {len(self.target_route_vd_ids)}å€‹")
        if self.verbose:
            print(f"   ğŸ’¾ è¨˜æ†¶é«”æ¨¡å¼: {self.total_memory_gb:.1f}GBç’°å¢ƒ")
    
    def _init_memory_optimization_silent(self):
        """éœé»˜åˆå§‹åŒ–è¨˜æ†¶é«”å„ªåŒ–è¨­å®š"""
        try:
            # æª¢æ¸¬ç³»çµ±è¨˜æ†¶é«”
            memory_info = psutil.virtual_memory()
            self.total_memory_gb = memory_info.total / (1024**3)
            self.available_memory_gb = memory_info.available / (1024**3)
            
            # è¨­å®šè¨˜æ†¶é«”ä½¿ç”¨ç­–ç•¥ï¼ˆéœé»˜ï¼‰
            if self.total_memory_gb >= 16:
                self.max_memory_usage_percent = 80
                self.chunk_processing = True
                self.force_gc_frequency = 3
            elif self.total_memory_gb >= 8:
                self.max_memory_usage_percent = 70
                self.chunk_processing = True
                self.force_gc_frequency = 2
            else:
                self.max_memory_usage_percent = 60
                self.chunk_processing = True
                self.force_gc_frequency = 1
                
            self.operation_count = 0
            
        except Exception as e:
            # éœé»˜éŒ¯èª¤è™•ç†
            self.total_memory_gb = 8
            self.max_memory_usage_percent = 70
            self.chunk_processing = True
            self.force_gc_frequency = 2
            self.operation_count = 0
    
    def _calculate_optimal_batch_size(self) -> int:
        """éœé»˜è¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°"""
        if self.total_memory_gb >= 32:
            return 500
        elif self.total_memory_gb >= 16:
            return 300
        elif self.total_memory_gb >= 8:
            return 200
        else:
            return 100
    
    def _monitor_memory_and_gc(self, force: bool = False):
        """éœé»˜è¨˜æ†¶é«”ç›£æ§å’Œåƒåœ¾å›æ”¶"""
        self.operation_count += 1
        
        try:
            if force or self.operation_count % self.force_gc_frequency == 0:
                memory_info = psutil.virtual_memory()
                memory_usage_percent = memory_info.percent
                
                if memory_usage_percent > self.max_memory_usage_percent or force:
                    gc.collect()
                    
                    # åªåœ¨verboseæ¨¡å¼ä¸‹é¡¯ç¤º
                    if self.verbose and force:
                        memory_info_after = psutil.virtual_memory()
                        print(f"      ğŸ§¹ è¨˜æ†¶é«”æ¸…ç†: {memory_info_after.percent:.1f}%")
                
        except Exception:
            pass  # éœé»˜è™•ç†éŒ¯èª¤
    
    def _optimize_dataframe_memory(self, df: pd.DataFrame) -> pd.DataFrame:
        """éœé»˜å„ªåŒ–DataFrameè¨˜æ†¶é«”ä½¿ç”¨"""
        if df.empty:
            return df
        
        try:
            # æ™‚é–“é¡å‹æœ€ä½³åŒ–
            time_columns = ['date', 'update_time']
            for col in time_columns:
                if col in df.columns and not pd.api.types.is_datetime64_any_dtype(df[col]):
                    df[col] = pd.to_datetime(df[col], errors='coerce')
            
            # æ•¸å€¼é¡å‹æœ€ä½³åŒ–
            int_columns = [
                'lane_id', 'lane_type', 'speed', 'occupancy',
                'volume_total', 'volume_small', 'volume_large', 'volume_truck',
                'speed_small', 'speed_large', 'speed_truck'
            ]
            
            for col in int_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                    
                    if df[col].notna().any():
                        col_min = df[col].min()
                        col_max = df[col].max()
                        
                        if col_min >= 0 and col_max <= 255:
                            df[col] = df[col].astype('uint8')
                        elif col_min >= -128 and col_max <= 127:
                            df[col] = df[col].astype('int8')
                        elif col_min >= 0 and col_max <= 65535:
                            df[col] = df[col].astype('uint16')
                        elif col_min >= -32768 and col_max <= 32767:
                            df[col] = df[col].astype('int16')
                        else:
                            df[col] = df[col].astype('int32')
            
            # é¡åˆ¥é¡å‹æœ€ä½³åŒ–
            if 'vd_id' in df.columns:
                df['vd_id'] = df['vd_id'].astype('category')
            
            if 'time_category' in df.columns:
                df['time_category'] = df['time_category'].astype('category')
            
            return df
            
        except Exception:
            return df  # éœé»˜è™•ç†éŒ¯èª¤
    
    def check_archive_status_silent(self) -> Dict[str, Any]:
        """éœé»˜Archiveç‹€æ…‹æª¢æŸ¥"""
        archive_status = {
            "archive_exists": self.archive_folder.exists(),
            "archived_dates": [],
            "archived_date_count": 0
        }
        
        if not self.archive_folder.exists():
            return archive_status
        
        try:
            date_folders = []
            for item in self.archive_folder.iterdir():
                if item.is_dir() and item.name.count('-') == 2:
                    date_folders.append(item.name)
            
            archive_status["archived_dates"] = sorted(date_folders)
            archive_status["archived_date_count"] = len(date_folders)
                
        except Exception:
            pass  # éœé»˜è™•ç†éŒ¯èª¤
        
        return archive_status
    
    def check_raw_folder(self) -> Dict[str, Any]:
        """æª¢æŸ¥rawè³‡æ–™å¤¾ç‹€æ…‹"""
        print("ğŸ” æª¢æŸ¥Rawè³‡æ–™å¤¾...")
        
        if not self.raw_folder.exists():
            print(f"   âŒ rawè³‡æ–™å¤¾ä¸å­˜åœ¨: {self.raw_folder}")
            return {"exists": False, "xml_files": 0, "vd_files": 0, "unprocessed": 0}
        
        # æƒæXMLæª”æ¡ˆ
        xml_files = list(self.raw_folder.rglob("*.xml")) + list(self.raw_folder.rglob("*.txt"))
        vd_files = [f for f in xml_files if self._is_vd_file(f)]
        
        # éœé»˜æª¢æŸ¥Archiveç‹€æ…‹
        archive_status = self.check_archive_status_silent()
        archived_dates = set(archive_status["archived_dates"])
        
        # å¿«é€Ÿæª¢æŸ¥æª”æ¡ˆç‹€æ…‹
        unprocessed_files = []
        processed_files = []
        
        for file_path in vd_files:
            try:
                file_date = self._extract_file_date_quick(file_path)
                if file_date and file_date in archived_dates:
                    processed_files.append(file_path)
                else:
                    unprocessed_files.append(file_path)
            except:
                unprocessed_files.append(file_path)
        
        result = {
            "exists": True,
            "xml_files": len(xml_files),
            "vd_files": len(vd_files),
            "unprocessed": len(unprocessed_files),
            "processed": len(processed_files),
            "archived_dates": len(archived_dates)
        }
        
        print(f"   ğŸ“Š æª”æ¡ˆç‹€æ…‹:")
        print(f"      â€¢ VDæª”æ¡ˆç¸½æ•¸: {result['vd_files']}")
        print(f"      â€¢ å·²æ­¸æª”: {result['processed']} (æ¶µè“‹ {len(archived_dates)} å€‹æ—¥æœŸ)")
        print(f"      â€¢ å¾…è™•ç†: {result['unprocessed']}")
        
        if result['unprocessed'] > 0:
            estimated_minutes = result['unprocessed'] * 0.005
            print(f"      â€¢ é ä¼°è™•ç†æ™‚é–“: {estimated_minutes:.1f} åˆ†é˜")
        
        return result
    
    def _extract_file_date_quick(self, file_path: Path) -> str:
        """å¿«é€Ÿæå–æª”æ¡ˆæ—¥æœŸ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    if i > 20:
                        break
                    if '<UpdateTime>' in line:
                        import re
                        date_match = re.search(r'(\d{4}-\d{2}-\d{2})', line)
                        if date_match:
                            return date_match.group(1)
            return None
        except:
            return None
    
    def _is_vd_file(self, file_path: Path) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºVDæª”æ¡ˆ"""
        name_lower = file_path.name.lower()
        keywords = ['vd', 'éœæ…‹è³‡è¨Š', 'traffic', 'detector']
        return any(keyword in name_lower for keyword in keywords)
    
    def get_or_create_date_folder(self, xml_timestamp: datetime) -> Path:
        """ç²å–æˆ–å»ºç«‹æ—¥æœŸè³‡æ–™å¤¾"""
        date_str = xml_timestamp.strftime('%Y-%m-%d')
        
        if date_str not in self.date_folders:
            date_folder = self.processed_base_folder / date_str
            date_folder.mkdir(parents=True, exist_ok=True)
            self.date_folders[date_str] = date_folder
        
        return self.date_folders[date_str]
    
    def get_date_file_paths(self, date_folder: Path) -> Dict[str, Path]:
        """ç²å–æ—¥æœŸè³‡æ–™å¤¾ä¸­çš„æª”æ¡ˆè·¯å¾‘"""
        return {
            'main_csv': date_folder / "vd_data_all.csv",
            'main_json': date_folder / "vd_data_all_summary.json",
            'peak_csv': date_folder / "vd_data_peak.csv",
            'offpeak_csv': date_folder / "vd_data_offpeak.csv",
            'peak_json': date_folder / "vd_data_peak_summary.json",
            'offpeak_json': date_folder / "vd_data_offpeak_summary.json",
            'target_route_csv': date_folder / "target_route_data.csv",
            'target_route_json': date_folder / "target_route_data_summary.json",
            'target_route_peak_csv': date_folder / "target_route_peak.csv",
            'target_route_offpeak_csv': date_folder / "target_route_offpeak.csv",
            'target_route_peak_json': date_folder / "target_route_peak_summary.json",
            'target_route_offpeak_json': date_folder / "target_route_offpeak_summary.json",
            'processed_files_log': date_folder / "processed_files.json"
        }
    
    def classify_peak_hours_vectorized(self, datetime_series: pd.Series) -> pd.Series:
        """å‘é‡åŒ–å°–å³°é›¢å³°åˆ†é¡"""
        hours = datetime_series.dt.hour
        weekdays = datetime_series.dt.weekday
        
        categories = ['å¹³æ—¥å°–å³°', 'å¹³æ—¥é›¢å³°', 'å‡æ—¥å°–å³°', 'å‡æ—¥é›¢å³°']
        result = pd.Series('å¹³æ—¥é›¢å³°', index=datetime_series.index)
        
        # å‡æ—¥åˆ¤æ–·
        is_weekend = weekdays >= 5
        
        # å‡æ—¥æ™‚é–“åˆ†é¡
        weekend_peak_mask = is_weekend & (
            ((hours >= 9) & (hours < 12)) |
            ((hours >= 15) & (hours < 19))
        )
        
        weekend_offpeak_mask = is_weekend & (
            ((hours >= 6) & (hours < 9)) |
            (hours >= 19) | (hours < 6)
        )
        
        result.loc[weekend_peak_mask] = 'å‡æ—¥å°–å³°'
        result.loc[weekend_offpeak_mask] = 'å‡æ—¥é›¢å³°'
        
        # å¹³æ—¥æ™‚é–“åˆ†é¡
        weekday_peak_mask = ~is_weekend & (
            ((hours >= 7) & (hours < 9)) |
            ((hours >= 17) & (hours < 20))
        )
        
        weekday_offpeak_mask = ~is_weekend & (
            ((hours >= 9) & (hours < 17)) |
            (hours >= 20) | (hours < 7)
        )
        
        result.loc[weekday_peak_mask] = 'å¹³æ—¥å°–å³°'
        result.loc[weekday_offpeak_mask] = 'å¹³æ—¥é›¢å³°'
        
        result = result.astype(pd.CategoricalDtype(categories=categories))
        return result
    
    def is_target_route_vectorized(self, vd_id_series: pd.Series) -> pd.Series:
        """å‘é‡åŒ–è·¯æ®µåˆ¤æ–·"""
        return vd_id_series.isin(self.target_route_vd_ids)
    
    def quick_load_existing_data(self, target_date: str = None) -> pd.DataFrame:
        """å¿«é€Ÿè¼‰å…¥å·²è™•ç†æ•¸æ“š"""
        if target_date:
            date_folder = self.processed_base_folder / target_date
            if date_folder.exists():
                file_paths = self.get_date_file_paths(date_folder)
                main_csv = file_paths['main_csv']
                
                if main_csv.exists():
                    try:
                        print(f"âš¡ è¼‰å…¥ {target_date} æ•¸æ“š...")
                        
                        if self.chunk_processing:
                            df = pd.read_csv(main_csv, engine='c', low_memory=True, chunksize=10000)
                            df = pd.concat(df, ignore_index=True)
                        else:
                            df = pd.read_csv(main_csv, engine='c', low_memory=False)
                        
                        df = self._optimize_dataframe_memory(df)
                        self._monitor_memory_and_gc()
                        
                        print(f"   âœ… è¼‰å…¥æˆåŠŸ: {len(df):,} ç­†è¨˜éŒ„")
                        return df
                    except Exception as e:
                        print(f"   âŒ è¼‰å…¥å¤±æ•—: {e}")
        else:
            print("âš¡ è¼‰å…¥æ‰€æœ‰æ—¥æœŸæ•¸æ“š...")
            all_data = []
            
            for date_folder in self.processed_base_folder.iterdir():
                if date_folder.is_dir():
                    file_paths = self.get_date_file_paths(date_folder)
                    main_csv = file_paths['main_csv']
                    
                    if main_csv.exists():
                        try:
                            if self.chunk_processing:
                                df_chunks = pd.read_csv(main_csv, engine='c', low_memory=True, chunksize=10000)
                                df = pd.concat(df_chunks, ignore_index=True)
                            else:
                                df = pd.read_csv(main_csv, engine='c', low_memory=False)
                            
                            df = self._optimize_dataframe_memory(df)
                            all_data.append(df)
                            print(f"   âœ… {date_folder.name}: {len(df):,} ç­†è¨˜éŒ„")
                            
                            self._monitor_memory_and_gc()
                            
                        except Exception as e:
                            print(f"   âŒ {date_folder.name}: è¼‰å…¥å¤±æ•— - {e}")
            
            if all_data:
                combined_df = pd.concat(all_data, ignore_index=True)
                combined_df = self._optimize_dataframe_memory(combined_df)
                
                del all_data
                self._monitor_memory_and_gc(force=True)
                
                print(f"   ğŸ¯ ç¸½è¨ˆè¼‰å…¥: {len(combined_df):,} ç­†è¨˜éŒ„")
                return combined_df
        
        print("   â„¹ï¸ æ²’æœ‰æ‰¾åˆ°å·²è™•ç†æ•¸æ“š")
        return pd.DataFrame()
    
    def process_all_files(self) -> pd.DataFrame:
        """ä¸€æ¬¡æ€§è™•ç†æ‰€æœ‰XMLæª”æ¡ˆ - ç°¡åŒ–ç‰ˆ"""
        print("ğŸš€ é–‹å§‹è™•ç†Rawæ•¸æ“š")
        print("=" * 60)
        
        start_time = time_module.time()
        
        # æª¢æŸ¥rawè³‡æ–™å¤¾
        folder_status = self.check_raw_folder()
        
        if not folder_status["exists"]:
            print("âŒ rawè³‡æ–™å¤¾ä¸å­˜åœ¨ï¼Œè«‹å…ˆæ”¾å…¥XMLæª”æ¡ˆ")
            return pd.DataFrame()
        
        if folder_status["unprocessed"] == 0:
            print("âœ… æ‰€æœ‰æª”æ¡ˆéƒ½å·²è™•ç†ï¼Œè¼‰å…¥ç¾æœ‰æ•¸æ“š")
            main_df = self.quick_load_existing_data()
            return main_df
        
        # æ‰¾åˆ°æ‰€æœ‰æœªè™•ç†æª”æ¡ˆ
        xml_files = list(self.raw_folder.rglob("*.xml")) + list(self.raw_folder.rglob("*.txt"))
        vd_files = [f for f in xml_files if self._is_vd_file(f)]
        
        # éœé»˜æª¢æŸ¥Archiveç‹€æ…‹
        archive_status = self.check_archive_status_silent()
        archived_dates = set(archive_status["archived_dates"])
        
        unprocessed_files = []
        for file_path in vd_files:
            file_date = self._extract_file_date_quick(file_path)
            if not file_date or file_date not in archived_dates:
                unprocessed_files.append(file_path)
        
        total_files = len(unprocessed_files)
        
        print(f"\nğŸ“‹ è™•ç†è¨ˆåŠƒ:")
        print(f"   â€¢ å¾…è™•ç†æª”æ¡ˆ: {total_files:,}")
        print(f"   â€¢ è™•ç†ç·šç¨‹: {self.max_workers}")
        print(f"   â€¢ è¨˜æ†¶é«”ç®¡ç†: è‡ªå‹•å„ªåŒ–ï¼ˆå¾Œå°ï¼‰")
        
        # é–‹å§‹è™•ç†
        print(f"\nğŸš€ é–‹å§‹è™•ç†...")
        
        date_organized_data = {}
        processed_count = 0
        failed_count = 0
        
        # åˆ†æ‰¹æ¬¡è™•ç†
        batch_count = (total_files + self.internal_batch_size - 1) // self.internal_batch_size
        
        for batch_idx in range(batch_count):
            start_idx = batch_idx * self.internal_batch_size
            end_idx = min(start_idx + self.internal_batch_size, total_files)
            batch_files = unprocessed_files[start_idx:end_idx]
            
            print(f"   ğŸ“¦ æ‰¹æ¬¡ {batch_idx + 1}/{batch_count} ({len(batch_files)} æª”æ¡ˆ)")
            
            # ä¸¦è¡Œè™•ç†
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_file = {
                    executor.submit(self._process_single_file_ultra_fast, file_path): file_path
                    for file_path in batch_files
                }
                
                for future in as_completed(future_to_file):
                    file_path = future_to_file[future]
                    processed_count += 1
                    
                    try:
                        result = future.result()
                        if result and result.get("success") and "data" in result:
                            xml_timestamp = result["xml_timestamp"]
                            date_str = xml_timestamp.strftime('%Y-%m-%d')
                            
                            if date_str not in date_organized_data:
                                date_organized_data[date_str] = []
                            date_organized_data[date_str].extend(result["data"])
                            
                            # ç«‹å³æ­¸æª”æª”æ¡ˆ
                            try:
                                self._archive_file_optimized(file_path, xml_timestamp)
                            except Exception:
                                pass  # éœé»˜è™•ç†æ­¸æª”éŒ¯èª¤
                        else:
                            failed_count += 1
                    except Exception:
                        failed_count += 1
                    
                    # ç°¡åŒ–é€²åº¦é¡¯ç¤º
                    if processed_count % 200 == 0 or processed_count == total_files:
                        progress = (processed_count / total_files) * 100
                        elapsed = time_module.time() - start_time
                        speed = processed_count / elapsed if elapsed > 0 else 0
                        total_records = sum(len(data_list) for data_list in date_organized_data.values())
                        
                        print(f"      é€²åº¦: {processed_count}/{total_files} ({progress:.1f}%) "
                              f"| é€Ÿåº¦: {speed:.1f} æª”æ¡ˆ/ç§’ | è¨˜éŒ„: {total_records:,}")
                        
                        # éœé»˜è¨˜æ†¶é«”æ¸…ç†
                        self._monitor_memory_and_gc()
            
            # æ‰¹æ¬¡çµæŸå¾Œéœé»˜è¨˜æ†¶é«”æ¸…ç†
            self._monitor_memory_and_gc(force=True)
        
        # æŒ‰æ—¥æœŸä¿å­˜æ•¸æ“šä¸¦åˆ†é¡
        print(f"\nğŸ“Š æŒ‰æ—¥æœŸä¿å­˜æ•¸æ“šä¸¦åˆ†é¡...")
        
        all_combined_data = []
        date_summary = {}
        
        for date_str, data_list in date_organized_data.items():
            if data_list:
                print(f"   ğŸ“… {date_str}: {len(data_list):,} ç­†è¨˜éŒ„")
                
                date_folder = self.processed_base_folder / date_str
                date_folder.mkdir(parents=True, exist_ok=True)
                
                df = pd.DataFrame(data_list)
                df = self._optimize_dataframe_memory(df)
                
                file_paths = self.get_date_file_paths(date_folder)
                self._save_date_data_and_classify_silent(df, file_paths, date_str)
                
                all_combined_data.extend(data_list)
                date_summary[date_str] = len(data_list)
                
                del df
                self._monitor_memory_and_gc()
        
        # æ¸…ç†è‡¨æ™‚æ•¸æ“š
        del date_organized_data
        self._monitor_memory_and_gc(force=True)
        
        # ç”Ÿæˆç¸½çµå ±å‘Š
        total_time = time_module.time() - start_time
        
        if all_combined_data:
            total_df = pd.DataFrame(all_combined_data)
            total_df = self._optimize_dataframe_memory(total_df)
            
            print(f"\nğŸ è™•ç†å®Œæˆï¼")
            print("=" * 60)
            print(f"ğŸ“Š è™•ç†çµæœ:")
            print(f"   â±ï¸ ç¸½æ™‚é–“: {total_time/60:.2f} åˆ†é˜")
            print(f"   âœ… æˆåŠŸè™•ç†: {processed_count - failed_count:,} æª”æ¡ˆ")
            print(f"   ğŸ“Š ç¸½è¨˜éŒ„æ•¸: {len(total_df):,}")
            print(f"   ğŸ“… è™•ç†æ—¥æœŸæ•¸: {len(date_summary)}")
            
            print(f"\nğŸ“… å„æ—¥æœŸçµ±è¨ˆ:")
            for date_str, count in sorted(date_summary.items()):
                print(f"      {date_str}: {count:,} ç­†è¨˜éŒ„")
            
            # æœ€çµ‚è¨˜æ†¶é«”æ¸…ç†
            del all_combined_data
            self._monitor_memory_and_gc(force=True)
            
            return total_df
        else:
            print("âŒ ç„¡æœ‰æ•ˆæ•¸æ“š")
            return pd.DataFrame()
    
    def _save_date_data_and_classify_silent(self, df: pd.DataFrame, file_paths: Dict[str, Path], date_str: str):
        """éœé»˜ä¿å­˜ç‰¹å®šæ—¥æœŸçš„æ•¸æ“šä¸¦é€²è¡Œåˆ†é¡"""
        if df.empty:
            return
        
        # ç¢ºä¿æ™‚é–“æ¬„ä½æ­£ç¢º
        if not pd.api.types.is_datetime64_any_dtype(df['update_time']):
            df['update_time'] = pd.to_datetime(df['update_time'], errors='coerce')
        
        # å‘é‡åŒ–åˆ†é¡
        df['time_category'] = self.classify_peak_hours_vectorized(df['update_time'])
        df['is_target_route'] = self.is_target_route_vectorized(df['vd_id'])
        
        # 1. ä¿å­˜ä¸»æª”æ¡ˆ
        df_main = self._optimize_dataframe_memory(df.copy())
        df_main.to_csv(file_paths['main_csv'], index=False, encoding='utf-8-sig')
        summary = self._get_ultra_fast_summary(df_main, f"all_{date_str}", f"{date_str} å…¨éƒ¨VDè³‡æ–™")
        with open(file_paths['main_json'], 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2, default=str)
        del df_main
        self._monitor_memory_and_gc()
        
        # 2. åˆ†æ‰¹è™•ç†åˆ†é¡æ•¸æ“š
        peak_mask = df['time_category'].str.contains('å°–å³°', na=False)
        target_route_mask = df['is_target_route']
        
        # åˆ†æ‰¹ä¿å­˜ï¼Œé¿å…åŒæ™‚è¼‰å…¥æ‰€æœ‰åˆ†é¡
        classification_jobs = [
            ('peak', peak_mask, file_paths['peak_csv'], file_paths['peak_json'], f"{date_str} å°–å³°æ™‚æ®µ"),
            ('offpeak', ~peak_mask, file_paths['offpeak_csv'], file_paths['offpeak_json'], f"{date_str} é›¢å³°æ™‚æ®µ"),
            ('target_route', target_route_mask, file_paths['target_route_csv'], file_paths['target_route_json'], f"{date_str} ç›®æ¨™è·¯æ®µ"),
            ('target_peak', target_route_mask & peak_mask, file_paths['target_route_peak_csv'], file_paths['target_route_peak_json'], f"{date_str} ç›®æ¨™è·¯æ®µå°–å³°"),
            ('target_offpeak', target_route_mask & ~peak_mask, file_paths['target_route_offpeak_csv'], file_paths['target_route_offpeak_json'], f"{date_str} ç›®æ¨™è·¯æ®µé›¢å³°")
        ]
        
        for job_name, mask, csv_path, json_path, description in classification_jobs:
            if mask.any():
                subset_df = df[mask].copy()
                subset_df = self._optimize_dataframe_memory(subset_df)
                
                subset_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
                summary = self._get_ultra_fast_summary(subset_df, f"{csv_path.stem}_{date_str}", description)
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(summary, f, ensure_ascii=False, indent=2, default=str)
                
                del subset_df
                self._monitor_memory_and_gc()
    
    def _process_single_file_ultra_fast(self, file_path: Path) -> Dict[str, Any]:
        """è¶…å¿«å–®æª”è™•ç†"""
        try:
            tree = ET.parse(file_path)
            root = tree.getroot()
            
            # å¿«é€Ÿæå–æ™‚é–“
            update_time = self._extract_update_time_ultra_fast(root)
            date_str = update_time.strftime('%Y-%m-%d')
            
            # å¿«é€Ÿæå–æ•¸æ“š
            vd_data_list = []
            vd_lives = root.findall('.//traffic:VDLive', self.namespace)
            
            for vd_live in vd_lives:
                vd_id_element = vd_live.find('traffic:VDID', self.namespace)
                if vd_id_element is None:
                    continue
                
                vd_id = vd_id_element.text
                lanes = vd_live.findall('.//traffic:Lane', self.namespace)
                
                for lane in lanes:
                    try:
                        lane_data = self._extract_lane_data_ultra_fast(lane, vd_id, date_str, update_time)
                        vd_data_list.append(lane_data)
                    except:
                        continue
            
            # æ¸…ç†XMLç‰©ä»¶
            del tree, root
            
            if vd_data_list:
                return {
                    "success": True,
                    "file_name": file_path.name,
                    "record_count": len(vd_data_list),
                    "xml_timestamp": update_time,
                    "data": vd_data_list
                }
            else:
                return {"success": False, "file_name": file_path.name, "error": "ç„¡æ•¸æ“š"}
                
        except Exception as e:
            return {"success": False, "file_name": file_path.name, "error": str(e)}
    
    def _extract_update_time_ultra_fast(self, root) -> datetime:
        """è¶…å¿«æå–æ›´æ–°æ™‚é–“"""
        try:
            update_time_element = root.find('traffic:UpdateTime', self.namespace)
            if update_time_element is not None:
                time_str = update_time_element.text.replace('+08:00', '')
                return datetime.strptime(time_str, '%Y-%m-%dT%H:%M:%S')
        except:
            pass
        return datetime.now()
    
    def _extract_lane_data_ultra_fast(self, lane_element, vd_id: str, date_str: str, 
                                    update_time: datetime) -> Dict[str, Any]:
        """è¶…å¿«æå–è»Šé“æ•¸æ“š"""
        def safe_get_int(element, tag_name, default=0):
            elem = element.find(f'traffic:{tag_name}', self.namespace)
            if elem is not None and elem.text is not None:
                try:
                    return int(elem.text)
                except:
                    return default
            return default
        
        # åŸºæœ¬è³‡è¨Š
        lane_id = safe_get_int(lane_element, 'LaneID')
        lane_type = safe_get_int(lane_element, 'LaneType')
        speed = safe_get_int(lane_element, 'Speed')
        occupancy = safe_get_int(lane_element, 'Occupancy')
        
        # è»Šç¨®æ•¸æ“š
        vehicles = lane_element.findall('traffic:Vehicles/traffic:Vehicle', self.namespace)
        volume_small = volume_large = volume_truck = 0
        speed_small = speed_large = speed_truck = 0
        
        for vehicle in vehicles:
            vehicle_type_elem = vehicle.find('traffic:VehicleType', self.namespace)
            if vehicle_type_elem is None:
                continue
            
            vehicle_type = vehicle_type_elem.text
            volume = safe_get_int(vehicle, 'Volume')
            v_speed = safe_get_int(vehicle, 'Speed')
            
            if vehicle_type == 'S':
                volume_small = volume
                speed_small = v_speed
            elif vehicle_type == 'L':
                volume_large = volume
                speed_large = v_speed
            elif vehicle_type == 'T':
                volume_truck = volume
                speed_truck = v_speed
        
        return {
            'date': date_str,
            'update_time': update_time,
            'vd_id': vd_id,
            'lane_id': lane_id,
            'lane_type': lane_type,
            'speed': speed,
            'occupancy': occupancy,
            'volume_total': volume_small + volume_large + volume_truck,
            'volume_small': volume_small,
            'volume_large': volume_large,
            'volume_truck': volume_truck,
            'speed_small': speed_small,
            'speed_large': speed_large,
            'speed_truck': speed_truck
        }
    
    def _get_ultra_fast_summary(self, df: pd.DataFrame, category: str, description: str) -> Dict[str, Any]:
        """è¶…å¿«ç”Ÿæˆæ•¸æ“šæ‘˜è¦"""
        if df.empty:
            return {"category": category, "description": description, "error": "ç„¡æ•¸æ“š"}
        
        summary = {
            "category": category,
            "description": description,
            "ç¸½è¨˜éŒ„æ•¸": len(df),
            "VDè¨­å‚™æ•¸": df['vd_id'].nunique(),
            "äº¤é€šçµ±è¨ˆ": {
                "å¹³å‡é€Ÿåº¦": round(df['speed'].mean(), 1),
                "å¹³å‡ä½”æœ‰ç‡": round(df['occupancy'].mean(), 1),
                "å¹³å‡æµé‡": round(df['volume_total'].mean(), 1),
                "æœ€é«˜é€Ÿåº¦": int(df['speed'].max()),
                "æœ€é«˜ä½”æœ‰ç‡": int(df['occupancy'].max()),
                "æœ€é«˜æµé‡": int(df['volume_total'].max())
            }
        }
        
        if 'date' in df.columns:
            summary["æ™‚é–“ç¯„åœ"] = {
                "é–‹å§‹": str(df['date'].min())[:10],
                "çµæŸ": str(df['date'].max())[:10],
                "å¤©æ•¸": df['date'].nunique()
            }
        
        if 'time_category' in df.columns:
            summary["æ™‚é–“åˆ†é¡çµ±è¨ˆ"] = df['time_category'].value_counts().to_dict()
        
        if 'is_target_route' in df.columns:
            summary["è·¯æ®µçµ±è¨ˆ"] = {
                "ç›®æ¨™è·¯æ®µè¨˜éŒ„": df['is_target_route'].sum(),
                "éç›®æ¨™è·¯æ®µè¨˜éŒ„": (~df['is_target_route']).sum()
            }
        
        return summary
    
    def _archive_file_optimized(self, file_path: Path, xml_timestamp: datetime) -> str:
        """å„ªåŒ–ç‰ˆæ­¸æª”æª”æ¡ˆ"""
        try:
            archive_date_folder = self.archive_folder / xml_timestamp.strftime("%Y-%m-%d")
            archive_date_folder.mkdir(exist_ok=True)
            
            new_filename = f"{xml_timestamp.strftime('%Y%m%d_%H%M%S')}_{file_path.name}"
            archive_path = archive_date_folder / new_filename
            
            shutil.move(str(file_path), str(archive_path))
            return str(archive_path)
        except Exception:
            return ""  # éœé»˜è™•ç†éŒ¯èª¤
    
    def load_classified_data(self, target_date: str = None) -> Dict[str, pd.DataFrame]:
        """è¼‰å…¥å·²åˆ†é¡çš„æ•¸æ“š"""
        print("ğŸ“‚ è¼‰å…¥å·²åˆ†é¡æ•¸æ“š...")
        
        classified_data = {}
        
        if target_date:
            # è¼‰å…¥ç‰¹å®šæ—¥æœŸçš„æ•¸æ“š
            date_folder = self.processed_base_folder / target_date
            if date_folder.exists():
                print(f"   ğŸ“… è¼‰å…¥ {target_date} æ•¸æ“š")
                file_paths = self.get_date_file_paths(date_folder)
                classified_data = self._load_date_classified_data_silent(file_paths, target_date)
            else:
                print(f"   âš ï¸ æ—¥æœŸè³‡æ–™å¤¾ä¸å­˜åœ¨: {target_date}")
        else:
            # è¼‰å…¥æ‰€æœ‰æ—¥æœŸçš„æ•¸æ“šä¸¦åˆä½µ
            print("   ğŸ“… è¼‰å…¥æ‰€æœ‰æ—¥æœŸæ•¸æ“šä¸¦åˆä½µ")
            all_date_data = {}
            
            for date_folder in self.processed_base_folder.iterdir():
                if date_folder.is_dir():
                    date_str = date_folder.name
                    file_paths = self.get_date_file_paths(date_folder)
                    date_data = self._load_date_classified_data_silent(file_paths, date_str)
                    
                    # åˆä½µåˆ°ç¸½æ•¸æ“š
                    for key, df in date_data.items():
                        if not df.empty:
                            if key not in all_date_data:
                                all_date_data[key] = []
                            all_date_data[key].append(df)
                    
                    # éœé»˜è¨˜æ†¶é«”æ¸…ç†
                    self._monitor_memory_and_gc()
            
            # åˆä½µå„æ—¥æœŸçš„æ•¸æ“š
            for key, df_list in all_date_data.items():
                if df_list:
                    combined_df = pd.concat(df_list, ignore_index=True)
                    combined_df = self._optimize_dataframe_memory(combined_df)
                    classified_data[key] = combined_df
                    print(f"   âœ… {key}: {len(combined_df):,} ç­†è¨˜éŒ„")
                    
                    # æ¸…ç†è‡¨æ™‚æ•¸æ“š
                    del df_list
                    self._monitor_memory_and_gc()
        
        return classified_data
    
    def _load_date_classified_data_silent(self, file_paths: Dict[str, Path], date_str: str) -> Dict[str, pd.DataFrame]:
        """éœé»˜è¼‰å…¥ç‰¹å®šæ—¥æœŸçš„åˆ†é¡æ•¸æ“š"""
        file_mappings = {
            'all': file_paths['main_csv'],
            'peak': file_paths['peak_csv'],
            'offpeak': file_paths['offpeak_csv'],
            'target_route': file_paths['target_route_csv'],
            'target_route_peak': file_paths['target_route_peak_csv'],
            'target_route_offpeak': file_paths['target_route_offpeak_csv']
        }
        
        classified_data = {}
        
        for name, file_path in file_mappings.items():
            if file_path.exists():
                try:
                    # è¨˜æ†¶é«”å„ªåŒ–è¼‰å…¥
                    if self.chunk_processing and file_path.stat().st_size > 50 * 1024 * 1024:  # 50MBä»¥ä¸Šä½¿ç”¨chunk
                        df_chunks = pd.read_csv(file_path, engine='c', low_memory=True, chunksize=10000)
                        df = pd.concat(df_chunks, ignore_index=True)
                    else:
                        df = pd.read_csv(file_path, engine='c', low_memory=True)
                    
                    df = self._optimize_dataframe_memory(df)
                    classified_data[name] = df
                    print(f"      âœ… {date_str} {name}: {len(df):,} ç­†è¨˜éŒ„")
                    
                    # éœé»˜è¨˜æ†¶é«”ç›£æ§
                    self._monitor_memory_and_gc()
                    
                except Exception:
                    classified_data[name] = pd.DataFrame()
            else:
                classified_data[name] = pd.DataFrame()
        
        return classified_data
    
    def list_available_dates(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨çš„æ—¥æœŸè³‡æ–™å¤¾"""
        available_dates = []
        
        if self.processed_base_folder.exists():
            for date_folder in self.processed_base_folder.iterdir():
                if date_folder.is_dir() and date_folder.name.count('-') == 2:
                    available_dates.append(date_folder.name)
        
        return sorted(available_dates)
    
    def get_date_summary(self) -> Dict[str, Any]:
        """ç²å–å„æ—¥æœŸæ•¸æ“šæ‘˜è¦"""
        print("ğŸ“Š ç”Ÿæˆæ—¥æœŸæ‘˜è¦...")
        
        date_summary = {
            "ç¸½è¦½": {
                "å¯ç”¨æ—¥æœŸæ•¸": 0,
                "ç¸½è¨˜éŒ„æ•¸": 0,
                "æ—¥æœŸç¯„åœ": {"æœ€æ—©": None, "æœ€æ™š": None}
            },
            "å„æ—¥æœŸè©³æƒ…": {}
        }
        
        available_dates = self.list_available_dates()
        date_summary["ç¸½è¦½"]["å¯ç”¨æ—¥æœŸæ•¸"] = len(available_dates)
        
        if available_dates:
            date_summary["ç¸½è¦½"]["æ—¥æœŸç¯„åœ"]["æœ€æ—©"] = available_dates[0]
            date_summary["ç¸½è¦½"]["æ—¥æœŸç¯„åœ"]["æœ€æ™š"] = available_dates[-1]
        
        total_records = 0
        
        for date_str in available_dates:
            date_folder = self.processed_base_folder / date_str
            file_paths = self.get_date_file_paths(date_folder)
            
            main_csv = file_paths['main_csv']
            if main_csv.exists():
                try:
                    # åªè®€å–ç¬¬ä¸€è¡Œä¾†æª¢æŸ¥çµæ§‹
                    df_sample = pd.read_csv(main_csv, nrows=1)
                    
                    # å¾æª”æ¡ˆå¤§å°ä¼°ç®—è¨˜éŒ„æ•¸
                    file_size = main_csv.stat().st_size
                    estimated_records = int(file_size / 1024 * 10)
                    
                    date_summary["å„æ—¥æœŸè©³æƒ…"][date_str] = {
                        "ä¸»æª”æ¡ˆå­˜åœ¨": True,
                        "é ä¼°è¨˜éŒ„æ•¸": estimated_records,
                        "æª”æ¡ˆå¤§å°MB": round(file_size / 1024 / 1024, 1),
                        "æ¬„ä½æ•¸": len(df_sample.columns)
                    }
                    
                    total_records += estimated_records
                    del df_sample
                    
                except Exception as e:
                    date_summary["å„æ—¥æœŸè©³æƒ…"][date_str] = {
                        "ä¸»æª”æ¡ˆå­˜åœ¨": True,
                        "éŒ¯èª¤": str(e)
                    }
            else:
                date_summary["å„æ—¥æœŸè©³æƒ…"][date_str] = {
                    "ä¸»æª”æ¡ˆå­˜åœ¨": False
                }
        
        date_summary["ç¸½è¦½"]["ç¸½è¨˜éŒ„æ•¸"] = total_records
        
        return date_summary


# ä¾¿åˆ©å‡½æ•¸
def process_all_files_one_shot(folder_path: str = "data") -> pd.DataFrame:
    """ä¸€æ¬¡æ€§è™•ç†æ‰€æœ‰æª”æ¡ˆ - ç°¡åŒ–ç‰ˆ"""
    loader = VDDataLoader(base_folder=folder_path)
    return loader.process_all_files()


def load_classified_data_quick(folder_path: str = "data", target_date: str = None) -> Dict[str, pd.DataFrame]:
    """å¿«é€Ÿè¼‰å…¥å·²åˆ†é¡æ•¸æ“š - ç°¡åŒ–ç‰ˆ"""
    loader = VDDataLoader(base_folder=folder_path)
    return loader.load_classified_data(target_date=target_date)


def get_date_summary_quick(folder_path: str = "data") -> Dict[str, Any]:
    """å¿«é€Ÿç²å–æ—¥æœŸæ‘˜è¦ - ç°¡åŒ–ç‰ˆ"""
    loader = VDDataLoader(base_folder=folder_path)
    return loader.get_date_summary()


if __name__ == "__main__":
    print("ğŸš€ VDæ•¸æ“šè¼‰å…¥å™¨ - ç°¡åŒ–ç‰ˆï¼ˆéœé»˜è¨˜æ†¶é«”å„ªåŒ–ï¼‰")
    print("=" * 70)
    print("ç‰¹è‰²ï¼šå°ˆæ³¨Rawè™•ç† + å¾Œå°è¨˜æ†¶é«”å„ªåŒ–")
    print("=" * 70)
    
    loader = VDDataLoader()
    
    # é¡¯ç¤ºå¯ç”¨æ—¥æœŸ
    available_dates = loader.list_available_dates()
    if available_dates:
        print(f"\nğŸ“… å·²è™•ç†æ—¥æœŸ:")
        for date_str in available_dates:
            print(f"   â€¢ {date_str}")
    
    # æª¢æŸ¥rawè³‡æ–™å¤¾
    status = loader.check_raw_folder()
    
    if status["unprocessed"] > 0:
        print(f"\nğŸ¯ ç™¼ç¾ {status['unprocessed']} å€‹å¾…è™•ç†æª”æ¡ˆ")
        print("ç°¡åŒ–ç‰ˆç‰¹è‰²ï¼š")
        print("   â€¢ å¾Œå°è‡ªå‹•è¨˜æ†¶é«”å„ªåŒ–ï¼Œä¸é¡¯ç¤ºè©³ç´°ä¿¡æ¯")
        print("   â€¢ å°ˆæ³¨Rawæ•¸æ“šè™•ç†é€²åº¦")
        print("   â€¢ æ™ºæ…§Archiveæª¢æŸ¥ï¼Œé¿å…é‡è¤‡è™•ç†")
        print("   â€¢ æŒ‰æ—¥æœŸçµ„ç¹”è¼¸å‡º")
        
        response = input(f"\né–‹å§‹è™•ç†Rawæ•¸æ“šï¼Ÿ(y/N): ")
        if response.lower() in ['y', 'yes']:
            df = loader.process_all_files()
            
            if not df.empty:
                print(f"\nğŸ‰ Rawæ•¸æ“šè™•ç†å®Œæˆï¼")
                
                # é¡¯ç¤ºæ—¥æœŸæ‘˜è¦
                date_summary = loader.get_date_summary()
                print(f"\nğŸ“Š è™•ç†çµæœ:")
                print(f"   ğŸ“… è™•ç†æ—¥æœŸæ•¸: {date_summary['ç¸½è¦½']['å¯ç”¨æ—¥æœŸæ•¸']}")
                print(f"   ğŸ“Š ç¸½è¨˜éŒ„æ•¸: {date_summary['ç¸½è¦½']['ç¸½è¨˜éŒ„æ•¸']:,}")
                print(f"   ğŸ“† æ—¥æœŸç¯„åœ: {date_summary['ç¸½è¦½']['æ—¥æœŸç¯„åœ']['æœ€æ—©']} ~ {date_summary['ç¸½è¦½']['æ—¥æœŸç¯„åœ']['æœ€æ™š']}")
                
                print(f"\nğŸ“ è¼¸å‡ºçµæ§‹:")
                for date_str in loader.list_available_dates():
                    print(f"   ğŸ“‚ data/processed/{date_str}/")
                    print(f"      â”œâ”€â”€ vd_data_all.csv + _summary.json")
                    print(f"      â”œâ”€â”€ vd_data_peak.csv + _summary.json")
                    print(f"      â”œâ”€â”€ vd_data_offpeak.csv + _summary.json")
                    print(f"      â”œâ”€â”€ target_route_*.csv + _summary.json")
                    print(f"      â””â”€â”€ processed_files.json")
    else:
        if available_dates:
            print(f"\nâœ… å·²æœ‰ {len(available_dates)} å€‹æ—¥æœŸçš„è™•ç†æ•¸æ“š")
            
            # é¡¯ç¤ºæ—¥æœŸæ‘˜è¦
            date_summary = loader.get_date_summary()
            print(f"\nğŸ“Š ç¾æœ‰æ•¸æ“šæ‘˜è¦:")
            for date_str, details in date_summary["å„æ—¥æœŸè©³æƒ…"].items():
                if details.get("ä¸»æª”æ¡ˆå­˜åœ¨"):
                    print(f"   ğŸ“… {date_str}: {details.get('é ä¼°è¨˜éŒ„æ•¸', 0):,} ç­†è¨˜éŒ„ ({details.get('æª”æ¡ˆå¤§å°MB', 0):.1f}MB)")
        else:
            print("ğŸ’¡ è«‹å°‡XMLæª”æ¡ˆæ”¾å…¥ data/raw/ è³‡æ–™å¤¾")
    
    print(f"\nğŸ’¡ ç°¡åŒ–ç‰ˆä½¿ç”¨æ–¹æ³•:")
    print(f"   # è™•ç†æ‰€æœ‰Rawæª”æ¡ˆï¼ˆå¾Œå°è¨˜æ†¶é«”å„ªåŒ–ï¼‰")
    print(f"   loader = VDDataLoader()")
    print(f"   df = loader.process_all_files()")
    print(f"   ")
    print(f"   # è¼‰å…¥ç‰¹å®šæ—¥æœŸæ•¸æ“š")
    print(f"   date_data = loader.load_classified_data(target_date='2025-06-27')")
    print(f"   ")
    print(f"   # ç²å–æ—¥æœŸæ‘˜è¦")
    print(f"   summary = loader.get_date_summary()")
    
    print(f"\nğŸ¯ ç°¡åŒ–ç‰ˆå„ªå‹¢:")
    print(f"   ğŸ¯ å°ˆæ³¨Rawè™•ç†ï¼šä¸»è¦é¡¯ç¤ºè™•ç†é€²åº¦")
    print(f"   ğŸ’¾ å¾Œå°è¨˜æ†¶é«”å„ªåŒ–ï¼šè‡ªå‹•ç®¡ç†ï¼Œä¸å¹²æ“¾ç”¨æˆ¶")
    print(f"   ğŸ“‚ æ™ºæ…§Archiveæª¢æŸ¥ï¼šå¿«é€Ÿæª¢æŸ¥ï¼Œé¿å…é‡è¤‡")
    print(f"   ğŸ“Š ç°¡æ½”è¼¸å‡ºï¼šåªé¡¯ç¤ºé‡è¦ä¿¡æ¯")
    print(f"   âš¡ ä¿æŒé«˜é€Ÿï¼šç¶­æŒ3-5åˆ†é˜è™•ç†åƒè¬ç­†è¨˜éŒ„")
    print(f"   ğŸ”„ å®Œæ•´åŠŸèƒ½ï¼šä¿ç•™æ‰€æœ‰åŸåŠŸèƒ½")