# src/data_loader.py - å¼·åŒ–ç‰ˆå½ˆæ€§è™•ç†

"""
VDæ•¸æ“šè¼‰å…¥å™¨ - å¼·åŒ–ç‰ˆå½ˆæ€§è™•ç†
===============================

æ ¸å¿ƒç‰¹æ€§ï¼š
1. ğŸ”„ å½ˆæ€§æ•¸æ“šé‡æª¢æ¸¬ - è‡ªå‹•æª¢æ¸¬rawè³‡æ–™å¤¾æª”æ¡ˆæ•¸é‡
2. ğŸ’¾ ç©æ¥µè¨˜æ†¶é«”ç®¡ç† - åˆ†æ®µè™•ç†ï¼Œå³æ™‚é‡‹æ”¾è¨˜æ†¶é«”
3. ğŸ¯ ç²¾æº–è·¯æ®µç¯©é¸ - åœ“å±±ã€å°åŒ—ã€ä¸‰é‡è·¯æ®µå°ˆé …è™•ç†
4. ğŸ“ æ¨™æº–åŒ–è¼¸å‡ºæ ¼å¼ - çµ±ä¸€ä¸‰å€‹ç›®æ¨™æª”æ¡ˆ
5. ğŸ·ï¸ åŸæª”åæ­¸æª” - ä¿æŒåŸå§‹æª”æ¡ˆåç¨±
"""

import xml.etree.ElementTree as ET
import pandas as pd
import numpy as np
from datetime import datetime
import os
import json
import shutil
import threading
import time as time_module
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Tuple, Callable, Optional
import warnings
import gc
import psutil
warnings.filterwarnings('ignore')


class FlexibleResourceManager:
    """å½ˆæ€§è³‡æºç®¡ç†å™¨"""
    
    def __init__(self, target_memory_percent: float = 60.0):
        self.target_memory_percent = target_memory_percent
        self.safe_memory_percent = 55.0  # å®‰å…¨è¨˜æ†¶é«”é–¾å€¼
        self.critical_memory_percent = 80.0  # è‡¨ç•Œè¨˜æ†¶é«”é–¾å€¼
        self.min_batch_size = 10
        self.max_batch_size = 100
        self.current_batch_size = 50
        
    def get_memory_status(self) -> Dict[str, float]:
        """ç²å–è¨˜æ†¶é«”ç‹€æ…‹"""
        memory = psutil.virtual_memory()
        return {
            'percent': memory.percent,
            'available_gb': memory.available / (1024**3),
            'used_gb': memory.used / (1024**3),
            'total_gb': memory.total / (1024**3)
        }
    
    def should_pause_processing(self) -> bool:
        """æ˜¯å¦æ‡‰è©²æš«åœè™•ç†"""
        memory_percent = psutil.virtual_memory().percent
        return memory_percent > self.critical_memory_percent
    
    def should_force_gc(self) -> bool:
        """æ˜¯å¦å¼·åˆ¶åƒåœ¾å›æ”¶"""
        memory_percent = psutil.virtual_memory().percent
        return memory_percent > self.target_memory_percent
    
    def adjust_batch_size(self, current_memory: float) -> int:
        """èª¿æ•´æ‰¹æ¬¡å¤§å°"""
        if current_memory > 75:
            self.current_batch_size = max(self.min_batch_size, int(self.current_batch_size * 0.7))
        elif current_memory < 45:
            self.current_batch_size = min(self.max_batch_size, int(self.current_batch_size * 1.3))
        
        return self.current_batch_size


class VDDataLoader:
    """VDæ•¸æ“šè¼‰å…¥å™¨ - å¼·åŒ–ç‰ˆå½ˆæ€§è™•ç†"""
    
    def __init__(self, base_folder: str = "data", max_workers: int = None, 
                 target_memory_percent: float = 60.0, verbose: bool = True):
        """åˆå§‹åŒ–è¼‰å…¥å™¨"""
        self.base_folder = Path(base_folder)
        self.raw_folder = self.base_folder / "raw"
        self.processed_base_folder = self.base_folder / "processed"
        self.archive_folder = self.base_folder / "archive"
        self.verbose = verbose
        
        # å‰µå»ºå¿…è¦è³‡æ–™å¤¾
        for folder in [self.raw_folder, self.processed_base_folder, self.archive_folder]:
            folder.mkdir(parents=True, exist_ok=True)
        
        # è³‡æºç®¡ç†å™¨
        self.resource_manager = FlexibleResourceManager(target_memory_percent)
        
        # ç·šç¨‹æ•¸è¨­å®š
        self.max_workers = max_workers or max(2, min(os.cpu_count() or 4, 6))
        
        # XMLå‘½åç©ºé–“
        self.namespace = {
            'traffic': 'http://traffic.transportdata.tw/standard/traffic/schema/'
        }
        
        # ç›®æ¨™è·¯æ®µé—œéµå­—
        self.target_keywords = ['åœ“å±±', 'å°åŒ—', 'ä¸‰é‡', 'N1-N-2', 'N1-S-2']
        
        # ç·šç¨‹é–
        self.file_lock = threading.Lock()
        self.progress_callback = None
        
        print(f"ğŸš€ VDæ•¸æ“šè¼‰å…¥å™¨å¼·åŒ–ç‰ˆåˆå§‹åŒ–")
        print(f"   ğŸ“ è³‡æ–™å¤¾: {self.base_folder}")
        print(f"   ğŸ§µ ç·šç¨‹æ•¸: {self.max_workers}")
        print(f"   ğŸ’¾ ç›®æ¨™è¨˜æ†¶é«”: {target_memory_percent}%")
    
    def scan_raw_files(self) -> Dict[str, Any]:
        """å½ˆæ€§æƒærawè³‡æ–™å¤¾"""
        print("ğŸ” å½ˆæ€§æƒærawè³‡æ–™å¤¾...")
        
        if not self.raw_folder.exists():
            return {"exists": False, "file_count": 0, "files": []}
        
        # æƒææ‰€æœ‰å¯èƒ½çš„æª”æ¡ˆé¡å‹
        xml_files = list(self.raw_folder.rglob("*.xml"))
        txt_files = list(self.raw_folder.rglob("*.txt"))
        all_files = xml_files + txt_files
        
        # ç¯©é¸VDç›¸é—œæª”æ¡ˆ
        vd_files = []
        for file_path in all_files:
            if self._is_vd_file(file_path):
                vd_files.append(file_path)
        
        # æª¢æŸ¥å·²è™•ç†æª”æ¡ˆ
        archived_files = self._get_archived_files()
        unprocessed_files = []
        
        for file_path in vd_files:
            if file_path.name not in archived_files:
                unprocessed_files.append(file_path)
        
        result = {
            "exists": True,
            "file_count": len(vd_files),
            "unprocessed_count": len(unprocessed_files),
            "unprocessed_files": unprocessed_files,
            "processed_count": len(vd_files) - len(unprocessed_files)
        }
        
        print(f"   ğŸ“Š æƒæçµæœ:")
        print(f"      â€¢ ç¸½VDæª”æ¡ˆ: {result['file_count']}")
        print(f"      â€¢ å¾…è™•ç†: {result['unprocessed_count']}")
        print(f"      â€¢ å·²è™•ç†: {result['processed_count']}")
        
        return result
    
    def _is_vd_file(self, file_path: Path) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºVDæª”æ¡ˆ"""
        name_lower = file_path.name.lower()
        keywords = ['vd', 'traffic', 'detector', 'éœæ…‹', 'å‹•æ…‹']
        return any(keyword in name_lower for keyword in keywords)
    
    def _get_archived_files(self) -> set:
        """ç²å–å·²æ­¸æª”çš„æª”æ¡ˆåç¨±"""
        archived_files = set()
        
        if not self.archive_folder.exists():
            return archived_files
        
        for date_folder in self.archive_folder.iterdir():
            if date_folder.is_dir():
                for archived_file in date_folder.iterdir():
                    if archived_file.is_file():
                        # æå–åŸå§‹æª”æ¡ˆåç¨±
                        original_name = self._extract_original_filename(archived_file.name)
                        if original_name:
                            archived_files.add(original_name)
        
        return archived_files
    
    def _extract_original_filename(self, archived_name: str) -> str:
        """å¾æ­¸æª”æª”æ¡ˆåæå–åŸå§‹æª”æ¡ˆå"""
        # ç§»é™¤æ™‚é–“æˆ³å‰ç¶´ (æ ¼å¼: YYYYMMDD_HHMMSS_åŸæª”å)
        parts = archived_name.split('_', 2)
        if len(parts) >= 3:
            return parts[2]
        return archived_name
    
    def process_all_files_flexible(self) -> pd.DataFrame:
        """å½ˆæ€§è™•ç†æ‰€æœ‰æª”æ¡ˆ"""
        print("ğŸš€ é–‹å§‹å½ˆæ€§è™•ç†")
        print("=" * 50)
        
        # æƒææª”æ¡ˆ
        scan_result = self.scan_raw_files()
        
        if not scan_result["exists"] or scan_result["unprocessed_count"] == 0:
            print("ğŸ“‚ è¼‰å…¥ç¾æœ‰æ•¸æ“š...")
            return self.load_existing_data()
        
        unprocessed_files = scan_result["unprocessed_files"]
        total_files = len(unprocessed_files)
        
        print(f"ğŸ“‹ è™•ç†è¨ˆåŠƒ: {total_files} æª”æ¡ˆ")
        
        # åˆ†æ®µè™•ç†
        processed_count = 0
        all_date_data = {}
        
        start_time = time_module.time()
        
        while processed_count < total_files:
            # æª¢æŸ¥è¨˜æ†¶é«”ç‹€æ³
            memory_status = self.resource_manager.get_memory_status()
            
            if self.resource_manager.should_pause_processing():
                print(f"   âš ï¸ è¨˜æ†¶é«”ä½¿ç”¨éé«˜({memory_status['percent']:.1f}%)ï¼ŒåŸ·è¡Œæ¸…ç†...")
                self._aggressive_cleanup()
                continue
            
            # å‹•æ…‹èª¿æ•´æ‰¹æ¬¡å¤§å°
            batch_size = self.resource_manager.adjust_batch_size(memory_status['percent'])
            
            # è™•ç†æ‰¹æ¬¡
            batch_start = processed_count
            batch_end = min(batch_start + batch_size, total_files)
            batch_files = unprocessed_files[batch_start:batch_end]
            
            if self.verbose:
                print(f"   ğŸ“¦ æ‰¹æ¬¡è™•ç† {batch_start+1}-{batch_end}/{total_files} "
                      f"(è¨˜æ†¶é«”: {memory_status['percent']:.1f}%, æ‰¹æ¬¡: {len(batch_files)})")
            
            # è™•ç†æ‰¹æ¬¡æª”æ¡ˆ
            batch_data = self._process_batch_safe(batch_files)
            
            # åˆä½µæ‰¹æ¬¡æ•¸æ“š
            for date_str, data_list in batch_data.items():
                if date_str not in all_date_data:
                    all_date_data[date_str] = []
                all_date_data[date_str].extend(data_list)
            
            processed_count = batch_end
            
            # é€²åº¦å ±å‘Š
            if processed_count % 100 == 0 or processed_count == total_files:
                elapsed = time_module.time() - start_time
                speed = processed_count / elapsed if elapsed > 0 else 0
                total_records = sum(len(data_list) for data_list in all_date_data.values())
                
                print(f"   ğŸ“ˆ é€²åº¦: {processed_count}/{total_files} ({processed_count/total_files*100:.1f}%) "
                      f"| é€Ÿåº¦: {speed:.1f} æª”æ¡ˆ/ç§’ | è¨˜éŒ„: {total_records:,} "
                      f"| è¨˜æ†¶é«”: {memory_status['percent']:.1f}%")
                
                # é€²åº¦å›èª¿
                if self.progress_callback:
                    try:
                        self.progress_callback({
                            'progress': processed_count/total_files*100,
                            'memory_usage': memory_status['percent'],
                            'records': total_records
                        })
                    except:
                        pass
            
            # æ¸…ç†æ‰¹æ¬¡æ•¸æ“š
            del batch_data
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦åƒåœ¾å›æ”¶
            if self.resource_manager.should_force_gc():
                gc.collect()
            
            # é˜²æ­¢è™•ç†éå¿«å°è‡´ç³»çµ±è² è¼‰éé«˜
            time_module.sleep(0.1)
        
        # ä¿å­˜æœ€çµ‚æ•¸æ“š
        return self._save_final_data_flexible(all_date_data)
    
    def _process_batch_safe(self, batch_files: List[Path]) -> Dict[str, List]:
        """å®‰å…¨çš„æ‰¹æ¬¡è™•ç†"""
        batch_data = {}
        
        # è¼ƒä¿å®ˆçš„ç·šç¨‹æ•¸
        safe_workers = min(self.max_workers, 4)
        
        with ThreadPoolExecutor(max_workers=safe_workers) as executor:
            future_to_file = {
                executor.submit(self._process_single_file_safe, file_path): file_path
                for file_path in batch_files
            }
            
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                
                try:
                    result = future.result(timeout=60)  # å¢åŠ è¶…æ™‚æ™‚é–“
                    
                    if result and result.get("success") and "data" in result:
                        date_str = result["xml_timestamp"].strftime('%Y-%m-%d')
                        
                        if date_str not in batch_data:
                            batch_data[date_str] = []
                        batch_data[date_str].extend(result["data"])
                        
                        # ç«‹å³æ­¸æª”ï¼ˆä½¿ç”¨åŸæª”åï¼‰
                        self._archive_file_original_name(file_path, result["xml_timestamp"])
                        
                except Exception as e:
                    if self.verbose:
                        print(f"   âš ï¸ æª”æ¡ˆè™•ç†å¤±æ•—: {file_path.name} - {e}")
        
        return batch_data
    
    def _process_single_file_safe(self, file_path: Path) -> Dict[str, Any]:
        """å®‰å…¨çš„å–®æª”è™•ç†"""
        try:
            # è®€å–æª”æ¡ˆå…§å®¹
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # è§£æXML
            root = ET.fromstring(content)
            
            # æå–æ™‚é–“
            update_time = self._extract_update_time(root)
            date_str = update_time.strftime('%Y-%m-%d')
            
            # æå–ç›®æ¨™è·¯æ®µæ•¸æ“š
            target_data = []
            
            for vd_live in root.findall('.//traffic:VDLive', self.namespace):
                vd_id_element = vd_live.find('traffic:VDID', self.namespace)
                if vd_id_element is None:
                    continue
                
                vd_id = vd_id_element.text or ""
                
                # æª¢æŸ¥æ˜¯å¦ç‚ºç›®æ¨™è·¯æ®µ
                if not self._is_target_route(vd_id):
                    continue
                
                # æå–è»Šé“æ•¸æ“š
                for lane in vd_live.findall('.//traffic:Lane', self.namespace):
                    try:
                        lane_data = self._extract_lane_data(lane, vd_id, date_str, update_time)
                        target_data.append(lane_data)
                    except:
                        continue
            
            # æ¸…ç†XMLå°è±¡
            del root, content
            
            return {
                "success": True,
                "file_name": file_path.name,
                "record_count": len(target_data),
                "xml_timestamp": update_time,
                "data": target_data
            }
            
        except Exception as e:
            return {"success": False, "file_name": file_path.name, "error": str(e)}
    
    def _is_target_route(self, vd_id: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºç›®æ¨™è·¯æ®µ"""
        if not isinstance(vd_id, str):
            return False
        
        # æª¢æŸ¥é—œéµå­—
        for keyword in self.target_keywords:
            if keyword in vd_id:
                return True
        
        # æª¢æŸ¥åœ‹é“1è™Ÿåœ“å±±-ä¸‰é‡è·¯æ®µçš„é‡Œç¨‹æ•¸
        if 'N1' in vd_id:
            # æå–é‡Œç¨‹æ•¸
            parts = vd_id.split('-')
            for part in parts:
                try:
                    if '.' in part:
                        km = float(part)
                    else:
                        km = int(part)
                    
                    # åœ“å±±-ä¸‰é‡è·¯æ®µå¤§ç´„åœ¨20-30å…¬é‡Œ
                    if 20 <= km <= 30:
                        return True
                except:
                    continue
        
        return False
    
    def _extract_update_time(self, root) -> datetime:
        """æå–æ›´æ–°æ™‚é–“"""
        try:
            update_time_element = root.find('traffic:UpdateTime', self.namespace)
            if update_time_element is not None:
                time_str = update_time_element.text.replace('+08:00', '')
                return datetime.strptime(time_str, '%Y-%m-%dT%H:%M:%S')
        except:
            pass
        return datetime.now()
    
    def _extract_lane_data(self, lane_element, vd_id: str, date_str: str, 
                          update_time: datetime) -> Dict[str, Any]:
        """æå–è»Šé“æ•¸æ“š"""
        def safe_get_int(element, tag_name, default=0):
            elem = element.find(f'traffic:{tag_name}', self.namespace)
            try:
                return int(elem.text) if elem is not None and elem.text else default
            except:
                return default
        
        # åŸºæœ¬è³‡è¨Š
        lane_id = safe_get_int(lane_element, 'LaneID')
        lane_type = safe_get_int(lane_element, 'LaneType')
        speed = safe_get_int(lane_element, 'Speed')
        occupancy = safe_get_int(lane_element, 'Occupancy')
        
        # è»Šç¨®æ•¸æ“š
        volume_small = volume_large = volume_truck = 0
        speed_small = speed_large = speed_truck = 0
        
        for vehicle in lane_element.findall('traffic:Vehicles/traffic:Vehicle', self.namespace):
            vehicle_type_elem = vehicle.find('traffic:VehicleType', self.namespace)
            if vehicle_type_elem is None:
                continue
            
            vehicle_type = vehicle_type_elem.text
            volume = safe_get_int(vehicle, 'Volume')
            v_speed = safe_get_int(vehicle, 'Speed')
            
            if vehicle_type == 'S':  # å°è»Š
                volume_small, speed_small = volume, v_speed
            elif vehicle_type == 'L':  # å¤§è»Š
                volume_large, speed_large = volume, v_speed
            elif vehicle_type == 'T':  # å¡è»Š
                volume_truck, speed_truck = volume, v_speed
        
        return {
            'date': date_str,
            'update_time': update_time,
            'vd_id': vd_id,
            'lane_id': lane_id,
            'lane_type': lane_type,
            'speed': speed,
            'occupancy': occupancy,
            'volume_total': volume_small + volume_large + volume_truck,
            'volume_small': volume_small,
            'volume_large': volume_large,
            'volume_truck': volume_truck,
            'speed_small': speed_small,
            'speed_large': speed_large,
            'speed_truck': speed_truck
        }
    
    def _archive_file_original_name(self, file_path: Path, xml_timestamp: datetime) -> str:
        """ä½¿ç”¨åŸæª”åæ­¸æª”æª”æ¡ˆ"""
        try:
            archive_date_folder = self.archive_folder / xml_timestamp.strftime("%Y-%m-%d")
            archive_date_folder.mkdir(exist_ok=True)
            
            # ä½¿ç”¨åŸæª”åï¼ŒåªåŠ æ™‚é–“æˆ³å‰ç¶´é¿å…é‡å
            new_filename = f"{xml_timestamp.strftime('%Y%m%d_%H%M%S')}_{file_path.name}"
            archive_path = archive_date_folder / new_filename
            
            shutil.move(str(file_path), str(archive_path))
            return str(archive_path)
        except:
            return ""
    
    def _save_final_data_flexible(self, all_date_data: Dict[str, List]) -> pd.DataFrame:
        """å½ˆæ€§ä¿å­˜æœ€çµ‚æ•¸æ“š"""
        print(f"\nğŸ“Š ä¿å­˜è™•ç†çµæœ...")
        
        all_data = []
        
        for date_str, data_list in all_date_data.items():
            if not data_list:
                continue
            
            print(f"   ğŸ’¾ è™•ç† {date_str}: {len(data_list):,} ç­†è¨˜éŒ„")
            
            # å‰µå»ºDataFrame
            df = pd.DataFrame(data_list)
            
            # å„ªåŒ–è¨˜æ†¶é«”
            df = self._optimize_dataframe_memory(df)
            
            # åˆ†é¡ä¸¦ä¿å­˜
            self._save_date_target_files(df, date_str)
            
            all_data.extend(data_list)
            
            # æ¸…ç†
            del df, data_list
            
            # åƒåœ¾å›æ”¶
            if self.resource_manager.should_force_gc():
                gc.collect()
        
        # è¿”å›åˆä½µçµæœ
        if all_data:
            final_df = pd.DataFrame(all_data)
            final_df = self._optimize_dataframe_memory(final_df)
            
            print(f"ğŸ¯ è™•ç†å®Œæˆ: {len(final_df):,} ç­†ç›®æ¨™è·¯æ®µè¨˜éŒ„")
            
            # æœ€çµ‚æ¸…ç†
            del all_data
            gc.collect()
            
            return final_df
        
        return pd.DataFrame()
    
    def _save_date_target_files(self, df: pd.DataFrame, date_str: str):
        """ä¿å­˜æŒ‡å®šæ—¥æœŸçš„ç›®æ¨™æª”æ¡ˆ"""
        date_folder = self.processed_base_folder / date_str
        date_folder.mkdir(parents=True, exist_ok=True)
        
        # æ·»åŠ æ™‚é–“åˆ†é¡
        df['time_category'] = self._classify_peak_hours(df['update_time'])
        
        # 1. ç›®æ¨™è·¯æ®µæ•¸æ“š (æ‰€æœ‰ç›®æ¨™è·¯æ®µè¨˜éŒ„)
        target_route_csv = date_folder / "target_route_data.csv"
        df.to_csv(target_route_csv, index=False, encoding='utf-8-sig')
        
        # 2. ç›®æ¨™è·¯æ®µå°–å³°
        peak_mask = df['time_category'].str.contains('å°–å³°', na=False)
        if peak_mask.any():
            peak_df = df[peak_mask].copy()
            target_peak_csv = date_folder / "target_route_peak.csv"
            peak_df.to_csv(target_peak_csv, index=False, encoding='utf-8-sig')
            del peak_df
        
        # 3. ç›®æ¨™è·¯æ®µé›¢å³°
        offpeak_mask = ~peak_mask
        if offpeak_mask.any():
            offpeak_df = df[offpeak_mask].copy()
            target_offpeak_csv = date_folder / "target_route_offpeak.csv"
            offpeak_df.to_csv(target_offpeak_csv, index=False, encoding='utf-8-sig')
            del offpeak_df
        
        # ç”Ÿæˆæ‘˜è¦
        summary = {
            "date": date_str,
            "total_records": len(df),
            "peak_records": peak_mask.sum(),
            "offpeak_records": offpeak_mask.sum(),
            "avg_speed": df['speed'].mean(),
            "avg_volume": df['volume_total'].mean(),
            "unique_vd_count": df['vd_id'].nunique()
        }
        
        summary_path = date_folder / "target_route_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2, default=str)
    
    def _classify_peak_hours(self, datetime_series: pd.Series) -> pd.Series:
        """åˆ†é¡å°–å³°é›¢å³°æ™‚æ®µ"""
        # ç¢ºä¿æ˜¯datetimeé¡å‹
        if not pd.api.types.is_datetime64_any_dtype(datetime_series):
            datetime_series = pd.to_datetime(datetime_series, errors='coerce')
        
        hours = datetime_series.dt.hour
        weekdays = datetime_series.dt.weekday
        
        # é€±æœ«åˆ¤æ–·
        is_weekend = weekdays >= 5
        
        # å°–å³°æ™‚æ®µåˆ¤æ–·
        weekday_peak = ((hours >= 7) & (hours < 9)) | ((hours >= 17) & (hours < 20))
        weekend_peak = ((hours >= 9) & (hours < 12)) | ((hours >= 15) & (hours < 19))
        
        # åˆ†é¡
        result = pd.Series('å¹³æ—¥é›¢å³°', index=datetime_series.index)
        result[weekday_peak & ~is_weekend] = 'å¹³æ—¥å°–å³°'
        result[weekend_peak & is_weekend] = 'å‡æ—¥å°–å³°'
        result[~weekend_peak & is_weekend] = 'å‡æ—¥é›¢å³°'
        
        return result
    
    def _optimize_dataframe_memory(self, df: pd.DataFrame) -> pd.DataFrame:
        """å„ªåŒ–DataFrameè¨˜æ†¶é«”ä½¿ç”¨"""
        if df.empty:
            return df
        
        # æ•¸å€¼é¡å‹å„ªåŒ–
        numeric_columns = ['lane_id', 'lane_type', 'speed', 'occupancy',
                          'volume_total', 'volume_small', 'volume_large', 'volume_truck',
                          'speed_small', 'speed_large', 'speed_truck']
        
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce', downcast='integer')
        
        # é¡åˆ¥é¡å‹å„ªåŒ–
        if 'vd_id' in df.columns:
            df['vd_id'] = df['vd_id'].astype('category')
        
        if 'time_category' in df.columns:
            df['time_category'] = df['time_category'].astype('category')
        
        # æ™‚é–“é¡å‹ç¢ºä¿
        if 'update_time' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['update_time']):
            df['update_time'] = pd.to_datetime(df['update_time'], errors='coerce')
        
        return df
    
    def _aggressive_cleanup(self):
        """ç©æ¥µæ¸…ç†è¨˜æ†¶é«”"""
        print("   ğŸ§¹ åŸ·è¡Œç©æ¥µè¨˜æ†¶é«”æ¸…ç†...")
        
        # å¤šæ¬¡åƒåœ¾å›æ”¶
        for _ in range(3):
            gc.collect()
            time_module.sleep(0.5)
        
        # æª¢æŸ¥æ¸…ç†æ•ˆæœ
        memory_after = psutil.virtual_memory().percent
        print(f"   ğŸ’¾ æ¸…ç†å¾Œè¨˜æ†¶é«”: {memory_after:.1f}%")
        
        # å¦‚æœè¨˜æ†¶é«”ä»ç„¶éé«˜ï¼Œæš«åœæ›´é•·æ™‚é–“
        if memory_after > 75:
            print("   â³ è¨˜æ†¶é«”ä»é«˜ï¼Œç­‰å¾…ç³»çµ±é‡‹æ”¾...")
            time_module.sleep(5)
    
    def load_existing_data(self, target_date: str = None) -> pd.DataFrame:
        """è¼‰å…¥ç¾æœ‰æ•¸æ“š"""
        print("ğŸ“‚ è¼‰å…¥ç¾æœ‰ç›®æ¨™è·¯æ®µæ•¸æ“š...")
        
        available_dates = self.list_available_dates()
        
        if not available_dates:
            print("   âš ï¸ æ²’æœ‰æ‰¾åˆ°å·²è™•ç†æ•¸æ“š")
            return pd.DataFrame()
        
        all_data = []
        
        dates_to_load = [target_date] if target_date else available_dates
        
        for date_str in dates_to_load:
            if date_str not in available_dates:
                continue
            
            date_folder = self.processed_base_folder / date_str
            target_csv = date_folder / "target_route_data.csv"
            
            if target_csv.exists():
                try:
                    df = pd.read_csv(target_csv, engine='c', low_memory=True)
                    df = self._optimize_dataframe_memory(df)
                    all_data.append(df)
                    print(f"   âœ… {date_str}: {len(df):,} ç­†è¨˜éŒ„")
                except Exception as e:
                    print(f"   âŒ {date_str}: è¼‰å…¥å¤±æ•— - {e}")
        
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            combined_df = self._optimize_dataframe_memory(combined_df)
            print(f"   ğŸ¯ ç¸½è¨ˆè¼‰å…¥: {len(combined_df):,} ç­†ç›®æ¨™è·¯æ®µè¨˜éŒ„")
            return combined_df
        
        return pd.DataFrame()
    
    def list_available_dates(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨æ—¥æœŸ"""
        if not self.processed_base_folder.exists():
            return []
        
        dates = []
        for date_folder in self.processed_base_folder.iterdir():
            if date_folder.is_dir() and date_folder.name.count('-') == 2:
                # æª¢æŸ¥æ˜¯å¦æœ‰ç›®æ¨™æª”æ¡ˆ
                target_csv = date_folder / "target_route_data.csv"
                if target_csv.exists():
                    dates.append(date_folder.name)
        
        return sorted(dates)
    
    def check_data_readiness(self) -> Dict[str, Any]:
        """æª¢æŸ¥æ•¸æ“šå°±ç·’åº¦"""
        scan_result = self.scan_raw_files()
        available_dates = self.list_available_dates()
        memory_status = self.resource_manager.get_memory_status()
        
        readiness = {
            'timestamp': datetime.now().isoformat(),
            'raw_files': scan_result,
            'processed_dates': len(available_dates),
            'memory_status': memory_status,
            'overall_readiness': 'unknown',
            'next_action': 'unknown',
            'recommendations': []
        }
        
        # åˆ¤æ–·æ•´é«”å°±ç·’åº¦
        unprocessed_count = scan_result.get('unprocessed_count', 0)
        
        if unprocessed_count > 0:
            readiness['overall_readiness'] = 'raw_processing_needed'
            readiness['next_action'] = 'process_raw_files'
            readiness['recommendations'].append(f"ç™¼ç¾ {unprocessed_count} å€‹å¾…è™•ç†æª”æ¡ˆ")
            
            # è¨˜æ†¶é«”å»ºè­°
            if memory_status['percent'] > 70:
                readiness['recommendations'].append("å»ºè­°å…ˆé—œé–‰å…¶ä»–ç¨‹åºé‡‹æ”¾è¨˜æ†¶é«”")
        elif len(available_dates) > 0:
            readiness['overall_readiness'] = 'ready_for_analysis'
            readiness['next_action'] = 'proceed_to_analysis'
            readiness['recommendations'].append(f"å·²æœ‰ {len(available_dates)} å€‹æ—¥æœŸçš„ç›®æ¨™è·¯æ®µæ•¸æ“š")
        else:
            readiness['overall_readiness'] = 'no_data'
            readiness['next_action'] = 'add_raw_files'
            readiness['recommendations'].append("è«‹å°‡VD XMLæª”æ¡ˆæ”¾å…¥ data/raw/ è³‡æ–™å¤¾")
        
        return readiness
    
    def auto_process_if_needed(self, progress_callback: Callable = None) -> Dict[str, Any]:
        """æ™ºèƒ½è‡ªå‹•è™•ç†"""
        self.progress_callback = progress_callback
        
        readiness = self.check_data_readiness()
        result = {'action_taken': 'none', 'success': False, 'message': '', 'data': None}
        
        if readiness['overall_readiness'] == 'raw_processing_needed':
            try:
                df = self.process_all_files_flexible()
                result['action_taken'] = 'processed_raw_files'
                result['success'] = not df.empty
                result['message'] = f"æˆåŠŸè™•ç† {len(df):,} ç­†ç›®æ¨™è·¯æ®µè¨˜éŒ„" if result['success'] else "è™•ç†å¤±æ•—"
                result['data'] = df
            except Exception as e:
                result['message'] = f"è‡ªå‹•è™•ç†å¤±æ•—: {e}"
        elif readiness['overall_readiness'] == 'ready_for_analysis':
            result['action_taken'] = 'ready_for_analysis'
            result['success'] = True
            result['message'] = "ç›®æ¨™è·¯æ®µæ•¸æ“šå·²å°±ç·’ï¼Œå¯é€²è¡ŒAIåˆ†æ"
        else:
            result['message'] = readiness['recommendations'][0] if readiness['recommendations'] else "ç„¡éœ€è™•ç†"
        
        return result
    
    def get_processing_summary(self) -> Dict[str, Any]:
        """ç²å–è™•ç†æ‘˜è¦"""
        available_dates = self.list_available_dates()
        
        if not available_dates:
            return {'available_dates': 0, 'total_records': 0, 'date_details': {}}
        
        total_records = 0
        date_details = {}
        
        for date_str in available_dates:
            date_folder = self.processed_base_folder / date_str
            summary_file = date_folder / "target_route_summary.json"
            
            if summary_file.exists():
                try:
                    with open(summary_file, 'r', encoding='utf-8') as f:
                        summary = json.load(f)
                    date_details[date_str] = summary
                    total_records += summary.get('total_records', 0)
                except:
                    pass
        
        return {
            'available_dates': len(available_dates),
            'total_records': total_records,
            'date_details': date_details,
            'date_range': {
                'start': available_dates[0] if available_dates else None,
                'end': available_dates[-1] if available_dates else None
            }
        }


# ============================================================
# ä¾¿åˆ©å‡½æ•¸
# ============================================================

def process_target_route_data(folder_path: str = "data", 
                             target_memory_percent: float = 60.0) -> pd.DataFrame:
    """è™•ç†ç›®æ¨™è·¯æ®µæ•¸æ“š"""
    loader = VDDataLoader(base_folder=folder_path, 
                         target_memory_percent=target_memory_percent)
    return loader.process_all_files_flexible()


def load_target_route_data(folder_path: str = "data", 
                          target_date: str = None) -> pd.DataFrame:
    """è¼‰å…¥ç›®æ¨™è·¯æ®µæ•¸æ“š"""
    loader = VDDataLoader(base_folder=folder_path)
    return loader.load_existing_data(target_date)


def check_system_readiness(folder_path: str = "data") -> Dict[str, Any]:
    """æª¢æŸ¥ç³»çµ±å°±ç·’åº¦"""
    loader = VDDataLoader(base_folder=folder_path)
    return loader.check_data_readiness()


def auto_process_data(folder_path: str = "data", 
                     progress_callback: Callable = None) -> Dict[str, Any]:
    """è‡ªå‹•è™•ç†æ•¸æ“š"""
    loader = VDDataLoader(base_folder=folder_path)
    return loader.auto_process_if_needed(progress_callback)


if __name__ == "__main__":
    print("ğŸš€ VDæ•¸æ“šè¼‰å…¥å™¨ - å¼·åŒ–ç‰ˆå½ˆæ€§è™•ç†")
    print("=" * 60)
    print("ğŸ¯ æ ¸å¿ƒç‰¹æ€§:")
    print("   ğŸ”„ å½ˆæ€§æ•¸æ“šé‡æª¢æ¸¬ - è‡ªå‹•æª¢æ¸¬æª”æ¡ˆæ•¸é‡")
    print("   ğŸ’¾ ç©æ¥µè¨˜æ†¶é«”ç®¡ç† - åˆ†æ®µè™•ç†ï¼Œå³æ™‚é‡‹æ”¾")
    print("   ğŸ¯ ç²¾æº–è·¯æ®µç¯©é¸ - åœ“å±±ã€å°åŒ—ã€ä¸‰é‡å°ˆé …")
    print("   ğŸ“ æ¨™æº–åŒ–è¼¸å‡º - target_route_data/peak/offpeak.csv")
    print("   ğŸ·ï¸ åŸæª”åæ­¸æª” - ä¿æŒåŸå§‹æª”æ¡ˆåç¨±")
    print("=" * 60)
    
    # ç¤ºç¯„ä½¿ç”¨
    loader = VDDataLoader(target_memory_percent=60.0, verbose=True)
    
    # æª¢æŸ¥å°±ç·’åº¦
    readiness = loader.check_data_readiness()
    print(f"\nğŸ“Š ç³»çµ±ç‹€æ…‹: {readiness['overall_readiness']}")
    
    # é¡¯ç¤ºæª”æ¡ˆæƒæçµæœ
    raw_files = readiness['raw_files']
    if raw_files['exists']:
        print(f"ğŸ“ Rawæª”æ¡ˆç‹€æ³:")
        print(f"   â€¢ ç¸½æª”æ¡ˆæ•¸: {raw_files['file_count']}")
        print(f"   â€¢ å¾…è™•ç†: {raw_files['unprocessed_count']}")
        print(f"   â€¢ å·²è™•ç†: {raw_files['processed_count']}")
    
    # é¡¯ç¤ºè¨˜æ†¶é«”ç‹€æ³
    memory_status = readiness['memory_status']
    print(f"ğŸ’¾ è¨˜æ†¶é«”ç‹€æ³:")
    print(f"   â€¢ ä½¿ç”¨ç‡: {memory_status['percent']:.1f}%")
    print(f"   â€¢ å¯ç”¨: {memory_status['available_gb']:.1f}GB")
    print(f"   â€¢ ç¸½è¨ˆ: {memory_status['total_gb']:.1f}GB")
    
    # å»ºè­°è¡Œå‹•
    if readiness['recommendations']:
        print(f"ğŸ’¡ ç³»çµ±å»ºè­°:")
        for i, rec in enumerate(readiness['recommendations'], 1):
            print(f"   {i}. {rec}")
    
    if readiness['overall_readiness'] == 'raw_processing_needed':
        print(f"\nğŸš€ å¯åŸ·è¡Œè‡ªå‹•è™•ç†:")
        print(f"   loader = VDDataLoader()")
        print(f"   result = loader.auto_process_if_needed()")
    
    print(f"\nğŸ¯ è¼¸å‡ºæª”æ¡ˆçµæ§‹:")
    print(f"   data/processed/YYYY-MM-DD/")
    print(f"   â”œâ”€â”€ target_route_data.csv     # ç›®æ¨™è·¯æ®µæ•¸æ“š")
    print(f"   â”œâ”€â”€ target_route_peak.csv     # ç›®æ¨™è·¯æ®µå°–å³°")
    print(f"   â”œâ”€â”€ target_route_offpeak.csv  # ç›®æ¨™è·¯æ®µé›¢å³°")
    print(f"   â””â”€â”€ target_route_summary.json # æ•¸æ“šæ‘˜è¦")
    
    print(f"\nğŸ”„ å½ˆæ€§è™•ç†ç‰¹è‰²:")
    print(f"   âœ… è‡ªå‹•æª¢æ¸¬æª”æ¡ˆæ•¸é‡ï¼ˆä¸é™2880å€‹ï¼‰")
    print(f"   âœ… åˆ†æ®µè™•ç†é˜²æ­¢è¨˜æ†¶é«”æº¢å‡º")
    print(f"   âœ… å°ˆæ³¨ç›®æ¨™è·¯æ®µï¼ˆåœ“å±±ã€å°åŒ—ã€ä¸‰é‡ï¼‰")
    print(f"   âœ… æ¨™æº–åŒ–è¼¸å‡ºæ ¼å¼")
    print(f"   âœ… åŸæª”åæ­¸æª”ä¿å­˜")
    print(f"   âœ… ç©æ¥µè¨˜æ†¶é«”æ¸…ç†")
    
    print(f"\nğŸš€ Ready for Flexible Processing! ğŸš€")