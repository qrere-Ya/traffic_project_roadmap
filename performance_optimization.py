# performance_optimization.py - æ€§èƒ½å„ªåŒ–å’Œèª¿åƒ

"""
äº¤é€šé æ¸¬ç³»çµ±æ€§èƒ½å„ªåŒ–å’Œèª¿åƒ
========================

å„ªåŒ–ç›®æ¨™ï¼š
1. æå‡é æ¸¬æº–ç¢ºç‡åˆ°90%+
2. é™ä½éŸ¿æ‡‰æ™‚é–“åˆ°50msä»¥ä¸‹
3. æ¨¡å‹åƒæ•¸è‡ªå‹•èª¿å„ª
4. ç‰¹å¾µé‡è¦æ€§åˆ†æ
5. é æ¸¬èª¤å·®åˆ†æ

å„ªåŒ–ç­–ç•¥ï¼š
- è¶…åƒæ•¸ç¶²æ ¼æœç´¢
- ç‰¹å¾µé¸æ“‡å„ªåŒ–
- æ¨¡å‹ensembleå„ªåŒ–
- é æ¸¬pipelineå„ªåŒ–

ä½œè€…: äº¤é€šé æ¸¬å°ˆæ¡ˆåœ˜éšŠ
æ—¥æœŸ: 2025-01-23
"""

import sys
import os
import time
import json
from pathlib import Path
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ  src ç›®éŒ„åˆ°è·¯å¾‘
sys.path.append('src')

class PerformanceOptimizer:
    """æ€§èƒ½å„ªåŒ–å™¨"""
    
    def __init__(self, debug=True):
        self.debug = debug
        self.optimization_results = {}
        self.best_models = {}
        
        if self.debug:
            print("ğŸ”§ æ€§èƒ½å„ªåŒ–å™¨åˆå§‹åŒ–")
    
    def load_training_data(self, sample_rate=1.0):
        """è¼‰å…¥å®Œæ•´è¨“ç·´æ•¸æ“š"""
        if self.debug:
            print(f"ğŸ“Š è¼‰å…¥è¨“ç·´æ•¸æ“š (æ¡æ¨£ç‡: {sample_rate})")
        
        try:
            from enhanced_predictor import EnhancedPredictor
            
            predictor = EnhancedPredictor(debug=False)
            df = predictor.load_fusion_data(sample_rate=sample_rate)
            X, y = predictor.prepare_features(df)
            
            # æ™‚é–“åºåˆ—åˆ†å‰²
            split_idx = int(len(X) * 0.8)
            
            self.X_train = X[:split_idx]
            self.X_test = X[split_idx:]
            self.y_train = y[:split_idx]
            self.y_test = y[split_idx:]
            self.feature_names = predictor.feature_names
            
            if self.debug:
                print(f"   âœ… è¨“ç·´é›†: {len(self.X_train):,} ç­†")
                print(f"   âœ… æ¸¬è©¦é›†: {len(self.X_test):,} ç­†")
                print(f"   âœ… ç‰¹å¾µæ•¸: {len(self.feature_names)}")
            
            # æ¨™æº–åŒ–ç‰¹å¾µ
            from sklearn.preprocessing import StandardScaler
            self.scaler = StandardScaler()
            self.X_train_scaled = self.scaler.fit_transform(self.X_train)
            self.X_test_scaled = self.scaler.transform(self.X_test)
            
            return True
            
        except Exception as e:
            if self.debug:
                print(f"âŒ æ•¸æ“šè¼‰å…¥å¤±æ•—: {e}")
            return False
    
    def optimize_xgboost_parameters(self):
        """XGBoostè¶…åƒæ•¸å„ªåŒ–"""
        if self.debug:
            print("\nğŸ¯ XGBoostè¶…åƒæ•¸å„ªåŒ–")
            print("-" * 40)
        
        try:
            import xgboost as xgb
            
            # å®šç¾©åƒæ•¸æœç´¢ç©ºé–“
            param_grid = {
                'n_estimators': [100, 200, 300],
                'max_depth': [6, 8, 10],
                'learning_rate': [0.05, 0.1, 0.15],
                'subsample': [0.8, 0.9, 1.0],
                'colsample_bytree': [0.8, 0.9, 1.0]
            }
            
            if self.debug:
                print(f"ğŸ” åƒæ•¸æœç´¢ç©ºé–“: {len(param_grid['n_estimators']) * len(param_grid['max_depth']) * len(param_grid['learning_rate'])} çµ„åˆ")
            
            # ä½¿ç”¨RandomizedSearchCVåŠ é€Ÿæœç´¢
            xgb_model = xgb.XGBRegressor(random_state=42, n_jobs=-1)
            
            random_search = RandomizedSearchCV(
                xgb_model,
                param_grid,
                n_iter=20,  # éš¨æ©Ÿæœç´¢20çµ„åƒæ•¸
                cv=3,       # 3æŠ˜äº¤å‰é©—è­‰
                scoring='r2',
                n_jobs=-1,
                random_state=42
            )
            
            start_time = time.time()
            random_search.fit(self.X_train_scaled, self.y_train)
            search_time = time.time() - start_time
            
            # æœ€ä½³åƒæ•¸
            best_params = random_search.best_params_
            best_score = random_search.best_score_
            
            if self.debug:
                print(f"   â±ï¸ æœç´¢æ™‚é–“: {search_time:.1f} ç§’")
                print(f"   ğŸ† æœ€ä½³CVåˆ†æ•¸: {best_score:.3f}")
                print(f"   ğŸ“Š æœ€ä½³åƒæ•¸:")
                for param, value in best_params.items():
                    print(f"     {param}: {value}")
            
            # ä½¿ç”¨æœ€ä½³åƒæ•¸è¨“ç·´æ¨¡å‹
            best_xgb = xgb.XGBRegressor(**best_params, random_state=42, n_jobs=-1)
            best_xgb.fit(self.X_train_scaled, self.y_train)
            
            # æ¸¬è©¦é›†è©•ä¼°
            y_pred = best_xgb.predict(self.X_test_scaled)
            test_r2 = r2_score(self.y_test, y_pred)
            test_rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))
            
            if self.debug:
                print(f"   ğŸ“ˆ æ¸¬è©¦é›†RÂ²: {test_r2:.3f}")
                print(f"   ğŸ“ˆ æ¸¬è©¦é›†RMSE: {test_rmse:.2f}")
            
            self.best_models['xgboost'] = {
                'model': best_xgb,
                'params': best_params,
                'cv_score': best_score,
                'test_r2': test_r2,
                'test_rmse': test_rmse
            }
            
            return True
            
        except Exception as e:
            if self.debug:
                print(f"âŒ XGBoostå„ªåŒ–å¤±æ•—: {e}")
            return False
    
    def optimize_randomforest_parameters(self):
        """RandomForestè¶…åƒæ•¸å„ªåŒ–"""
        if self.debug:
            print("\nğŸŒ² RandomForestè¶…åƒæ•¸å„ªåŒ–")
            print("-" * 40)
        
        try:
            from sklearn.ensemble import RandomForestRegressor
            
            # å®šç¾©åƒæ•¸æœç´¢ç©ºé–“
            param_grid = {
                'n_estimators': [50, 100, 200],
                'max_depth': [10, 15, 20, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4],
                'max_features': ['sqrt', 'log2', None]
            }
            
            if self.debug:
                print(f"ğŸ” åƒæ•¸æœç´¢ä¸­...")
            
            # RandomizedSearchCV
            rf_model = RandomForestRegressor(random_state=42, n_jobs=-1)
            
            random_search = RandomizedSearchCV(
                rf_model,
                param_grid,
                n_iter=15,  # éš¨æ©Ÿæœç´¢15çµ„åƒæ•¸
                cv=3,
                scoring='r2',
                n_jobs=-1,
                random_state=42
            )
            
            start_time = time.time()
            random_search.fit(self.X_train_scaled, self.y_train)
            search_time = time.time() - start_time
            
            # æœ€ä½³åƒæ•¸
            best_params = random_search.best_params_
            best_score = random_search.best_score_
            
            if self.debug:
                print(f"   â±ï¸ æœç´¢æ™‚é–“: {search_time:.1f} ç§’")
                print(f"   ğŸ† æœ€ä½³CVåˆ†æ•¸: {best_score:.3f}")
                print(f"   ğŸ“Š æœ€ä½³åƒæ•¸:")
                for param, value in best_params.items():
                    print(f"     {param}: {value}")
            
            # ä½¿ç”¨æœ€ä½³åƒæ•¸è¨“ç·´æ¨¡å‹
            best_rf = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)
            best_rf.fit(self.X_train_scaled, self.y_train)
            
            # æ¸¬è©¦é›†è©•ä¼°
            y_pred = best_rf.predict(self.X_test_scaled)
            test_r2 = r2_score(self.y_test, y_pred)
            test_rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))
            
            if self.debug:
                print(f"   ğŸ“ˆ æ¸¬è©¦é›†RÂ²: {test_r2:.3f}")
                print(f"   ğŸ“ˆ æ¸¬è©¦é›†RMSE: {test_rmse:.2f}")
            
            self.best_models['random_forest'] = {
                'model': best_rf,
                'params': best_params,
                'cv_score': best_score,
                'test_r2': test_r2,
                'test_rmse': test_rmse
            }
            
            return True
            
        except Exception as e:
            if self.debug:
                print(f"âŒ RandomForestå„ªåŒ–å¤±æ•—: {e}")
            return False
    
    def analyze_feature_importance(self):
        """ç‰¹å¾µé‡è¦æ€§åˆ†æ"""
        if self.debug:
            print("\nğŸ“Š ç‰¹å¾µé‡è¦æ€§åˆ†æ")
            print("-" * 40)
        
        try:
            feature_importance_data = {}
            
            # XGBoostç‰¹å¾µé‡è¦æ€§
            if 'xgboost' in self.best_models:
                xgb_importance = self.best_models['xgboost']['model'].feature_importances_
                feature_importance_data['xgboost'] = dict(zip(self.feature_names, xgb_importance))
            
            # RandomForestç‰¹å¾µé‡è¦æ€§
            if 'random_forest' in self.best_models:
                rf_importance = self.best_models['random_forest']['model'].feature_importances_
                feature_importance_data['random_forest'] = dict(zip(self.feature_names, rf_importance))
            
            # è¨ˆç®—å¹³å‡é‡è¦æ€§
            if len(feature_importance_data) > 0:
                avg_importance = {}
                for feature in self.feature_names:
                    importances = [data[feature] for data in feature_importance_data.values() if feature in data]
                    avg_importance[feature] = np.mean(importances) if importances else 0
                
                # æ’åºç‰¹å¾µé‡è¦æ€§
                sorted_features = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)
                
                if self.debug:
                    print(f"ğŸ† å‰10é‡è¦ç‰¹å¾µ:")
                    for i, (feature, importance) in enumerate(sorted_features[:10]):
                        print(f"   {i+1:2d}. {feature}: {importance:.3f}")
                
                # åˆ†æä½é‡è¦æ€§ç‰¹å¾µ
                low_importance_features = [f for f, imp in sorted_features if imp < 0.01]
                if self.debug and low_importance_features:
                    print(f"âš ï¸ ä½é‡è¦æ€§ç‰¹å¾µ (<0.01): {len(low_importance_features)} å€‹")
                    print(f"   å»ºè­°ç§»é™¤: {low_importance_features[:5]}...")
                
                self.feature_analysis = {
                    'feature_importance': avg_importance,
                    'sorted_features': sorted_features,
                    'low_importance': low_importance_features
                }
                
                return True
            else:
                if self.debug:
                    print("âŒ æ²’æœ‰å¯ç”¨çš„æ¨¡å‹é€²è¡Œç‰¹å¾µåˆ†æ")
                return False
                
        except Exception as e:
            if self.debug:
                print(f"âŒ ç‰¹å¾µé‡è¦æ€§åˆ†æå¤±æ•—: {e}")
            return False
    
    def optimize_ensemble_weights(self):
        """å„ªåŒ–ensembleæ¬Šé‡"""
        if self.debug:
            print("\nğŸ”— ensembleæ¬Šé‡å„ªåŒ–")
            print("-" * 40)
        
        try:
            if len(self.best_models) < 2:
                if self.debug:
                    print("âš ï¸ éœ€è¦è‡³å°‘2å€‹æ¨¡å‹é€²è¡Œensembleå„ªåŒ–")
                return False
            
            # ç²å–å„æ¨¡å‹é æ¸¬
            predictions = {}
            for model_name, model_info in self.best_models.items():
                pred = model_info['model'].predict(self.X_test_scaled)
                predictions[model_name] = pred
            
            # æ¸¬è©¦ä¸åŒæ¬Šé‡çµ„åˆ
            best_r2 = -np.inf
            best_weights = None
            
            weight_combinations = [
                {'xgboost': 0.7, 'random_forest': 0.3},
                {'xgboost': 0.8, 'random_forest': 0.2},
                {'xgboost': 0.6, 'random_forest': 0.4},
                {'xgboost': 0.5, 'random_forest': 0.5},
                {'xgboost': 0.9, 'random_forest': 0.1}
            ]
            
            if self.debug:
                print(f"ğŸ” æ¸¬è©¦ {len(weight_combinations)} ç¨®æ¬Šé‡çµ„åˆ...")
            
            for weights in weight_combinations:
                # è¨ˆç®—åŠ æ¬Šå¹³å‡é æ¸¬
                ensemble_pred = np.zeros_like(self.y_test)
                for model_name, weight in weights.items():
                    if model_name in predictions:
                        ensemble_pred += weight * predictions[model_name]
                
                # è¨ˆç®—RÂ²
                r2 = r2_score(self.y_test, ensemble_pred)
                
                if r2 > best_r2:
                    best_r2 = r2
                    best_weights = weights.copy()
                
                if self.debug:
                    weight_str = ', '.join([f"{k}:{v}" for k, v in weights.items()])
                    print(f"   {weight_str} â†’ RÂ²: {r2:.3f}")
            
            if best_weights:
                if self.debug:
                    print(f"ğŸ† æœ€ä½³æ¬Šé‡çµ„åˆ: RÂ² = {best_r2:.3f}")
                    for model, weight in best_weights.items():
                        print(f"   {model}: {weight}")
                
                self.best_ensemble = {
                    'weights': best_weights,
                    'r2': best_r2
                }
                
                return True
            else:
                return False
                
        except Exception as e:
            if self.debug:
                print(f"âŒ ensembleæ¬Šé‡å„ªåŒ–å¤±æ•—: {e}")
            return False
    
    def benchmark_prediction_speed(self, num_tests=1000):
        """é æ¸¬é€Ÿåº¦åŸºæº–æ¸¬è©¦"""
        if self.debug:
            print(f"\nâš¡ é æ¸¬é€Ÿåº¦åŸºæº–æ¸¬è©¦ ({num_tests} æ¬¡)")
            print("-" * 40)
        
        try:
            speed_results = {}
            
            # æº–å‚™æ¸¬è©¦æ•¸æ“š
            test_indices = np.random.choice(len(self.X_test_scaled), num_tests, replace=True)
            
            for model_name, model_info in self.best_models.items():
                model = model_info['model']
                
                # æ¸¬è©¦é æ¸¬é€Ÿåº¦
                times = []
                for idx in test_indices:
                    sample = self.X_test_scaled[idx:idx+1]
                    
                    start_time = time.time()
                    _ = model.predict(sample)
                    pred_time = time.time() - start_time
                    
                    times.append(pred_time * 1000)  # è½‰æ›ç‚ºæ¯«ç§’
                
                avg_time = np.mean(times)
                min_time = np.min(times)
                max_time = np.max(times)
                
                speed_results[model_name] = {
                    'avg_time_ms': avg_time,
                    'min_time_ms': min_time,
                    'max_time_ms': max_time
                }
                
                if self.debug:
                    print(f"   {model_name}:")
                    print(f"     å¹³å‡: {avg_time:.2f} ms")
                    print(f"     ç¯„åœ: {min_time:.2f} - {max_time:.2f} ms")
            
            self.speed_benchmark = speed_results
            return True
            
        except Exception as e:
            if self.debug:
                print(f"âŒ é€Ÿåº¦åŸºæº–æ¸¬è©¦å¤±æ•—: {e}")
            return False
    
    def save_optimized_models(self):
        """ä¿å­˜å„ªåŒ–å¾Œçš„æ¨¡å‹"""
        if self.debug:
            print("\nğŸ’¾ ä¿å­˜å„ªåŒ–å¾Œçš„æ¨¡å‹")
            print("-" * 40)
        
        try:
            import pickle
            import json
            
            models_folder = Path("models/optimized_models")
            models_folder.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹
            for model_name, model_info in self.best_models.items():
                if model_name == 'xgboost':
                    model_file = models_folder / f"optimized_{model_name}.json"
                    model_info['model'].save_model(str(model_file))
                else:
                    model_file = models_folder / f"optimized_{model_name}.pkl"
                    with open(model_file, 'wb') as f:
                        pickle.dump(model_info['model'], f)
                
                if self.debug:
                    print(f"   âœ… {model_name}: {model_file}")
            
            # ä¿å­˜scaler
            scaler_file = models_folder / "optimized_scaler.pkl"
            with open(scaler_file, 'wb') as f:
                pickle.dump(self.scaler, f)
            
            # ä¿å­˜ç‰¹å¾µåç¨±
            features_file = models_folder / "optimized_features.json"
            with open(features_file, 'w') as f:
                json.dump(self.feature_names, f)
            
            # ä¿å­˜å„ªåŒ–çµæœå ±å‘Š
            optimization_report = {
                'model_performance': {
                    name: {
                        'params': info['params'],
                        'cv_score': info['cv_score'],
                        'test_r2': info['test_r2'],
                        'test_rmse': info['test_rmse']
                    }
                    for name, info in self.best_models.items()
                },
                'ensemble_weights': getattr(self, 'best_ensemble', {}),
                'feature_analysis': getattr(self, 'feature_analysis', {}),
                'speed_benchmark': getattr(self, 'speed_benchmark', {})
            }
            
            report_file = models_folder / "optimization_report.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(optimization_report, f, ensure_ascii=False, indent=2, default=str)
            
            if self.debug:
                print(f"   âœ… å„ªåŒ–å ±å‘Š: {report_file}")
                print(f"   ğŸ“ æ¨¡å‹ç›®éŒ„: {models_folder}")
            
            return True
            
        except Exception as e:
            if self.debug:
                print(f"âŒ æ¨¡å‹ä¿å­˜å¤±æ•—: {e}")
            return False
    
    def generate_optimization_report(self):
        """ç”Ÿæˆå„ªåŒ–å ±å‘Š"""
        print(f"\n" + "="*60)
        print("ğŸ“‹ äº¤é€šé æ¸¬ç³»çµ±æ€§èƒ½å„ªåŒ–å ±å‘Š")
        print("="*60)
        
        if hasattr(self, 'X_train'):
            print(f"\nğŸ“Š å„ªåŒ–æ•¸æ“š:")
            print(f"   è¨“ç·´æ•¸æ“š: {len(self.X_train):,} ç­†")
            print(f"   æ¸¬è©¦æ•¸æ“š: {len(self.X_test):,} ç­†")
            print(f"   ç‰¹å¾µæ•¸é‡: {len(self.feature_names)}")
        
        print(f"\nğŸ† å„ªåŒ–å¾Œæ¨¡å‹æ€§èƒ½:")
        if self.best_models:
            for model_name, model_info in self.best_models.items():
                print(f"   {model_name}:")
                print(f"     CV RÂ²: {model_info['cv_score']:.3f}")
                print(f"     æ¸¬è©¦RÂ²: {model_info['test_r2']:.3f}")
                print(f"     æ¸¬è©¦RMSE: {model_info['test_rmse']:.2f} km/h")
        
        if hasattr(self, 'best_ensemble'):
            ensemble = self.best_ensemble
            print(f"\nğŸ”— Ensembleå„ªåŒ–:")
            print(f"   æœ€ä½³çµ„åˆRÂ²: {ensemble['r2']:.3f}")
            print(f"   æœ€ä½³æ¬Šé‡:")
            for model, weight in ensemble['weights'].items():
                print(f"     {model}: {weight}")
        
        if hasattr(self, 'speed_benchmark'):
            print(f"\nâš¡ é æ¸¬é€Ÿåº¦:")
            for model_name, speed_info in self.speed_benchmark.items():
                print(f"   {model_name}: {speed_info['avg_time_ms']:.1f} ms")
        
        if hasattr(self, 'feature_analysis'):
            analysis = self.feature_analysis
            print(f"\nğŸ“Š ç‰¹å¾µå„ªåŒ–:")
            print(f"   é‡è¦ç‰¹å¾µæ•¸: {len([f for f, imp in analysis['sorted_features'] if imp > 0.01])}")
            print(f"   ä½é‡è¦æ€§ç‰¹å¾µ: {len(analysis['low_importance'])}")
            print(f"   å¯ç§»é™¤ç‰¹å¾µ: {len(analysis['low_importance'])}")
        
        # æ€§èƒ½æ”¹å–„å»ºè­°
        print(f"\nğŸ’¡ æ€§èƒ½æ”¹å–„å»ºè­°:")
        
        best_r2 = max(info['test_r2'] for info in self.best_models.values()) if self.best_models else 0
        
        if best_r2 > 0.9:
            print(f"   ğŸ‰ æ¨¡å‹æ€§èƒ½å„ªç§€ (RÂ² = {best_r2:.3f})")
            print(f"   å»ºè­°: ç³»çµ±å·²é”åˆ°ç”Ÿç”¢ç´šæ¨™æº–")
        elif best_r2 > 0.8:
            print(f"   âœ… æ¨¡å‹æ€§èƒ½è‰¯å¥½ (RÂ² = {best_r2:.3f})")
            print(f"   å»ºè­°: å¯è€ƒæ…®å¢åŠ æ›´å¤šç‰¹å¾µå·¥ç¨‹")
        else:
            print(f"   âš ï¸ æ¨¡å‹æ€§èƒ½éœ€æ”¹å–„ (RÂ² = {best_r2:.3f})")
            print(f"   å»ºè­°: å¢åŠ è¨“ç·´æ•¸æ“šæˆ–èª¿æ•´ç‰¹å¾µ")
        
        if hasattr(self, 'speed_benchmark'):
            avg_speed = np.mean([info['avg_time_ms'] for info in self.speed_benchmark.values()])
            if avg_speed < 50:
                print(f"   âš¡ é æ¸¬é€Ÿåº¦å„ªç§€ ({avg_speed:.1f} ms)")
            elif avg_speed < 100:
                print(f"   âœ… é æ¸¬é€Ÿåº¦è‰¯å¥½ ({avg_speed:.1f} ms)")
            else:
                print(f"   âš ï¸ é æ¸¬é€Ÿåº¦éœ€å„ªåŒ– ({avg_speed:.1f} ms)")


def main():
    """ä¸»å„ªåŒ–ç¨‹åº"""
    print("ğŸ”§ äº¤é€šé æ¸¬ç³»çµ±æ€§èƒ½å„ªåŒ–")
    print("=" * 50)
    print("ğŸ¯ ç›®æ¨™: æå‡é æ¸¬æº–ç¢ºç‡åˆ°90%+ï¼Œé™ä½éŸ¿æ‡‰æ™‚é–“åˆ°50ms")
    print("=" * 50)
    
    optimizer = PerformanceOptimizer(debug=True)
    
    # åŸ·è¡Œå„ªåŒ–åºåˆ—
    optimization_steps = [
        ("è¼‰å…¥è¨“ç·´æ•¸æ“š", lambda: optimizer.load_training_data(sample_rate=0.8)),
        ("XGBooståƒæ•¸å„ªåŒ–", optimizer.optimize_xgboost_parameters),
        ("RandomForeståƒæ•¸å„ªåŒ–", optimizer.optimize_randomforest_parameters),
        ("ç‰¹å¾µé‡è¦æ€§åˆ†æ", optimizer.analyze_feature_importance),
        ("Ensembleæ¬Šé‡å„ªåŒ–", optimizer.optimize_ensemble_weights),
        ("é æ¸¬é€Ÿåº¦åŸºæº–æ¸¬è©¦", lambda: optimizer.benchmark_prediction_speed(500)),
        ("ä¿å­˜å„ªåŒ–æ¨¡å‹", optimizer.save_optimized_models)
    ]
    
    results = []
    total_start_time = time.time()
    
    for step_name, step_func in optimization_steps:
        if optimizer.debug:
            print(f"\nğŸ”„ åŸ·è¡Œ: {step_name}")
        
        try:
            success = step_func()
            results.append((step_name, success))
            
            if not success and step_name == "è¼‰å…¥è¨“ç·´æ•¸æ“š":
                print("âŒ æ•¸æ“šè¼‰å…¥å¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒå„ªåŒ–")
                break
                
        except Exception as e:
            print(f"âŒ {step_name} å¤±æ•—: {e}")
            results.append((step_name, False))
    
    total_duration = time.time() - total_start_time
    
    # ç”Ÿæˆå„ªåŒ–å ±å‘Š
    optimizer.generate_optimization_report()
    
    # å„ªåŒ–ç¸½çµ
    successful_steps = sum(1 for _, success in results if success)
    total_steps = len(results)
    
    print(f"\n" + "="*60)
    print(f"ğŸ“Š å„ªåŒ–ç¸½çµ")
    print("="*60)
    print(f"ç¸½å„ªåŒ–æ­¥é©Ÿ: {total_steps}")
    print(f"æˆåŠŸæ­¥é©Ÿ: {successful_steps}")
    print(f"æˆåŠŸç‡: {successful_steps/total_steps*100:.1f}%")
    print(f"ç¸½å„ªåŒ–æ™‚é–“: {total_duration:.1f} ç§’")
    
    print(f"\nè©³ç´°çµæœ:")
    for step_name, success in results:
        status = "âœ… å®Œæˆ" if success else "âŒ å¤±æ•—"
        print(f"   â€¢ {step_name}: {status}")
    
    if successful_steps >= total_steps * 0.8:
        print(f"\nğŸ‰ æ€§èƒ½å„ªåŒ–å®Œæˆï¼ç³»çµ±å·²é”åˆ°æœ€ä½³ç‹€æ…‹ï¼")
        print(f"\nğŸ“ å„ªåŒ–å¾Œæ¨¡å‹ä½ç½®: models/optimized_models/")
        print(f"ğŸ“‹ å„ªåŒ–å ±å‘Š: models/optimized_models/optimization_report.json")
        return True
    else:
        print(f"\nâš ï¸ å„ªåŒ–æœªå®Œå…¨æˆåŠŸï¼Œå»ºè­°æª¢æŸ¥:")
        print("   1. è¨“ç·´æ•¸æ“šè³ªé‡å’Œæ•¸é‡")
        print("   2. ç‰¹å¾µå·¥ç¨‹æ˜¯å¦å……åˆ†")
        print("   3. æ¨¡å‹åƒæ•¸æœç´¢ç¯„åœ")
        return False


if __name__ == "__main__":
    success = main()
    
    if success:
        print(f"\nğŸš€ ç³»çµ±æ€§èƒ½å„ªåŒ–å®Œæˆï¼")
        print(f"ğŸ’¡ ä¸‹ä¸€æ­¥: ä½¿ç”¨å„ªåŒ–å¾Œçš„æ¨¡å‹é€²è¡Œç”Ÿç”¢éƒ¨ç½²")
    else:
        print(f"\nğŸ”§ è«‹æ ¹æ“šå„ªåŒ–çµæœé€²è¡Œé€²ä¸€æ­¥èª¿æ•´")