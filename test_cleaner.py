# test_cleaner.py - æ—¥æœŸè³‡æ–™å¤¾çµ„ç¹”ç‰ˆ

"""
VDæ‰¹æ¬¡æ•¸æ“šæ¸…ç†å™¨æ¸¬è©¦ç¨‹å¼ - æ—¥æœŸè³‡æ–™å¤¾çµ„ç¹”ç‰ˆ
===============================================

æ–°å¢æ¸¬è©¦åŠŸèƒ½ï¼š
1. æ¸¬è©¦æŒ‰æ—¥æœŸçµ„ç¹”çš„æ¸…ç†ï¼šdata/cleaned/2025-06-27/
2. æ¸¬è©¦å¤šæ—¥æœŸè³‡æ–™å¤¾æ‰¹æ¬¡æ¸…ç†
3. æ¸¬è©¦æ—¥æœŸè³‡æ–™å¤¾æª¢æ¸¬åŠŸèƒ½
4. æ¸¬è©¦æŒ‡å®šæ—¥æœŸæ¸…ç†æ•¸æ“šè¼‰å…¥

- è‡ªå‹•æª¢æ¸¬ data/processed/YYYY-MM-DD/ ä¸­çš„æª”æ¡ˆ
- æ‰¹æ¬¡æ¸…ç†ä¸¦ä¿å­˜åˆ° data/cleaned/YYYY-MM-DD/
- ç”Ÿæˆå®Œæ•´çš„æ—¥æœŸçµ„ç¹”æ¸…ç†å ±å‘Š
"""

import sys
import os
import pandas as pd
import json
from datetime import datetime
from pathlib import Path

# æ·»åŠ srcç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

def test_date_folder_detection():
    """æ¸¬è©¦1: æ—¥æœŸè³‡æ–™å¤¾æª¢æ¸¬åŠŸèƒ½"""
    print("ğŸ§ª æ¸¬è©¦1: æ—¥æœŸè³‡æ–™å¤¾æª¢æ¸¬åŠŸèƒ½")
    print("-" * 70)
    
    try:
        from data_cleaner import VDBatchDataCleaner
        print("âœ… æˆåŠŸå°å…¥ VDBatchDataCleaner æ—¥æœŸçµ„ç¹”ç‰ˆ")
    except ImportError as e:
        print(f"âŒ å°å…¥å¤±æ•—: {e}")
        print("ğŸ’¡ è«‹ç¢ºèª data_cleaner.py åœ¨ src/ ç›®éŒ„ä¸­")
        return False
    
    # åˆå§‹åŒ–æ‰¹æ¬¡æ¸…ç†å™¨
    print("\n2ï¸âƒ£ åˆå§‹åŒ–æ—¥æœŸçµ„ç¹”æ‰¹æ¬¡æ¸…ç†å™¨...")
    try:
        cleaner = VDBatchDataCleaner()
        print("âœ… æ—¥æœŸçµ„ç¹”æ‰¹æ¬¡æ¸…ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        print(f"   ğŸ“ è¼¸å…¥åŸºç¤ç›®éŒ„: {cleaner.processed_base_folder}")
        print(f"   ğŸ“ è¼¸å‡ºåŸºç¤ç›®éŒ„: {cleaner.cleaned_base_folder}")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±æ•—: {e}")
        return False
    
    # æª¢æ¸¬å¯ç”¨æ—¥æœŸè³‡æ–™å¤¾
    print("\n3ï¸âƒ£ æª¢æ¸¬å¯ç”¨çš„æ—¥æœŸè³‡æ–™å¤¾...")
    available_date_folders = cleaner.detect_available_date_folders()
    
    if not available_date_folders:
        print("âŒ æ‰¾ä¸åˆ°å¯æ¸…ç†çš„æ—¥æœŸè³‡æ–™å¤¾ï¼")
        print("ğŸ’¡ è«‹å…ˆåŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š")
        print("   1. python test_loader.py  (ç”Ÿæˆæ—¥æœŸçµ„ç¹”çš„åˆ†é¡æª”æ¡ˆ)")
        print("   2. ç¢ºèª data/processed/YYYY-MM-DD/ ç›®éŒ„ä¸­æœ‰åˆ†é¡æª”æ¡ˆ")
        print("   3. é æœŸçµæ§‹:")
        print("      ğŸ“‚ data/processed/")
        print("         â”œâ”€â”€ 2025-06-27/")
        print("         â”‚   â”œâ”€â”€ vd_data_all.csv")
        print("         â”‚   â”œâ”€â”€ vd_data_peak.csv")
        print("         â”‚   â””â”€â”€ ...")
        print("         â””â”€â”€ 2025-06-26/")
        return False
    
    print(f"âœ… æ‰¾åˆ° {len(available_date_folders)} å€‹å¯æ¸…ç†çš„æ—¥æœŸè³‡æ–™å¤¾")
    
    # é¡¯ç¤ºå„æ—¥æœŸè³‡æ–™å¤¾è©³æƒ…
    total_files = 0
    total_size_mb = 0
    
    for date_str, date_info in sorted(available_date_folders.items()):
        file_count = date_info['file_count']
        available_files = date_info['available_files']
        
        print(f"\n   ğŸ“… {date_str}:")
        print(f"      æª”æ¡ˆæ•¸: {file_count}")
        
        date_size = 0
        for name, file_info in available_files.items():
            if file_info.get('status') == 'ready':
                size_mb = file_info['file_size_mb']
                records = file_info['estimated_records']
                description = file_info['description']
                
                print(f"         âœ… {description}: {size_mb:.1f}MB (~{records:,} è¨˜éŒ„)")
                date_size += size_mb
            else:
                print(f"         âŒ {file_info['description']}: {file_info.get('error', 'æª”æ¡ˆå•é¡Œ')}")
        
        print(f"      ç¸½å¤§å°: {date_size:.1f}MB")
        total_files += file_count
        total_size_mb += date_size
    
    print(f"\nğŸ“Š ç¸½è¨ˆ: {len(available_date_folders)} å€‹æ—¥æœŸ, {total_files} å€‹æª”æ¡ˆ, {total_size_mb:.1f}MB")
    
    return True


def test_single_date_folder_cleaning():
    """æ¸¬è©¦2: å–®ä¸€æ—¥æœŸè³‡æ–™å¤¾æ¸…ç†"""
    print("\nğŸ§ª æ¸¬è©¦2: å–®ä¸€æ—¥æœŸè³‡æ–™å¤¾æ¸…ç†")
    print("-" * 50)
    
    try:
        from data_cleaner import VDBatchDataCleaner
        
        cleaner = VDBatchDataCleaner()
        
        # æª¢æ¸¬å¯ç”¨æ—¥æœŸè³‡æ–™å¤¾
        available_date_folders = cleaner.detect_available_date_folders()
        
        if not available_date_folders:
            print("âš ï¸ æ²’æœ‰å¯æ¸…ç†çš„æ—¥æœŸè³‡æ–™å¤¾")
            return True
        
        # é¸æ“‡ç¬¬ä¸€å€‹æ—¥æœŸè³‡æ–™å¤¾é€²è¡Œæ¸¬è©¦
        test_date = list(available_date_folders.keys())[0]
        date_info = available_date_folders[test_date]
        
        print(f"ğŸ§¹ æ¸¬è©¦æ¸…ç†æ—¥æœŸè³‡æ–™å¤¾: {test_date}")
        print(f"   å¯æ¸…ç†æª”æ¡ˆæ•¸: {date_info['file_count']}")
        
        try:
            # åŸ·è¡Œå–®ä¸€æ—¥æœŸè³‡æ–™å¤¾æ¸…ç†
            date_result = cleaner.clean_single_date_folder(test_date, date_info, method='mark_nan')
            
            if date_result['successful_cleanings'] > 0:
                print(f"âœ… å–®ä¸€æ—¥æœŸè³‡æ–™å¤¾æ¸…ç†æˆåŠŸ")
                print(f"   æ¸…ç†æ—¥æœŸ: {date_result['date']}")
                print(f"   æˆåŠŸæ¸…ç†: {date_result['successful_cleanings']} å€‹æª”æ¡ˆ")
                print(f"   å¤±æ•—æ¸…ç†: {date_result['failed_cleanings']} å€‹æª”æ¡ˆ")
                print(f"   æˆåŠŸç‡: {date_result['success_rate']}")
                print(f"   è¼¸å‡ºè³‡æ–™å¤¾: {date_result['cleaned_folder']}")
                
                # æª¢æŸ¥è¼¸å‡ºæª”æ¡ˆ
                cleaned_folder = Path(date_result['cleaned_folder'])
                if cleaned_folder.exists():
                    output_files = list(cleaned_folder.glob("*.csv"))
                    print(f"   ç”ŸæˆCSVæª”æ¡ˆ: {len(output_files)} å€‹")
                    
                    for csv_file in output_files:
                        file_size = csv_file.stat().st_size / 1024 / 1024
                        print(f"      â€¢ {csv_file.name}: {file_size:.1f}MB")
                
                return True
            else:
                print(f"âŒ å–®ä¸€æ—¥æœŸè³‡æ–™å¤¾æ¸…ç†å¤±æ•—")
                return False
                
        except Exception as e:
            print(f"âŒ å–®ä¸€æ—¥æœŸè³‡æ–™å¤¾æ¸…ç†æ¸¬è©¦å¤±æ•—: {e}")
            return False
            
    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
        return False


def test_cleaning_methods():
    """æ¸¬è©¦3: ä¸åŒæ¸…ç†æ–¹æ³•æ¸¬è©¦"""
    print("\nğŸ§ª æ¸¬è©¦3: ä¸åŒæ¸…ç†æ–¹æ³•æ¸¬è©¦")
    print("-" * 50)
    
    try:
        from data_cleaner import VDBatchDataCleaner
        
        cleaner = VDBatchDataCleaner()
        
        # æª¢æ¸¬å¯ç”¨æ—¥æœŸè³‡æ–™å¤¾
        available_date_folders = cleaner.detect_available_date_folders()
        
        if not available_date_folders:
            print("âš ï¸ æ²’æœ‰å¯æ¸…ç†çš„æ—¥æœŸè³‡æ–™å¤¾")
            return True
        
        # é¸æ“‡æœ€å°çš„æ—¥æœŸè³‡æ–™å¤¾é€²è¡Œæ–¹æ³•æ¸¬è©¦
        smallest_date = min(available_date_folders.items(), 
                           key=lambda x: sum(f.get('file_size_mb', 0) for f in x[1]['available_files'].values()))
        
        test_date = smallest_date[0]
        date_info = smallest_date[1]
        
        print(f"ğŸ§ª ä½¿ç”¨ {test_date} é€²è¡Œæ¸…ç†æ–¹æ³•æ¸¬è©¦")
        
        # æ¸¬è©¦ä¸åŒæ¸…ç†æ–¹æ³•
        cleaning_methods = [
            ('mark_nan', 'æ¨™è¨˜ç‚ºNaN'),
            ('remove_rows', 'åˆªé™¤ç•°å¸¸è¡Œ')
        ]
        
        method_results = {}
        
        # ç²å–æ¸¬è©¦æª”æ¡ˆï¼ˆé¸æ“‡æœ€å°çš„æª”æ¡ˆï¼‰
        available_files = date_info['available_files']
        test_file_info = None
        test_file_name = None
        
        for name, file_info in available_files.items():
            if file_info.get('status') == 'ready':
                test_file_info = file_info
                test_file_name = name
                break
        
        if not test_file_info:
            print("âŒ æ²’æœ‰å¯ç”¨çš„æ¸¬è©¦æª”æ¡ˆ")
            return False
        
        print(f"   æ¸¬è©¦æª”æ¡ˆ: {test_file_info['description']}")
        
        for method, description in cleaning_methods:
            print(f"\n   ğŸ§ª æ¸¬è©¦ {method} - {description}")
            
            try:
                # è¼‰å…¥åŸå§‹æ•¸æ“šé€²è¡Œæ¸¬è©¦
                input_csv = test_file_info['input_path']
                df_test = pd.read_csv(input_csv)
                
                # å„ªåŒ–æ•¸æ“šé¡å‹
                df_test = cleaner._optimize_data_types(df_test)
                
                # è­˜åˆ¥ç„¡æ•ˆæ•¸æ“š
                invalid_stats = cleaner._identify_invalid_data_quick(df_test)
                
                # æ‡‰ç”¨æ¸…ç†æ–¹æ³•
                df_cleaned_test = cleaner._clean_invalid_values(df_test, method)
                
                # è¨ˆç®—æ•ˆæœ
                original_count = len(df_test)
                cleaned_count = len(df_cleaned_test)
                removed_records = original_count - cleaned_count
                
                method_results[method] = {
                    'description': description,
                    'original_records': original_count,
                    'cleaned_records': cleaned_count,
                    'records_removed': removed_records,
                    'removal_percentage': round(removed_records / original_count * 100, 2),
                    'invalid_values_found': invalid_stats['total_invalid'],
                    'invalid_percentage': invalid_stats['invalid_percentage']
                }
                
                print(f"      âœ… æ¸¬è©¦æˆåŠŸ")
                print(f"         åŸå§‹è¨˜éŒ„: {original_count:,}")
                print(f"         æ¸…ç†å¾Œè¨˜éŒ„: {cleaned_count:,}")
                print(f"         ç§»é™¤è¨˜éŒ„: {removed_records:,} ({method_results[method]['removal_percentage']:.1f}%)")
                print(f"         ç™¼ç¾ç•°å¸¸å€¼: {invalid_stats['total_invalid']:,} ({invalid_stats['invalid_percentage']:.2f}%)")
                
            except Exception as e:
                print(f"      âŒ æ¸¬è©¦å¤±æ•—: {e}")
                method_results[method] = {
                    'description': description,
                    'success': False,
                    'error': str(e)
                }
        
        # é¡¯ç¤ºæ–¹æ³•æ¯”è¼ƒ
        print(f"\nğŸ“Š æ¸…ç†æ–¹æ³•æ¯”è¼ƒ:")
        for method, result in method_results.items():
            if result.get('success', True):
                print(f"   {method}: ä¿ç•™ {result['cleaned_records']:,}/{result['original_records']:,} è¨˜éŒ„ ({100-result['removal_percentage']:.1f}%)")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¸…ç†æ–¹æ³•æ¸¬è©¦å¤±æ•—: {e}")
        return False


def test_batch_date_cleaning():
    """æ¸¬è©¦4: æ‰¹æ¬¡æ—¥æœŸæ¸…ç†"""
    print("\nğŸ§ª æ¸¬è©¦4: æ‰¹æ¬¡æ—¥æœŸæ¸…ç†")
    print("-" * 50)
    
    try:
        from data_cleaner import VDBatchDataCleaner
        
        cleaner = VDBatchDataCleaner()
        
        # æª¢æ¸¬å¯ç”¨æ—¥æœŸè³‡æ–™å¤¾
        available_date_folders = cleaner.detect_available_date_folders()
        
        if not available_date_folders:
            print("âš ï¸ æ²’æœ‰å¯æ¸…ç†çš„æ—¥æœŸè³‡æ–™å¤¾")
            return True
        
        print(f"ğŸš€ é–‹å§‹æ‰¹æ¬¡æ¸…ç† {len(available_date_folders)} å€‹æ—¥æœŸè³‡æ–™å¤¾...")
        
        # é¸æ“‡æ¨è–¦çš„æ¸…ç†æ–¹æ³•
        recommended_method = 'mark_nan'
        print(f"   ä½¿ç”¨æ¨è–¦æ–¹æ³•: {recommended_method}")
        
        try:
            start_time = datetime.now()
            
            # åŸ·è¡Œæ‰¹æ¬¡æ¸…ç†
            batch_report = cleaner.clean_all_date_folders(method=recommended_method)
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            if batch_report['æ¸…ç†çµ±è¨ˆ']['æˆåŠŸæ¸…ç†æ—¥æœŸæ•¸'] > 0:
                print(f"âœ… æ‰¹æ¬¡æ¸…ç†æˆåŠŸå®Œæˆ")
                
                stats = batch_report['æ¸…ç†çµ±è¨ˆ']
                print(f"   â±ï¸ è€—æ™‚: {duration:.1f} ç§’")
                print(f"   ğŸ“… æˆåŠŸæ—¥æœŸ: {stats['æˆåŠŸæ¸…ç†æ—¥æœŸæ•¸']}/{stats['ç¸½æ—¥æœŸè³‡æ–™å¤¾æ•¸']}")
                print(f"   ğŸ“„ æˆåŠŸæª”æ¡ˆ: {stats['ç¸½æˆåŠŸæª”æ¡ˆæ•¸']}")
                print(f"   âŒ å¤±æ•—æª”æ¡ˆ: {stats['ç¸½å¤±æ•—æª”æ¡ˆæ•¸']}")
                print(f"   ğŸ“ˆ æ—¥æœŸæˆåŠŸç‡: {stats['æ—¥æœŸæˆåŠŸç‡']}")
                
                # é¡¯ç¤ºå„æ—¥æœŸæ¸…ç†çµæœ
                print(f"\nğŸ“… å„æ—¥æœŸæ¸…ç†çµæœ:")
                for date_result in batch_report['å„æ—¥æœŸæ¸…ç†çµæœ']:
                    if 'successful_cleanings' in date_result:
                        date_str = date_result['date']
                        success_count = date_result['successful_cleanings']
                        total_count = date_result['total_files']
                        print(f"      {date_str}: {success_count}/{total_count} æª”æ¡ˆæ¸…ç†æˆåŠŸ")
                
                return True
            else:
                print(f"âŒ æ‰¹æ¬¡æ¸…ç†å¤±æ•—")
                return False
                
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡æ¸…ç†éç¨‹å¤±æ•—: {e}")
            return False
            
    except Exception as e:
        print(f"âŒ æ‰¹æ¬¡æ¸…ç†æ¸¬è©¦å¤±æ•—: {e}")
        return False


def test_cleaned_data_verification():
    """æ¸¬è©¦5: æ¸…ç†æ•¸æ“šé©—è­‰"""
    print("\nğŸ§ª æ¸¬è©¦5: æ¸…ç†æ•¸æ“šé©—è­‰")
    print("-" * 50)
    
    try:
        from data_cleaner import VDBatchDataCleaner
        
        cleaner = VDBatchDataCleaner()
        
        # ç²å–æ¸…ç†å¾Œæª”æ¡ˆæ‘˜è¦
        print("ğŸ“Š ç²å–æ¸…ç†å¾Œæª”æ¡ˆæ‘˜è¦...")
        summary = cleaner.get_cleaned_files_summary()
        
        if summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['æ¸…ç†æ—¥æœŸæ•¸'] == 0:
            print("âš ï¸ æ²’æœ‰æ‰¾åˆ°æ¸…ç†å¾Œæª”æ¡ˆ")
            return True
        
        stats = summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']
        print(f"âœ… æ¸…ç†å¾Œçµ±è¨ˆ:")
        print(f"   æ¸…ç†æ—¥æœŸæ•¸: {stats['æ¸…ç†æ—¥æœŸæ•¸']}")
        print(f"   ç¸½æª”æ¡ˆæ•¸: {stats['å­˜åœ¨æª”æ¡ˆæ•¸']}")
        print(f"   ç¸½è¨˜éŒ„æ•¸: {stats['ç¸½è¨˜éŒ„æ•¸']:,}")
        print(f"   ç¸½æª”æ¡ˆå¤§å°: {stats['æª”æ¡ˆå¤§å°_MB']:.1f} MB")
        
        # æª¢æŸ¥å„æ—¥æœŸçš„æ¸…ç†çµæœ
        print(f"\nğŸ“… å„æ—¥æœŸæ¸…ç†çµæœ:")
        for date_str, date_details in summary['å„æ—¥æœŸè©³æƒ…'].items():
            print(f"   {date_str}:")
            print(f"      æª”æ¡ˆæ•¸: {date_details['æª”æ¡ˆæ•¸']}")
            print(f"      è¨˜éŒ„æ•¸: {date_details['ç¸½è¨˜éŒ„æ•¸']:,}")
            print(f"      æª”æ¡ˆå¤§å°: {date_details['ç¸½æª”æ¡ˆå¤§å°_MB']:.1f}MB")
        
        # é©—è­‰æª”æ¡ˆå®Œæ•´æ€§
        print(f"\nğŸ” é©—è­‰æª”æ¡ˆå®Œæ•´æ€§...")
        
        available_cleaned_dates = cleaner.list_available_cleaned_dates()
        verification_passed = 0
        total_verifications = 0
        
        for date_str in available_cleaned_dates:
            print(f"   ğŸ“… é©—è­‰ {date_str}:")
            
            # æª¢æŸ¥è©²æ—¥æœŸçš„æ¸…ç†æª”æ¡ˆ
            cleaned_date_folder = cleaner.cleaned_base_folder / date_str
            
            for name, mapping in cleaner.file_mappings.items():
                output_csv = cleaned_date_folder / mapping['output_csv']
                description = mapping['description']
                total_verifications += 1
                
                if output_csv.exists():
                    try:
                        df_verify = pd.read_csv(output_csv, nrows=10)
                        print(f"      âœ… {description}: å¯æ­£å¸¸è®€å–")
                        verification_passed += 1
                    except Exception as e:
                        print(f"      âŒ {description}: è®€å–å¤±æ•— - {e}")
                else:
                    print(f"      âš ï¸ {description}: æª”æ¡ˆä¸å­˜åœ¨")
        
        print(f"\nğŸ“ˆ æª”æ¡ˆå®Œæ•´æ€§: {verification_passed}/{total_verifications} é€šéé©—è­‰ ({verification_passed/total_verifications*100:.1f}%)")
        
        return verification_passed >= total_verifications * 0.8  # è‡³å°‘80%é€šé
        
    except Exception as e:
        print(f"âŒ æ¸…ç†æ•¸æ“šé©—è­‰å¤±æ•—: {e}")
        return False


def test_date_specific_loading():
    """æ¸¬è©¦6: æŒ‡å®šæ—¥æœŸæ¸…ç†æ•¸æ“šè¼‰å…¥"""
    print("\nğŸ§ª æ¸¬è©¦6: æŒ‡å®šæ—¥æœŸæ¸…ç†æ•¸æ“šè¼‰å…¥")
    print("-" * 50)
    
    try:
        from data_cleaner import VDBatchDataCleaner
        
        cleaner = VDBatchDataCleaner()
        
        # ç²å–å¯ç”¨çš„æ¸…ç†æ—¥æœŸ
        available_cleaned_dates = cleaner.list_available_cleaned_dates()
        
        if not available_cleaned_dates:
            print("âš ï¸ æ²’æœ‰å¯ç”¨çš„æ¸…ç†æ—¥æœŸæ•¸æ“š")
            return True
        
        print(f"ğŸ“… å¯ç”¨æ¸…ç†æ—¥æœŸ: {available_cleaned_dates}")
        
        # æ¸¬è©¦è¼‰å…¥ç‰¹å®šæ—¥æœŸçš„æ¸…ç†æ•¸æ“š
        test_date = available_cleaned_dates[0]
        print(f"\nğŸ¯ æ¸¬è©¦è¼‰å…¥ {test_date} æ¸…ç†æ•¸æ“š...")
        
        try:
            start_time = datetime.now()
            cleaned_data = cleaner.load_cleaned_data_by_date(test_date)
            load_time = (datetime.now() - start_time).total_seconds()
            
            print(f"   â±ï¸ è¼‰å…¥æ™‚é–“: {load_time:.3f} ç§’")
            
            # æª¢æŸ¥è¼‰å…¥çµæœ
            file_descriptions = {
                'all': 'å…¨éƒ¨VDè³‡æ–™',
                'peak': 'å°–å³°æ™‚æ®µæ•¸æ“š',
                'offpeak': 'é›¢å³°æ™‚æ®µæ•¸æ“š',
                'target_route': 'ç›®æ¨™è·¯æ®µæ•¸æ“š',
                'target_route_peak': 'ç›®æ¨™è·¯æ®µå°–å³°',
                'target_route_offpeak': 'ç›®æ¨™è·¯æ®µé›¢å³°'
            }
            
            total_records = 0
            loaded_files = 0
            
            for name, description in file_descriptions.items():
                df = cleaned_data.get(name, pd.DataFrame())
                if not df.empty:
                    print(f"   âœ… {description}: {len(df):,} ç­†è¨˜éŒ„")
                    total_records += len(df)
                    loaded_files += 1
                else:
                    print(f"   âš ï¸ {description}: ç„¡æ•¸æ“š")
            
            print(f"\n   ğŸ“Š {test_date} è¼‰å…¥çµ±è¨ˆ:")
            print(f"      æˆåŠŸè¼‰å…¥æª”æ¡ˆ: {loaded_files}/{len(file_descriptions)}")
            print(f"      ç¸½è¨˜éŒ„æ•¸: {total_records:,}")
            print(f"      è¼‰å…¥é€Ÿåº¦: {total_records/load_time:,.0f} è¨˜éŒ„/ç§’")
            
            # é©—è­‰æ•¸æ“šå“è³ª
            if 'all' in cleaned_data and not cleaned_data['all'].empty:
                df_all = cleaned_data['all']
                
                print(f"\n   ğŸ” æ•¸æ“šå“è³ªæª¢æŸ¥:")
                
                # æª¢æŸ¥é—œéµæ¬„ä½
                key_columns = ['speed', 'occupancy', 'volume_total']
                for col in key_columns:
                    if col in df_all.columns:
                        valid_count = df_all[col].notna().sum()
                        completeness = valid_count / len(df_all) * 100
                        print(f"      {col}: {completeness:.1f}% å®Œæ•´åº¦")
            
            return True
            
        except Exception as e:
            print(f"   âŒ æŒ‡å®šæ—¥æœŸè¼‰å…¥å¤±æ•—: {e}")
            return False
            
    except Exception as e:
        print(f"âŒ æŒ‡å®šæ—¥æœŸè¼‰å…¥æ¸¬è©¦å¤±æ•—: {e}")
        return False


def test_convenience_functions():
    """æ¸¬è©¦7: ä¾¿åˆ©å‡½æ•¸"""
    print("\nğŸ§ª æ¸¬è©¦7: ä¾¿åˆ©å‡½æ•¸")
    print("-" * 50)
    
    try:
        from data_cleaner import (
            clean_all_vd_data_by_date,
            get_cleaned_data_summary_by_date,
            load_cleaned_data_by_date
        )
        
        print("ğŸ”§ æ¸¬è©¦ä¾¿åˆ©å‡½æ•¸å°å…¥...")
        print("   âœ… æˆåŠŸå°å…¥æ‰€æœ‰æ—¥æœŸçµ„ç¹”ä¾¿åˆ©å‡½æ•¸")
        
        # æ¸¬è©¦æ‘˜è¦ä¾¿åˆ©å‡½æ•¸
        print("\nğŸ“Š æ¸¬è©¦æ‘˜è¦ä¾¿åˆ©å‡½æ•¸...")
        summary = get_cleaned_data_summary_by_date()
        
        if summary and summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['æ¸…ç†æ—¥æœŸæ•¸'] > 0:
            dates_count = summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['æ¸…ç†æ—¥æœŸæ•¸']
            files_count = summary['æ¸…ç†æª”æ¡ˆçµ±è¨ˆ']['å­˜åœ¨æª”æ¡ˆæ•¸']
            print(f"   âœ… get_cleaned_data_summary_by_date(): {dates_count} å€‹æ—¥æœŸ, {files_count} å€‹æª”æ¡ˆ")
        else:
            print(f"   âš ï¸ get_cleaned_data_summary_by_date(): ç„¡çµæœ")
        
        # æ¸¬è©¦è¼‰å…¥ä¾¿åˆ©å‡½æ•¸
        print("\nğŸ“‚ æ¸¬è©¦è¼‰å…¥ä¾¿åˆ©å‡½æ•¸...")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æ¸…ç†æ—¥æœŸ
        cleaned_base = Path("data/cleaned")
        available_dates = []
        
        if cleaned_base.exists():
            available_dates = [d.name for d in cleaned_base.iterdir() 
                             if d.is_dir() and d.name.count('-') == 2]
        
        if available_dates:
            test_date = available_dates[0]
            try:
                date_data = load_cleaned_data_by_date(target_date=test_date)
                
                if date_data:
                    total_records = sum(len(df) for df in date_data.values() if not df.empty)
                    loaded_count = len([df for df in date_data.values() if not df.empty])
                    print(f"   âœ… load_cleaned_data_by_date({test_date}): {loaded_count} æª”æ¡ˆ, {total_records:,} ç­†è¨˜éŒ„")
                else:
                    print(f"   âš ï¸ load_cleaned_data_by_date({test_date}): ç„¡çµæœ")
            except Exception as e:
                print(f"   âŒ load_cleaned_data_by_date({test_date}): è¼‰å…¥å¤±æ•— - {e}")
        else:
            print(f"   âš ï¸ æ²’æœ‰å¯ç”¨çš„æ¸…ç†æ—¥æœŸè³‡æ–™å¤¾æ¸¬è©¦")
        
        return True
        
    except Exception as e:
        print(f"   âŒ ä¾¿åˆ©å‡½æ•¸æ¸¬è©¦å¤±æ•—: {e}")
        return False


def test_output_structure_verification():
    """æ¸¬è©¦8: è¼¸å‡ºçµæ§‹é©—è­‰"""
    print("\nğŸ§ª æ¸¬è©¦8: è¼¸å‡ºçµæ§‹é©—è­‰")
    print("-" * 50)
    
    try:
        from data_cleaner import VDBatchDataCleaner
        
        cleaner = VDBatchDataCleaner()
        
        # æª¢æŸ¥æ¸…ç†åŸºç¤è³‡æ–™å¤¾
        if not cleaner.cleaned_base_folder.exists():
            print("âš ï¸ æ¸…ç†åŸºç¤è³‡æ–™å¤¾ä¸å­˜åœ¨")
            return True
        
        print(f"ğŸ“‚ æª¢æŸ¥è¼¸å‡ºçµæ§‹: {cleaner.cleaned_base_folder}")
        
        # æª¢æŸ¥æ—¥æœŸè³‡æ–™å¤¾çµæ§‹
        date_folders = [d for d in cleaner.cleaned_base_folder.iterdir() 
                       if d.is_dir() and d.name.count('-') == 2]
        
        if not date_folders:
            print("âš ï¸ æ²’æœ‰æ‰¾åˆ°æ—¥æœŸè³‡æ–™å¤¾")
            return True
        
        print(f"ğŸ“… æ‰¾åˆ° {len(date_folders)} å€‹æ—¥æœŸè³‡æ–™å¤¾")
        
        # æª¢æŸ¥æ¯å€‹æ—¥æœŸè³‡æ–™å¤¾çš„çµæ§‹
        expected_files = {
            'vd_data_all_cleaned.csv': 'å…¨éƒ¨VDè³‡æ–™',
            'vd_data_peak_cleaned.csv': 'å°–å³°æ™‚æ®µæ•¸æ“š',
            'vd_data_offpeak_cleaned.csv': 'é›¢å³°æ™‚æ®µæ•¸æ“š',
            'target_route_data_cleaned.csv': 'ç›®æ¨™è·¯æ®µæ•¸æ“š',
            'target_route_peak_cleaned.csv': 'ç›®æ¨™è·¯æ®µå°–å³°',
            'target_route_offpeak_cleaned.csv': 'ç›®æ¨™è·¯æ®µé›¢å³°'
        }
        
        total_structure_score = 0
        max_structure_score = len(date_folders) * len(expected_files)
        
        for date_folder in sorted(date_folders):
            date_str = date_folder.name
            print(f"\n   ğŸ“… æª¢æŸ¥ {date_str}:")
            
            date_score = 0
            date_total_size = 0
            
            for filename, description in expected_files.items():
                csv_file = date_folder / filename
                json_file = date_folder / filename.replace('.csv', '_summary.json')
                report_file = date_folder / "date_cleaning_report.json"
                
                csv_exists = csv_file.exists()
                json_exists = json_file.exists()
                
                csv_status = "âœ…" if csv_exists else "âŒ"
                json_status = "âœ…" if json_exists else "âŒ"
                
                print(f"      {description}:")
                print(f"        CSV {csv_status} {filename}")
                print(f"        JSON {json_status} {json_file.name}")
                
                if csv_exists:
                    date_score += 1
                    total_structure_score += 1
                    
                    # æª¢æŸ¥æª”æ¡ˆå¤§å°
                    file_size = csv_file.stat().st_size / 1024 / 1024
                    date_total_size += file_size
                    print(f"        ğŸ“Š å¤§å°: {file_size:.1f}MB")
                    
                    # å¿«é€Ÿæª¢æŸ¥æª”æ¡ˆå…§å®¹
                    try:
                        df = pd.read_csv(csv_file, nrows=5)
                        print(f"        ğŸ“„ æ¬„ä½æ•¸: {len(df.columns)}")
                        
                        # æª¢æŸ¥å¿…è¦æ¬„ä½
                        required_columns = ['date', 'update_time', 'vd_id', 'speed', 'occupancy']
                        missing_columns = [col for col in required_columns if col not in df.columns]
                        
                        if missing_columns:
                            print(f"        âš ï¸ ç¼ºå°‘æ¬„ä½: {missing_columns}")
                        else:
                            print(f"        âœ… æ ¸å¿ƒæ¬„ä½å®Œæ•´")
                            
                    except Exception as e:
                        print(f"        âŒ è®€å–éŒ¯èª¤: {e}")
            
            # æª¢æŸ¥æ—¥æœŸæ¸…ç†å ±å‘Š
            if report_file.exists():
                print(f"      âœ… æ—¥æœŸæ¸…ç†å ±å‘Š: {report_file.name}")
            else:
                print(f"      âš ï¸ ç¼ºå°‘æ—¥æœŸæ¸…ç†å ±å‘Š")
            
            print(f"      ğŸ“ˆ {date_str} å®Œæ•´æ€§: {date_score}/{len(expected_files)} ({date_score/len(expected_files)*100:.1f}%)")
            print(f"      ğŸ’¾ {date_str} ç¸½å¤§å°: {date_total_size:.1f}MB")
        
        # æª¢æŸ¥æ‰¹æ¬¡æ¸…ç†å ±å‘Š
        batch_report_file = cleaner.cleaned_base_folder / "batch_date_cleaning_report.json"
        if batch_report_file.exists():
            print(f"\nâœ… æ‰¹æ¬¡æ¸…ç†å ±å‘Š: {batch_report_file.name}")
            
            # è®€å–å ±å‘Šå…§å®¹
            try:
                with open(batch_report_file, 'r', encoding='utf-8') as f:
                    batch_report = json.load(f)
                
                stats = batch_report.get('æ¸…ç†çµ±è¨ˆ', {})
                print(f"   ğŸ“Š å ±å‘Šçµ±è¨ˆ:")
                print(f"      æˆåŠŸæ¸…ç†æ—¥æœŸæ•¸: {stats.get('æˆåŠŸæ¸…ç†æ—¥æœŸæ•¸', 0)}")
                print(f"      ç¸½æˆåŠŸæª”æ¡ˆæ•¸: {stats.get('ç¸½æˆåŠŸæª”æ¡ˆæ•¸', 0)}")
                print(f"      æ—¥æœŸæˆåŠŸç‡: {stats.get('æ—¥æœŸæˆåŠŸç‡', '0%')}")
                
            except Exception as e:
                print(f"   âŒ è®€å–æ‰¹æ¬¡å ±å‘Šå¤±æ•—: {e}")
        else:
            print(f"\nâš ï¸ ç¼ºå°‘æ‰¹æ¬¡æ¸…ç†å ±å‘Š")
        
        print(f"\nğŸ“ˆ ç¸½é«”çµæ§‹å®Œæ•´æ€§: {total_structure_score}/{max_structure_score} ({total_structure_score/max_structure_score*100:.1f}%)")
        
        # é¡¯ç¤ºé æœŸçš„å®Œæ•´çµæ§‹
        print(f"\nğŸ“ é æœŸçš„å®Œæ•´è¼¸å‡ºçµæ§‹:")
        print(f"   ğŸ“‚ data/cleaned/")
        for date_folder in sorted(date_folders):
            print(f"      â”œâ”€â”€ {date_folder.name}/")
            print(f"      â”‚   â”œâ”€â”€ vd_data_all_cleaned.csv + _summary.json")
            print(f"      â”‚   â”œâ”€â”€ vd_data_peak_cleaned.csv + _summary.json")
            print(f"      â”‚   â”œâ”€â”€ vd_data_offpeak_cleaned.csv + _summary.json")
            print(f"      â”‚   â”œâ”€â”€ target_route_*_cleaned.csv + _summary.json")
            print(f"      â”‚   â””â”€â”€ date_cleaning_report.json")
        print(f"      â””â”€â”€ batch_date_cleaning_report.json")
        
        return total_structure_score >= max_structure_score * 0.8  # è‡³å°‘80%æª”æ¡ˆå­˜åœ¨
        
    except Exception as e:
        print(f"âŒ è¼¸å‡ºçµæ§‹é©—è­‰å¤±æ•—: {e}")
        return False


def generate_test_summary():
    """ç”Ÿæˆæ¸¬è©¦æ‘˜è¦å ±å‘Š"""
    
    print("\nğŸ“‹ ç”Ÿæˆæ¸¬è©¦æ‘˜è¦å ±å‘Š")
    print("=" * 50)
    
    try:
        # æª¢æŸ¥ç”Ÿæˆçš„å ±å‘Šæª”æ¡ˆ
        report_files = [
            "data/cleaned/batch_date_cleaning_report.json",
        ]
        
        existing_reports = []
        for report_file in report_files:
            if os.path.exists(report_file):
                file_size = os.path.getsize(report_file) / 1024  # KB
                existing_reports.append({
                    'file': report_file,
                    'size_kb': round(file_size, 1)
                })
        
        print(f"ğŸ“ å¯ç”¨æ¸…ç†å ±å‘Š: {len(existing_reports)} å€‹")
        for report in existing_reports:
            print(f"   â€¢ {report['file']} ({report['size_kb']} KB)")
        
        # æª¢æŸ¥æ¸…ç†æ•¸æ“šæª”æ¡ˆ
        cleaned_base_folder = Path("data/cleaned")
        if cleaned_base_folder.exists():
            date_folders = [d for d in cleaned_base_folder.iterdir() 
                           if d.is_dir() and d.name.count('-') == 2]
            
            total_files = 0
            total_size_mb = 0
            
            for date_folder in date_folders:
                csv_files = list(date_folder.glob("*.csv"))
                folder_size = sum(f.stat().st_size for f in csv_files) / 1024 / 1024
                total_files += len(csv_files)
                total_size_mb += folder_size
            
            print(f"\nğŸ“Š æ¸…ç†æ•¸æ“šçµ±è¨ˆ:")
            print(f"   æ—¥æœŸæ•¸: {len(date_folders)}")
            print(f"   æª”æ¡ˆæ•¸: {total_files}")
            print(f"   ç¸½å¤§å°: {total_size_mb:.1f} MB")
            
            print(f"\nğŸ“… å„æ—¥æœŸæ¸…ç†çµæœ:")
            for date_folder in sorted(date_folders):
                csv_files = list(date_folder.glob("*.csv"))
                folder_size = sum(f.stat().st_size for f in csv_files) / 1024 / 1024
                print(f"   {date_folder.name}: {len(csv_files)} æª”æ¡ˆ ({folder_size:.1f}MB)")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ‘˜è¦å ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
        return False


def demonstrate_date_organized_usage():
    """ç¤ºç¯„æ—¥æœŸçµ„ç¹”ä½¿ç”¨æ–¹æ³•"""
    print("\nğŸ’¡ æ—¥æœŸçµ„ç¹”æ¸…ç†ä½¿ç”¨æ–¹æ³•ç¤ºç¯„")
    print("=" * 60)
    
    print("ğŸ”§ åŸºæœ¬æ—¥æœŸçµ„ç¹”æ¸…ç†:")
    print("```python")
    print("from src.data_cleaner import VDBatchDataCleaner")
    print("")
    print("# åˆå§‹åŒ–æ—¥æœŸçµ„ç¹”æ‰¹æ¬¡æ¸…ç†å™¨")
    print("cleaner = VDBatchDataCleaner()")
    print("")
    print("# æª¢æ¸¬å¯ç”¨æ—¥æœŸè³‡æ–™å¤¾")
    print("available_dates = cleaner.detect_available_date_folders()")
    print("")
    print("# æ‰¹æ¬¡æ¸…ç†æ‰€æœ‰æ—¥æœŸè³‡æ–™å¤¾")
    print("report = cleaner.clean_all_date_folders()")
    print("")
    print("# æª¢æŸ¥æ¸…ç†çµæœ")
    print("summary = cleaner.get_cleaned_files_summary()")
    print("```")
    
    print("\nâš¡ ä¸€éµæ—¥æœŸçµ„ç¹”æ¸…ç†:")
    print("```python")
    print("from src.data_cleaner import clean_all_vd_data_by_date")
    print("")
    print("# ä¸€éµæ¸…ç†æ‰€æœ‰æ—¥æœŸè³‡æ–™å¤¾")
    print("report = clean_all_vd_data_by_date()")
    print("```")
    
    print("\nğŸ“… æŒ‡å®šæ—¥æœŸè¼‰å…¥:")
    print("```python")
    print("from src.data_cleaner import load_cleaned_data_by_date")
    print("")
    print("# è¼‰å…¥ç‰¹å®šæ—¥æœŸçš„æ¸…ç†æ•¸æ“š")
    print("date_data = load_cleaned_data_by_date(target_date='2025-06-27')")
    print("```")
    
    print("\nğŸ“ æ¸…ç†å¾Œæª”æ¡ˆçµæ§‹:")
    print("   ğŸ“‚ data/cleaned/")
    print("      â”œâ”€â”€ 2025-06-27/")
    print("      â”‚   â”œâ”€â”€ vd_data_all_cleaned.csv + .json")
    print("      â”‚   â”œâ”€â”€ vd_data_peak_cleaned.csv + .json")
    print("      â”‚   â”œâ”€â”€ vd_data_offpeak_cleaned.csv + .json")
    print("      â”‚   â”œâ”€â”€ target_route_data_cleaned.csv + .json")
    print("      â”‚   â”œâ”€â”€ target_route_peak_cleaned.csv + .json")
    print("      â”‚   â”œâ”€â”€ target_route_offpeak_cleaned.csv + .json")
    print("      â”‚   â””â”€â”€ date_cleaning_report.json")
    print("      â”œâ”€â”€ 2025-06-26/")
    print("      â”‚   â””â”€â”€ ... (åŒæ¨£çµæ§‹)")
    print("      â””â”€â”€ batch_date_cleaning_report.json")
    
    print("\nğŸ¯ AIè¨“ç·´æ¨è–¦æª”æ¡ˆï¼ˆæŒ‰æ—¥æœŸï¼‰:")
    print("   ğŸš€ ä¸»è¦è¨“ç·´æ•¸æ“š:")
    print("      â€¢ data/cleaned/2025-06-27/target_route_peak_cleaned.csv")
    print("      â€¢ data/cleaned/2025-06-27/target_route_offpeak_cleaned.csv")
    print("      â€¢ data/cleaned/2025-06-26/target_route_peak_cleaned.csv")
    print("      â€¢ data/cleaned/2025-06-26/target_route_offpeak_cleaned.csv")
    print("   ğŸ“Š æ™‚é–“åºåˆ—åˆ†æ:")
    print("      â€¢ è·¨æ—¥æœŸæ¯”è¼ƒåˆ†æ")
    print("      â€¢ æ™‚é–“è¶¨å‹¢é æ¸¬")
    print("      â€¢ é€±æœŸæ€§æ¨¡å¼è­˜åˆ¥")


def main():
    """ä¸»æ¸¬è©¦ç¨‹åº"""
    print("ğŸ§ª VDæ‰¹æ¬¡æ•¸æ“šæ¸…ç†å™¨æ—¥æœŸçµ„ç¹”ç‰ˆå®Œæ•´æ¸¬è©¦")
    print("=" * 80)
    print("é€™å°‡æ¸¬è©¦æ—¥æœŸçµ„ç¹”æ¸…ç†çš„æ‰€æœ‰åŠŸèƒ½ï¼ŒåŒ…æ‹¬:")
    print("â€¢ è‡ªå‹•æª¢æ¸¬ data/processed/YYYY-MM-DD/ ä¸­çš„æª”æ¡ˆ")
    print("â€¢ æ‰¹æ¬¡æ¸…ç†ä¸¦ä¿å­˜åˆ° data/cleaned/YYYY-MM-DD/")
    print("â€¢ å¤šæ—¥æœŸè³‡æ–™å¤¾æ‰¹æ¬¡è™•ç†")
    print("â€¢ æŒ‡å®šæ—¥æœŸæ¸…ç†æ•¸æ“šè¼‰å…¥")
    print("â€¢ å®Œæ•´æ€§é©—è­‰å’Œå“è³ªè©•ä¼°")
    print("â€¢ ç”Ÿæˆè©³ç´°çš„æ—¥æœŸçµ„ç¹”æ¸…ç†å ±å‘Š")
    print("=" * 80)
    
    start_time = datetime.now()
    
    # ä¸»è¦æ¸¬è©¦æµç¨‹
    test_results = []
    
    # æ¸¬è©¦1: æ—¥æœŸè³‡æ–™å¤¾æª¢æ¸¬
    success = test_date_folder_detection()
    test_results.append(("æ—¥æœŸè³‡æ–™å¤¾æª¢æ¸¬", success))
    
    if success:
        # æ¸¬è©¦2: å–®ä¸€æ—¥æœŸè³‡æ–™å¤¾æ¸…ç†
        success = test_single_date_folder_cleaning()
        test_results.append(("å–®ä¸€æ—¥æœŸè³‡æ–™å¤¾æ¸…ç†", success))
        
        # æ¸¬è©¦3: æ¸…ç†æ–¹æ³•æ¸¬è©¦
        success = test_cleaning_methods()
        test_results.append(("æ¸…ç†æ–¹æ³•æ¸¬è©¦", success))
        
        # æ¸¬è©¦4: æ‰¹æ¬¡æ—¥æœŸæ¸…ç†
        success = test_batch_date_cleaning()
        test_results.append(("æ‰¹æ¬¡æ—¥æœŸæ¸…ç†", success))
        
        # æ¸¬è©¦5: æ¸…ç†æ•¸æ“šé©—è­‰
        success = test_cleaned_data_verification()
        test_results.append(("æ¸…ç†æ•¸æ“šé©—è­‰", success))
        
        # æ¸¬è©¦6: æŒ‡å®šæ—¥æœŸè¼‰å…¥
        success = test_date_specific_loading()
        test_results.append(("æŒ‡å®šæ—¥æœŸè¼‰å…¥", success))
        
        # æ¸¬è©¦7: ä¾¿åˆ©å‡½æ•¸
        success = test_convenience_functions()
        test_results.append(("ä¾¿åˆ©å‡½æ•¸æ¸¬è©¦", success))
        
        # æ¸¬è©¦8: è¼¸å‡ºçµæ§‹é©—è­‰
        success = test_output_structure_verification()
        test_results.append(("è¼¸å‡ºçµæ§‹é©—è­‰", success))
        
        # ç”Ÿæˆæ¸¬è©¦æ‘˜è¦
        generate_test_summary()
        
        # ä½¿ç”¨æ–¹æ³•ç¤ºç¯„
        demonstrate_date_organized_usage()
    
    end_time = datetime.now()
    total_duration = (end_time - start_time).total_seconds()
    
    # æ¸¬è©¦çµæœçµ±è¨ˆ
    passed_tests = sum(1 for _, success in test_results if success)
    
    print(f"\nğŸ æ—¥æœŸçµ„ç¹”æ¸…ç†æ¸¬è©¦å®Œæˆ")
    print("=" * 80)
    print("ğŸ“‹ æ¸¬è©¦çµæœ:")
    
    for test_name, success in test_results:
        status = "âœ… é€šé" if success else "âŒ å¤±æ•—"
        print(f"   â€¢ {test_name}: {status}")
    
    print(f"\nğŸ“Š æ¸¬è©¦çµ±è¨ˆ:")
    print(f"   â€¢ ç¸½æ¸¬è©¦é …ç›®: {len(test_results)}")
    print(f"   â€¢ é€šéæ¸¬è©¦: {passed_tests}")
    print(f"   â€¢ æˆåŠŸç‡: {passed_tests/len(test_results)*100:.1f}%")
    print(f"   â€¢ åŸ·è¡Œæ™‚é–“: {total_duration:.1f} ç§’")
    
    if passed_tests == len(test_results):
        print(f"\nğŸ‰ æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼æ—¥æœŸçµ„ç¹”æ¸…ç†åŠŸèƒ½å®Œå…¨å°±ç·’ï¼")
        
        print(f"\nğŸ¯ æˆåŠŸå®Œæˆ:")
        print("   âœ… æ—¥æœŸè³‡æ–™å¤¾æª¢æ¸¬")
        print("   âœ… å¤šæ—¥æœŸæ‰¹æ¬¡æ¸…ç†")
        print("   âœ… æŒ‡å®šæ—¥æœŸè¼‰å…¥")
        print("   âœ… æ¸…ç†çµæœé©—è­‰")
        print("   âœ… è¼¸å‡ºçµæ§‹å®Œæ•´æ€§")
        
        print(f"\nğŸ“ è¼¸å‡ºä½ç½®:")
        print("   â€¢ æ¸…ç†æª”æ¡ˆ: data/cleaned/YYYY-MM-DD/")
        print("   â€¢ æ‰¹æ¬¡å ±å‘Š: data/cleaned/batch_date_cleaning_report.json")
        print("   â€¢ å„æ—¥æœŸå ±å‘Š: data/cleaned/YYYY-MM-DD/date_cleaning_report.json")
        
        print(f"\nğŸš€ ä¸‹ä¸€æ­¥è¡Œå‹•:")
        print("   1. æª¢æŸ¥ data/cleaned/YYYY-MM-DD/ ä¸­çš„æ¸…ç†æª”æ¡ˆ")
        print("   2. ä½¿ç”¨æŒ‰æ—¥æœŸçµ„ç¹”çš„æ¸…ç†æ•¸æ“šé€²è¡Œæ™‚é–“åºåˆ—åˆ†æ")
        print("   3. é‡é»ä½¿ç”¨å„æ—¥æœŸçš„ target_route_*_cleaned.csv é€²è¡ŒAIè¨“ç·´")
        print("   4. åŸ·è¡ŒæŒ‰æ—¥æœŸçµ„ç¹”çš„æ¢ç´¢æ€§æ•¸æ“šåˆ†æ")
        
        print(f"\nğŸ“… æ—¥æœŸçµ„ç¹”å„ªå‹¢:")
        print("   ğŸ•°ï¸ æ™‚é–“åºåˆ—åˆ†æï¼šä¾¿æ–¼è·¨æ—¥æœŸæ¯”è¼ƒ")
        print("   ğŸ¯ ç²¾æº–æŸ¥è©¢ï¼šå¿«é€Ÿè¼‰å…¥ç‰¹å®šæ—¥æœŸæ¸…ç†æ•¸æ“š")
        print("   ğŸ“Š è¶¨å‹¢åˆ†æï¼šè­˜åˆ¥æ—¥æœŸé–“çš„è®ŠåŒ–æ¨¡å¼")
        print("   ğŸ¤– AIè¨“ç·´ï¼šæ”¯æ´æ™‚é–“åºåˆ—æ¨¡å‹é–‹ç™¼")
        
        return True
    else:
        print(f"\nâŒ æœ‰ {len(test_results) - passed_tests} å€‹æ¸¬è©¦å¤±æ•—")
        print("   å»ºè­°æª¢æŸ¥ç›¸é—œåŠŸèƒ½å¾Œå†ä½¿ç”¨")
        
        print(f"\nğŸ”§ æ•…éšœæ’é™¤:")
        print("   1. ç¢ºèªæ˜¯å¦å·²åŸ·è¡Œ test_loader.py ç”Ÿæˆæ—¥æœŸçµ„ç¹”çš„è™•ç†æª”æ¡ˆ")
        print("   2. æª¢æŸ¥ data/processed/YYYY-MM-DD/ ç›®éŒ„æ˜¯å¦åŒ…å«æ‰€æœ‰åˆ†é¡æª”æ¡ˆ")
        print("   3. ç¢ºèª src/data_cleaner.py æª”æ¡ˆæ˜¯å¦æ­£ç¢º")
        
        return False


if __name__ == "__main__":
    main()